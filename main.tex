\documentclass[a4paper]{book}
\usepackage[Bjornstrup]{fncychap}
\usepackage[usenames, dvipsnames]{xcolor}
\usepackage[greek.ancient,italian]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{mathrsfs}
\usepackage{xfrac}
\usepackage{tocloft}
\usepackage{enumitem}
\usepackage{epigraph}
\usepackage{pgfplots}
\usepackage[labelfont=bf]{caption}
\usepackage[labelfont=bf]{subfig}
\usepgfplotslibrary{fillbetween}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{etoolbox}

\usepackage{hyperref} %da caricare per ultimo!

\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\arctg}{arctg}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\rot}{rot}

\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\numberwithin{equation}{section}

\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\theta}{\vartheta}
\renewcommand{\rho}{\varrho}
\renewcommand{\phi}{\varphi}

\pagestyle{plain}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norma{\lVert}{\rVert}%

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}

\let\oldnorm\norma
\def\norma{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\setlist{itemsep=0.5pt}

\setcounter{tocdepth}{2}

\theoremstyle{plain}
\newtheorem{teor}{Teorema}[section]
\newtheorem{cor}{Corollario}[section]

\theoremstyle{definition}
\newtheorem{defn}{Definizione}[section]
\newtheorem{prop}{Proposizione}[section]
\newtheorem{lem}{Lemma}[section]

\theoremstyle{remark}
\newtheorem{oss}{Osservazione}[section]

\renewcommand{\vec}{\boldsymbol}

\renewcommand*\thesection{\arabic{section}}

\newtheoremstyle{example}
{}
{}
{}
{}
{\scshape }
{}
{1em}
{}
\theoremstyle{example}
\newtheorem{exmp}{esempio}[section]

\AtBeginEnvironment{teor}{\setlist[enumerate,1]{label=\arabic*.,font=\upshape}}

\pgfplotsset{compat=1.15}
\begin{document}

\title{Perdete ogni speranza, voi ch'intrate}

\author{}
\date{}

\maketitle

\tableofcontents

\chapter{Funzioni di più variabili}

	\section{Funzioni da $\mathbb{R}^n$ in $\mathbb{R}$}
		
		\subsection{Derivate direzionali e parziali}
		
		Siano $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$, $A$ aperto di $\mathbb{R}^n$, $\vec{x} \in A$; introduciamo una \emph{direzione}, cioè un versore $\vec{v} \in \mathbb{R}^n$ tale che $\norma{\vec{v}} = 1$. Consideriamo, per ogni $t \in \mathbb{R}$ tale che $\vec{x} + t\vec{v} \in A$, il \emph{rapporto incrementale di $f$ nella direzione $\vec{v}$}:
			
			\begin{equation*}
			\frac{f(\vec{x} + t\vec{v}) - f(\vec{x})}{t}.
			\end{equation*}
			
		\begin{defn}
		Quando esiste finito, il
			
			\begin{equation}
			\lim_{t \to 0}\frac{f(\vec{x} + t\vec{v} - f(\vec{x})}{t}
			\end{equation}
		si chiama derivata nella direzione $\vec{v}$ di $f$ in $\vec{x}$ e si indica con $D_{\vec{v}}f(\vec{x})$; $f$ si dice derivabile nella direzione $\vec{v}$ in $\vec{x}$.
		\end{defn}

Le derivate lungo i versori della base canonica $\vec{e}_1, \vec{e}_2, \dots, \vec{e}_n$ si chiamano \emph{derivate parziali} e si indicano con i simboli
		
		\begin{equation*}
		\frac{\partial f}{\partial x_j}, \quad D_j f, \quad D_{x_j} f, \quad \partial_{x_j} f, \quad f_{x_j}.
		\end{equation*}
		
Nella derivata parziale rispetto a $x_j$ viene incrementata solo quella variabile; dunque per il calcolo di $f_{x_j}$ si può pensare alle altre variabili come costanti e utilizzare le classiche regole di derivazioni per funzioni di una variabile. 

Se una funzione $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$ ammette $n$ derivate parziali in un punto $\vec{x} \in A$ è definito il vettore \emph{gradiente} di $f$ in $\vec{x}$, le cui componenti sono le $n$ derivate parziali di $f$ in $\vec{x}$ e che si indica con $\nabla f(\vec{x})$:
	\begin{equation*}
	\nabla f(\vec{x}) \coloneqq (f_{x_1}(\vec{x}), f_{x_2}(\vec{x}), \dots, f_{x_n}(\vec{x})).
	\end{equation*}

È importante notare che l'esistenza di tutte le derivate direzionali in un punto non implica la continuità in quel punto, in dimensione maggiore di $1$; è quindi necessario introdurre un concetto più forte di quello di derivabilità. 

	\subsection{Differenziale}
	
L'idea è quella di approssimare l'incremento $\Delta f = f(\vec{x} + \vec{h}) - f(\vec{x})$ con una funzione lineare in $\vec{h}$ a meno di infinitesimi di ordine superiore a $\norma{\vec{h}}$.

Ricordiamo che ogni funzione lineare $L \colon \mathbb{R}^n \to \mathbb{R}$ è identificata da un unico vettore $\vec{a} \in \mathbb{R}^n$, nel senso che 
	\begin{equation*}
	L(\vec{x}) = (\vec{a}, \vec{h})
	\end{equation*}
per ogni $\vec{h} \in \mathbb{R}^n$.

\begin{defn}
Sia $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$, $A$ aperto; $f$ si dice differenziabile in $\vec{x} \in A$ se esiste un vettore $\vec{a} \in \mathbb{R}^n$ tale che 
	\begin{equation}
	\label{eqn:diff}
	f(\vec{x} + \vec{h}) - f(\vec{x}) = (\vec{a}, \vec{h}) + o(\norma{h})
	\end{equation}
per $\norma{h} \to 0$, per ogni $\vec{h} \in \mathbb{R}^n$ con $\vec{x} + \vec{h} \in A$.

L'applicazione lineare da $\mathbb{R}^n$ in $\mathbb{R}$ data da
	\begin{equation*}
	\vec{h} \mapsto (\vec{a}, \vec{h})
	\end{equation*}
si chiama differenziale di $f$ in $\vec{x}$ e si indica col simbolo $df(\vec{x}). $
\end{defn}

\begin{teor}
Sia $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$, $A$ aperto; se $f$ è differenziabile in $\vec{x} \in A$, allora:
	\begin{enumerate}[series = teorenum]
		\item $f$ è continua in $\vec{x}$;
		\item $f$ è derivabile in $\vec{x}$ lungo ogni direzione; in particolare esistono tutte le derivate parziali di $f$ in $\vec{x}$ e, se $\vec{a}$ è il vettore in~\eqref{eqn:diff}, si ha $\vec{a} = \nabla f(\vec{x})$. Inoltre vale la formula
			\begin{equation}
			\label{eqn:gradiente}
			D_{\vec{v}}f(\vec{x}) = (\nabla f(\vec{x}), \vec{v}).
			\end{equation}
	\end{enumerate}
\end{teor}

Si può dunque scrivere
	\begin{equation}
	df(\vec{x})(\vec{h}) = (\nabla f(\vec{x}), \vec{h})
	\end{equation}
per ogni $\vec{h} \in \mathbb{R}^n$.

La~\eqref{eqn:gradiente} permette di individuare le direzioni di massima e minima crescita di una funzione differenziabile. Si può scrivere infatti
	\begin{equation*}
	D_{\vec{v}}f(\vec{x}) = \norma{\nabla f(\vec{x})} \cos{\beta},
	\end{equation*}
ove $\beta$ è l'angolo formato dai vettori $\vec{v}$ e $\nabla f(\vec{x})$; ciò significa che $D_{\vec{v}}f(\vec{x})$ è massima quando $\beta = 0$ e minima quando $\beta = \pi$, quindi
	\begin{equation*}
	\vec{v}_\textup{max} =\frac{\nabla f(\vec{x})}{\norma{\nabla f(\vec{x})}}, \quad \vec{v}_\textup{min} =- \frac{\nabla f(\vec{x})}{\norma{\nabla f(\vec{x})}};
	\end{equation*}
in conclusione,	
	\begin{equation*}
	\max_{\norma{\vec{v}} = 1} D_{\vec{v}} f(\vec{x}) = \norma{\nabla f(\vec{x})}, \quad \min_{\norma{\vec{v}} = 1} D_{\vec{v}} f(\vec{x}) = - \norma{\nabla f(\vec{x})}.
	\end{equation*}

L'aspetto geometrico della differenziabilità è legato all'esistenza del piano tangente.
Sia $f$ differenziabile in un punto $\vec{x}_0$; ponendo $\vec{h} = \vec{x} - \vec{x}_0$ scriviamo la~\eqref{eqn:diff} nella forma
	\begin{equation}
	\label{eqn:iperpiano}
	f(\vec{x}) = f(\vec{x}_0) + (\nabla f(\vec{x}_0), \vec{x} - \vec{x}_0) + o(\norma{\vec{x} - \vec{x}_0}).
	\end{equation}
La funzione $z =  f(\vec{x}_0) + (\nabla f(\vec{x}_0), \vec{x} - \vec{x}_0)$ ha come grafico un iperpiano e la~\eqref{eqn:iperpiano} equivale ad affermare che essa è la funzione lineare (affine) che meglio approssima $f$ in un intorno di $\vec{x}_0$; tale piano si chiama piano tangente. 

In dimensione $2$, se $\vec{x}_0 = (x_0, y_0)$, $\vec{x} = (x, y)$ e $z_0 = f(x_0, y_0)$, la sua equazione si scrive esplicitamente come
	\begin{equation}
	\label{eqn:pianotangente}
	z - z_0 - f_x(x_0, y_0)(x - x_0) - f_y(x_0, y_0)(y - y_0) = 0.
	\end{equation}
La~\eqref{eqn:pianotangente} indica che il vettore $\vec{n} = (-f_x(x_0, y_0), -f_y(x_0, y_0)) \in \mathbb{R}^3$ è un vettore normale al piano tangente nel punto $P_0$ di coordinate $(x_0, y_0, z_0)$, dunque, per definizione, normale al grafico di $f$ nello stesso punto. 

\begin{teor}
Sia $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$, $A$ aperto; se in un intorno di $\vec{x} \in A$ esistono tutte le derivate parziali di $f$ ed esse sono continue in $\vec{x}$ allora $f$ è differenziabile in $\vec{x}$.
\end{teor}

La condizione è solo sufficiente: esistono funzioni differenziabili con derivata non continua, ad esempio
	\begin{equation*}
	f(x) = \begin{dcases*} 
	x^2 \sin{\frac{1}{x}} &se $x \ne 0$, \\
	0 &se $x = 0$.
	\end{dcases*}
	\end{equation*}
Dunque la classe delle funzioni differenziabili in un aperto $A$ contiene strettamente quella delle funzioni con derivate parziali continue in $A$ (differenziabili con continuità); quest'ultima classe di funzioni si indica col simbolo $C^1(A)$ ed è uno spazio vettoriale su $\mathbb{R}$. Il precedente Teorema si può enunciare nel seguente modo: \emph{se $f \in C^1(A)$, allora $f$ è differenziabile in A}.

\subsection{Derivate e differenziali di ordine superiore}

Sia $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$, $A$ aperto. Se, fissata la direzione $\vec{v} \in \mathbb{R}^n$, esiste $D_{\vec{v}}f$ in un intorno $U(\vec{x})$ di $\vec{x} \in A$, è definita la funzione
	\begin{equation*}
	D_{\vec{v}}f \colon U(\vec{x}) \to \mathbb{R}.
	\end{equation*}
Se $\vec{w} \in \mathbb{R}^n$ è un altro versore, è lecito chiedersi se esista $D_{\vec{w}} D_{\vec{v}} f(\vec{x})$, cioè la derivata seconda di $f$ lungo le direzioni $\vec{v}$ e $\vec{w}$ (nell'ordine), che si indica con $D_{\vec{w}\vec{v}}^2f(\vec{x})$. 

Nel caso in cui $\vec{v} = \vec{e}_j$ e $\vec{w} = \vec{e}_k$ si ha la derivata parziale seconda rispetto a $x_j$ e $x_k$, indicata con uno dei seguenti simboli:
	\begin{equation*}
	\frac{\partial^2 f}{\partial x_k x_j}(\vec{x}), \quad f_{x_k x_j}(\vec{x}), \quad D_{x_k x_j}f(\vec{x}), \quad D_{kj}^2f(\vec{x}), \quad \partial_{x_kx_j}f(\vec{x}).
	\end{equation*}
Se $k\ne j$ le derivate si chiamano miste; se $k=j$ si chiamano pure e il primo simbolo si semplifica in 
	\begin{equation*}
	\frac{\partial^2 f}{\partial x_j^2}(\vec{x}).
\end{equation*}

In generale, non è vero che, per una funzione due volte derivabile lungo $\vec{v}$ e $\vec{w}$, $D^2_{\vec{w}\vec{v}}f = D^2_{\vec{v}\vec{w}}$; il prossimo teorema indica una condizione sufficiente per l'uguaglianza delle derivate miste. 

	\begin{teor}[di Schwarz]
	Se $f_{x_kx_j}$ e $f_{x_jx_k}$ esistono in un intorno di $\vec{x}$ e sono continue in $\vec{x}$ allora
	\begin{equation*}
	f_{x_kx_j}(\vec{x}) = f_{x_jx_k}(\vec{x}).
\end{equation*}
	\end{teor}

\begin{oss}
Il Teorema vale per le derivate direzionali seconde qualunque, non solo per le derivate seconde miste. Inoltre, si può dimostrare che se $f_{x_k}$, $f_{x_j}$, $f_{x_kx_j}$ esistono in un intorno di $\vec{x}$ e $f_{x_kx_j}$ è continua in $\vec{x}$, allora esiste anche $f_{x_jx_k}(\vec{x})$ ed è uguale a $f_{x_kx_j}(\vec{x})$.
\end{oss}

In maniera del tutto analoga, si possono considerare derivate di ordine superiore.

Sia ora $f\colon \mathbb{R}^n \supseteq A \to \mathbb{R}$ differenziabile in $A$. Allora per ogni $\vec{x} \in A$ esistono le derivate parziali $f_{x_j}(\vec{x})$, $j = 1, \dots, n$; se queste derivate sono a loro volta differenziabili in $\vec{x}$ diremo che $f$ è due volte differenziabile in $\vec{x}$ e si chiama differenziale secondo di $f$ in $\vec{x}$ la forma quadratica nell'incremento $\vec{h} = (h_1, \dots, h_n)$ data da 
	\begin{equation*}
	d^2f(\vec{x}) \colon\! \vec{h} \mapsto \sum_{i,j = 1}^n f_{x_ix_j}(\vec{x})h_ih_j;
	\end{equation*}
in altri termini,
	\begin{equation}
	\label{eqn:diffsecondo}
	d^2f(\vec{x}) \coloneqq \sum_{i,j=1}^n f_{x_ix_j}(\vec{x})dx_idx_j.
	\end{equation}

La matrice quadrata di ordine $n$ i cui elementi sono $f_{x_ix_j}(\vec{x})$ si chiama matrice hessiana di $f$ in $\vec{x}$ e si indica col simbolo $\mathbf{H}_f(\vec{x})$, cioè
	\begin{equation*}
	\mathbf{H}_f(\vec{x}) \coloneqq \begin{pmatrix} f_{x_1x_1}(\vec{x}) & f_{x_1x_2}(\vec{x}) & \dots & f_{x_1x_n}(\vec{x}) \\
			f_{x_2x_1}(\vec{x}) & f_{x_2x2}(\vec{x}) & \dots & f_{x_2x_n}(\vec{x}) \\
			\vdots & \vdots & \ddots & \vdots \\
			f_{x_nx_1}(\vec{x}) & f_{x_nx_2}(\vec{x}) & \dots & f_{x_nx_n}(\vec{x}) 
	\end{pmatrix}.
	\end{equation*}

Si può dunque scrivere 
	\begin{equation*}
	d^2f(\vec{x}) = (\mathbf{H}_f(\vec{x})d\vec{x}, d\vec{x}).\footnote{$\mathbf{H}_f(\vec{x})d\vec{x}$ indica il prodotto righe per colonne della matrice hessiana per il vettore $d\vec{x}$.}
	\end{equation*}

Se $f$ è due volte differenziabile in $\vec{x}$ esistono le derivate $D_{\vec{v}\vec{w}}^2f(\vec{x})$ per ogni coppia di versori $\vec{v}, \vec{w} \in \mathbb{R}^n$; inoltre vale la formula 
	
	\begin{equation}
	D_{\vec{v}\vec{w}}^2f(\vec{x}) = \sum_{i,j=1}^nf_{x_ix_j}(\vec{x})v_iv_j = (\mathbf{H}_f(\vec{x})\vec{w}, \vec{v}).
\end{equation}

\begin{teor}
Se $f$ è due volte differenziabile in $\vec{x}$, l'ordine di derivazione delle derivate miste è invertibile.
\end{teor}

Terminiamo il paragrafo menzionando un importante operatore differenziale, l'operatore di Laplace (o \emph{laplaciana}):
	\begin{equation*}
	\Delta \colon \! f \mapsto \Delta f \coloneqq \frac{\partial^2 f}{\partial x_1^2} + \frac{\partial^2 f}{\partial x_2^2} + \dots + \frac{\partial^2 f}{\partial x_n^2}.
	\end{equation*}
Le funzioni $f \in C^2(A)$ tali che $\Delta f = 0$ in $A$ si dicono armoniche.









\chapter{Curve e integrali curvilinei}

\section{Curve in $\mathbb{R}^3$}
\subsection{Definizioni principali}
Sia $\gamma$ un sottoinsieme di $\mathbb{R}^3$ ed esista una funzione continua $\vec{r}\colon \!I \to \mathbb{R}^3$, dove $I \subseteq \mathbb{R}$ è un intervallo, di cui $\gamma$ è l'immagine; diremo che $\vec{r}$ è una parametrizzazione di $\gamma$.\footnote{È evidente che uno stesso insieme $\gamma$ può avere diverse parametrizzazioni.}

\begin{defn}
Si dice curva in $\mathbb{R}^3$ un insieme $\gamma \subseteq \mathbb{R}^3$ (detto sostegno della curva) con una sua parametrizzazione $\vec{r}(t)$, $t \in I \subseteq \mathbb{R}$.
\end{defn}

Più esplicitamente, una parametrizzazione è assegnata mediante l'equazione
	\begin{equation*}
	\vec{r}(t) = (x(t), y(t), z(t)),
	\end{equation*}
oppure, in forma vettoriale,
	\begin{equation*}
	\vec{r}(t) = x(t)\vec{i} + y(t)\vec{j} + z(t)\vec{k},
	\end{equation*}
con $t \in I$.

Se $I = [a, b]$ e $\vec{r}(a) = \vec{r}(b)$ la curva si dice \emph{chiusa}; se $\vec{r}(t_1) \ne \vec{r}(t_2)$ per ogni $t_1, t_2 \in I$ con almeno uno fra $t_1$ e $t_2$ interni a $I$ la curva si dice \emph{semplice} (cioè una curva semplice non chiusa non ha autointersezioni). Se il sostegno $\gamma$ di una curva è contenuto in un piano la curva si dice \emph{piana}; si può assegnare una curva piana mediante una funzione continua $\vec{r} \colon \! I \to \mathbb{R}^2$.

Le curve piane, semplici e chiuse si chiamano \emph{curve di Jordan}; un importante teorema afferma che il sostegno di una curva di Jordan è frontiera di due aperti nel piano, uno limitato (\emph{interno} della curva) e uno illimitato (\emph{esterno} della curva).

Si noti che, dato che $t \in I \subseteq \mathbb{R}$, essendo $\mathbb{R}$ orientato è automaticamente assegnato su $\gamma$ un verso di percorrenza, ovvero un'orientazione della curva.

\begin{exmp}
Sia $f \colon I \to \mathbb{R}$ una funzione reale di variabile reale, continua. Il suo grafico definisce una curva piana semplice di equazione $\vec{r}(t) = (t, f(t))$, detta \emph{curva cartesiana}.
\end{exmp}

\begin{exmp}
L'equazione $g(x, y) = 0$, con $g$ di classe $C^1$, definisce in un intorno di ogni punto $(x_0, y_0)$ non singolare per $g$ (in cui cioè $\nabla g \ne \vec{0}$) una curva piana. Infatti, per il teorema di Dini, se $g_x(x_0, y_0) \ne 0$ ($g_y(x_0, y_0) \ne 0$), l'equazione $g(x, y) = 0$ definisce implicitamente una funzione $x = f(y)$ ($y = f(x)$) in un intorno di $y_0$ ($x_0$).
\end{exmp}

\begin{exmp}
L'equazione $\rho = f(\theta)$, $\theta \in [\theta_0, \theta_1]$, dove $\rho$ e $\theta$ sono coordinate polari nel piano ed $f$ è continua, definisce una curva piana (in generale non semplice) mediante le equazioni parametriche
	\begin{equation*}
	x(\theta) = f(\theta)\cos\theta, \quad y(\theta) = f(\theta)\sin(\theta).
	\end{equation*}
\end{exmp}

\subsection{Curve regolari}
\begin{defn}
Una curva $\gamma$ di equazione $\vec{r} = \vec{r}(t)$ si dice regolare se $\vec{r} \in C^1(I)$ e se $\vec{r}'(t) \ne \vec{0}$ per ogni $t \in \overset{\circ}{I}$. Si dice regolare a tratti se $I$ si può suddividere nell'unione di un numero finito di intervalli su ciascuno dei quali $\gamma$ è regolare.
\end{defn}

Per una curva regolare è ben definito e diverso da $\vec{0}$ il vettore tangente
	\begin{equation*}
	\vec{r}'(t) = x'(t)\vec{i} + y'(t)\vec{j} + z'(t)\vec{k}.
	\end{equation*}
La retta di equazioni parametriche
	\begin{equation*}
		\begin{split}
		\xi &= x(t_0) + \alpha x'(t_0) \\
		\eta &= y(t_0) + \alpha y'(t_0) \\
		\zeta &= z(t_0) + \alpha z'(t_0)
		\end{split}
		\quad \alpha \in \mathbb{R}
	\end{equation*}
o di equazione vettoriale
	\begin{equation*}
	\vec{\xi}(\alpha) = \vec{r}(t_0) + \alpha \vec{r}'(t_0)
	\end{equation*}
si chiama retta tangente alla curva nel punto $\vec{r}(t_0)$; per una curva piana cartesiana definita dalla funzione $y = f(x)$ le equazioni parametriche della retta tangente in un punto $(x_0, f(x_0))$ si riducono a 
	\begin{equation*}
		\begin{split}
		x &= x_0 + \alpha \\
		y &= f(x_0) + \alpha f'(x_0).
		\end{split}
	\end{equation*}

Dal punto di vista cinematico, $\vec{r}'(t)$ rappresenta il vettore velocità, indicato anche con $\vec{v}(t)$. La velocità scalare $v(t)$ è definita come
	\begin{equation*}
	v(t) \coloneqq \norma{\vec{r}'(t)} = \sqrt{x'(t)^2 + y'(t)^2 + z'(t)^2}.
	\end{equation*}
Se la curva è regolare, $v(t) \ne 0$ per ogni $t \in I$. Risulta quindi ben definito il versore
	\begin{equation*}
	\vec{T}(t) \coloneqq \frac{\vec{r}'(t)}{\norma{\vec{r}'(t)}} = \frac{\vec{v}(t)}{v(t)},
	\end{equation*}
detto versore tangente. 

\begin{exmp}
Se $f \colon I \to \mathbb{R}$ è di classe $C^1$, la curva di equazione $\vec{r}(t) = t\vec{i} + f(t)\vec{j}$ è piana e regolare. Si ha:
	\begin{equation*}
	\vec{r}'(t) = \vec{i} + f'(t)\vec{j}, \quad v(t) = \sqrt{1 + f'(t)^2}.
	\end{equation*}
\end{exmp}

\begin{exmp}
La curva di equazione polare $\rho = f(\theta)$, $\theta \in [\theta_0, \theta_1]$ è regolare se $f$ è di classe $C^1$ e $f'(\theta)^2 + f(\theta)^2 \ne 0$ per ogni $\theta$.
\end{exmp}

\subsection{Curve equivalenti}
Siano $(\gamma, \vec{r})$, $\vec{r} \colon \! I \to \mathbb{R}^3$ una curva regolare e $\phi \colon I_1 \to I$, con $I_1$ intervallo di $\mathbb{R}$, una funzione di classe $C^1(I_1)$ tale che $\phi'(\alpha) \ne 0$ per ogni $\alpha \in I_1$ e che realizzi una corrispondenza biunivoca tra $I_1$ e $I$. La funzione composta
	\begin{equation*}
	\vec{r}_1(\alpha) = \vec{r} \circ \phi(\alpha) \colon \! I_1 \to \mathbb{R}^3
	\end{equation*}
è una nuova parametrizzazione di $\gamma$.

Poiché $\vec{r}_1'(\alpha) = \vec{r}'(\phi(\alpha))\,\phi'(\alpha)$, la coppia $(\gamma, \vec{r}_1)$ è ancora una curva regolare. 

Passando da $\vec{r}$ a $\vec{r}_1 = \vec{r} \circ \phi$, l'orientazione non muta se $\phi'(\alpha) > 0$ per ogni $\alpha \in I_1$, è opposta se $\phi'(\alpha) < 0$; due curve si dicono \emph{equivalenti} se possono essere ottenute l'una dall'altra con un cambio di parametro che non muti l'orientazione. 

\subsection{Curve rettificabili, lunghezza di una curva}

Sia $\gamma$ una curva di equazione $\vec{r} \colon \! [a, b] \to \mathbb{R}^3$.

Fissiamo una suddivisione $\mathcal{D} = \{t_0 = a, t_!, \dots, t_{n-1}, t_n = b\}$ di $[a, b]$ e poniamo, per $j = 0, \dots, n$, $\vec{r}(t_j) = \vec{p}_j$; tali punti individuano una poligonale inscritta nella curva. La lunghezza della poligonale è:
	\begin{equation*}
	l(\Gamma_{\mathcal{D}}) = \sum_{j= 0}^{n-1} \norma{\vec{p}_{j+1} - \vec{p}_j} = \sum_{j=0}^{n-1}\norma{\vec{r}(t_{j+1})-\vec{r}(t_j)}.
	\end{equation*}
Sia ora $L \coloneqq \sup l(\Gamma_{\mathcal{D}})$, dove l'estremo superiore è cercato al variare di tutte le possibili suddivisioni di $[a, b]$.

\begin{defn}
Se $L < +\infty$ si dice che la curva $(\gamma, \vec{r})$ è rettificabile e che $L$ è la sua lunghezza, indicata con $l(\gamma, \vec{r})$.
\end{defn}

\begin{teor}
Se $\gamma$ è una curva regolare di equazione $\vec{r} \colon \![a, b] \to \mathbb{R}^3$, allora è rettificabile e vale la formula
	\begin{equation}
	\label{eqn:lunghezza}
	L = l(\gamma, \vec{r}) = \int_a^b \norma{\vec{r}'(t)}\, dt.
	\end{equation}
\end{teor}

\begin{exmp}
Per le curve piane che sono grafico di una funzione $y = f(x)$, $x \in [a, b]$ la~\eqref{eqn:lunghezza} diventa
	\begin{equation}
	L = \int_a^b \sqrt{1 + f'(t)^2} \, dt.
	\end{equation}
\end{exmp}

\begin{exmp}
Per una curva piana regolare di equazione polare $\rho = f(\theta)$, $\theta \in [\theta_1, \theta_2]$ la lunghezza si trova con la formula
	\begin{equation}
	L = \int_{\theta_1}^{\theta_2} \sqrt{f'(\theta)^2 + f(\theta)^2}\, dt.
	\end{equation}
\end{exmp}

È importante notare che la lunghezza è invariante per cambi di parametrizzazione; in particolare, è identica per curve equivalenti e non dipende dall'orientazione. 

\begin{prop}
Se le curve $\gamma_j$ per $j = 1, \dots, N$ sono rettificabili, anche $\gamma = \gamma_1 \cup \gamma_2 \cup \dots \cup \gamma_N$ è rettificabile e, se $\vec{r}$ è la parametrizzazione di $\gamma$,
	\begin{equation}
	l(\gamma, \vec{r}) = \sum_{j=1}^N l(\gamma_j, \vec{r}_j).
	\end{equation}
\end{prop}

\subsection{Ascissa curvilinea}

Sia $\vec{r} \colon \! [a, b] \to \mathbb{R}^3$ la parametrizzazione di una curva $\gamma$ regolare con lunghezza $L$. Per ogni $t \in [a, b]$ è definita la funzione 
	\begin{equation*}
	s(t) = \int_a^t v(u)\, du
	\end{equation*}
che rappresenta cinematicamente lo spazio percorso al tempo $t$ partendo da $\vec{r}(a)$.

Per il teorema fondamentale del calcolo integrale, essendo $v$ continua in $[a, b]$, $s$ è derivabile e $s'(t) = v(t)$; poiché $v(t) \ne 0$ per ogni $t \in [a, b]$, $s$ risulta una funzione strettamente crescente e realizza una corrispondenza biunivoca tra $[a, b]$ e $[0, L]$. Anche la funzione inversa $t = t(s)$ è strettamente crescente e derivabile con derivata
	\begin{equation*}
	\frac{dt}{ds} = \frac{1}{v(t)}
	\end{equation*}	
continua in $[0, L]$. 

Segue che le curve di equazione $\vec{r} = \vec{r}(t)$ e $\vec{r_1} = \vec{r}(t(s))$ sono equivalenti; il parametro $s$ si chiama ascissa curvilinea (o lunghezza d'arco) e individua un sistema di coordinate ``intrinseco'' alla curva.

\section{Integrali curvilinei}
\subsection{Integrali curvilinei di prima specie}

Siano $f \colon \mathbb{R}^3 \supseteq E \to \mathbb{R}$, con $E$ aperto connesso, una funzione scalare e $\gamma \subset E$ una curva regolare a tratti di equazione $\vec{r} = \vec{r}(t)$, $t \in [a, b]$.

	\begin{defn}
	L'integrale di $f$ rispetto alla lunghezza d'arco lungo $\gamma$ è definito dalla formula
		\begin{equation}
		\label{eqn:intcurv1}
		\int_{\gamma}f\, ds \coloneqq \int_a^b f \circ \vec{r}(t)s'(t)\, dt
		\end{equation}
	quando $f \circ \vec{r}(t)s'(t)$ è integrabile in $[a, b]$.
	\end{defn}

Più esplicitamente, se $\vec{r}(t) = (x(t), y(t), z(t))$:
	\begin{equation*}
	\int_{\gamma} f \, ds = \int_a^b f(x(t), y(t), z(t)) \sqrt{x'(t)^2 + y'(t)^2 + z'(t)^2} \, dt.
	\end{equation*}

L'integrale~\eqref{eqn:intcurv1} è utile nel calcolo di baricentri e momenti d'inerzia (rispetto a un asse) di fili composti da materiali di cui si conosca la densità lineare di massa $\delta = \delta(x, y, z)$. 

Se il filo coincide con una curva $\gamma$ regolare a tratti di equazione $\vec{r} \colon\! [a, b] \to \mathbb{R}^3$, allora
	\begin{equation*}
	\int_{\gamma} \delta \, ds = m,
	\end{equation*}
ove $m$ è la massa totale del filo. Le coordinate del baricentro sono date dalle formule:
	\begin{equation}
	x_b = \frac{1}{m} \int_{\gamma}x\delta \, ds, \quad y_b = \frac{1}{m} \int_{\gamma} y\delta \, ds, \quad z_b = \frac{1}{m} \int_{\gamma} z\delta \, ds.
	\end{equation}
Il momento d'inerzia del filo rispetto a un asse è dato dalla formula
	\begin{equation}
	I = \int_{\gamma} d^2 \delta \, ds,
	\end{equation}
ove $d = d(x, y, z)$ indica la distanza del punto di coordinate $(x, y, z)$ dall'asse.

\subsection{Forme differenziali lineari, integrali curvilinei di seconda specie}
 Sia $\vec{F}(x, y, z) = F_1(x, y, z,)\vec{i} + F_2(x, y, z)\vec{j} + F_3(x, y, z)\vec{k}$ un campo vettoriale di classe $C^1(E)$, con $E$ aperto connesso di $\mathbb{R}^3$. Associamo a $\vec{F}$ l'espressione formale
 	\begin{equation}
 	\omega = F_1 dx + F_2dy + F_3dz,
 	\end{equation}
detta \emph{forma differenziale lineare} con coefficienti $F_1, F_2, F_3$.

Se pensiamo al vettore $d\vec{r} = dx\vec{i} + dy\vec{j} + dz\vec{k}$ come a un vettore ``spostamento infinitesimo'', $\omega = (\vec{F}, d\vec{r})$ rappresenta il lavoro effettuato da $\vec{F}$ in relazione a tale spostamento.

\begin{defn}
L'integrale curvilineo di $\omega$ lungo $\gamma$ è definito dalla formula
	\begin{equation}
	\begin{split}
	\label{eqn:intcurv2}
	\int_{\gamma}\omega \coloneqq &\int_a^b(F_1(x(t), y(t), z(t))x'(t) + F_2(x(t), y(t), z(t))y'(t)+  \\
	&+ F_3(x(t), y(t), z(t))z'(t))\, dt.
	\end{split}
	\end{equation}
\end{defn}

Introducendo il vettore posizione $\vec{r}(t) = x(t)\vec{i} + y(t)\vec{j} + z(t)\vec{k}$ si ha
	\begin{equation*}
	\int_{\gamma} \omega = \int_a^b (\vec{F}, \vec{r}') \, dt
	\end{equation*}
e, moltiplicando e dividendo per $s'(t) = \norma{\vec{r}'(t)}$,
	\begin{equation}
	\int_{\gamma} \omega = \int_{\gamma} (\vec{F}, \vec{T})\, ds,
	\end{equation}
dove
	\begin{equation*}
	\vec{T} = \frac{\vec{r}'}{\norma{\vec{r}'}}.
	\end{equation*}
	
\begin{defn}
Se, data una forma differenziale $\omega$ di classe $C^1(E)$, esiste una funzione $U \colon \! E \to \mathbb{R}$ di classe $C^2(E)$ tale che $dU = \omega$ in $E$, allora $\omega$ si dice esatta e $U$ si chiama funzione potenziale. 
\end{defn}

Più esplicitamente, $dU = \omega$ significa che
	\begin{equation}
\label{eqn:potenziale}
	\frac{\partial U}{\partial x} = F_1, \quad \frac{\partial U}{\partial y} = F_2, \quad \frac{\partial U}{\partial z} = F_3,
	\end{equation}
o più sinteticamente $\nabla U = \vec{F}$ in $E$. Se $U$ è una funzione potenziale per $\omega$ in $E$ lo è anche $U+c$, $c \in \mathbb{R}$. Essendo $E$ connesso, tutte le possibili funzioni potenziale per $\omega$ hanno questa forma.

\begin{lem}
Sia $\omega$ esatta in $E$ con funzione potenziale $U$. Sia $\gamma$ una curva regolare, contenuta in $E$, di equazione $\vec{r} = \vec{r}(t)$, $t \in [a, b]$. Allora
	\begin{equation}
	\int_{\gamma}\omega = U(\vec{r}(b) - \vec{r}(a)).
	\end{equation}
\end{lem}

Se $\omega$ è esatta il campo associato è uguale, per la~\eqref{eqn:potenziale}, al gradiente di un potenziale. In tal caso il campo vettoriale si dice \emph{conservativo}. 

\begin{teor}
Sia $\omega = F_1dx + F_2dy + F_3dz$ una forma differenziale lineare di classe $C^1(E)$, $E$ aperto connesso di $\mathbb{R}^3$. Le seguenti affermazioni sono equivalenti:
	\begin{enumerate}
	\item per ogni coppia di curve regolari a tratti $\gamma_1, \gamma_2$ contenute in $E$ e aventi stessi punti iniziale e finale
		\begin{equation*}
		\int_{\gamma_1}\omega = \int_{\gamma_2}\omega;
		\end{equation*}
	\item per ogni curva chiusa $\gamma$ regolare a tratti contenuta in $E$,
		\begin{equation*}
		\oint_{\gamma} \omega = 0;
		\end{equation*}
	\item $\omega$ è esatta in $E$.
	\end{enumerate}
\end{teor}

\subsection{Riconoscimento delle forme differenziali esatte. Costruzione della funzione potenziale}
Sia data la forma differenziale
	\begin{equation*}
	\omega = F_1dx + F_2 dy + F_3dz,
	\end{equation*}
con coefficienti di classe $C^1(E)$, dove $E$ è un aperto connesso di $\mathbb{R}^3$.

\begin{prop}
Se $\omega$ è esatta in $E$ ed $\vec{F}$ è il campo vettoriale associato, allora $\rot{\vec{F}} = \vec{0}$ in $E$, ovvero $\vec{F}$ è irrotazionale in $E$.
\end{prop}

Più esplicitamente, si devono verificare in ogni punto di $E$ le relazioni
	\begin{equation}
	\label{eqn:irrot}
	\frac{\partial F_3}{\partial y} = \frac{\partial F_2}{\partial z}, \quad \frac{\partial F_1}{\partial z} = \frac{\partial F_3}{\partial x}, \quad \frac{\partial F_2}{\partial x} = \frac{\partial F_1}{\partial y}.
	\end{equation}

In $\mathbb{R}^2$ le~\eqref{eqn:irrot} si riducono a
	\begin{equation}
	\frac{\partial F_2}{\partial x} = \frac{\partial F_1}{\partial y}.
	\end{equation}

Vi sono opportune condizioni topologiche su $E$ sotto le quali le~\eqref{eqn:irrot} diventano anche condizioni sufficienti; per adesso ci limitiamo a dare la seguente
	\begin{defn}
	Si dice che $E \subseteq \mathbb{R}^3$ è stellato se esiste un punto $\vec{p}_0 \in E$ tale che, per ogni punto $\vec{p} \in E$, il segmento di retta $[\vec{p}_0. \vec{p}]$ è tutto contenuto in $E$.
	\end{defn}

Ogni insieme convesso è stellato; un insieme stellato è ovviamente connesso per segmenti e perciò connesso. 

\begin{teor}
Siano $\omega = F_1dx + F_2 dy + F_3 dz$, $\vec{F} = F_1\vec{i} + F_2\vec{j} + F_3\vec{k}$ con $\vec{F} \in C^1(E)$, dove $E$ è un aperto stellato in $\vec{R}^3$. Allora $\omega$ è esatta se e solo se $\rot{\vec{F}} = \vec{0}$ in $E$.
\end{teor}

La dimostrazione del Teorema fornisce una formula per la costruzione della funzione potenziale; supponendo che $E$ sia stellato rispetto all'origine (a meno di una traslazione degli assi), dato un punto $\vec{p} = (x, y, z) \in E$ indichiamo con $\Gamma$ la curva di equazioni parametriche
	\begin{equation*}
	x(t) = tx, \quad y(t) = ty, \quad z(t) = tz \quad t \in [0, 1]
	\end{equation*}
il cui sostegno è il segmento di retta $[\vec{0}, \vec{p}]$ che risulta pertanto contenuto in $E$. Poniamo:
	\begin{equation}
	\begin{split}
	U(x, y, z) \coloneqq \int_{\Gamma}\omega = &\int_0^1 (F_1(tx, ty, tz)x + F_2(tx, ty, tz)y + \\
 &+F_3(tx, ty, tz)z) dt.
	\end{split}
	\end{equation}

Si dimostra che questa è una funzione potenziale; naturalmente, si può costruire $U$ come integrale di $\omega$ lungo una qualunque curva regolare a tratti con sostegno in $E$. 

\subsection{Insiemi semplicementi connessi}
Abbiamo visto che per gli insiemi stellati le~\eqref{eqn:irrot} sono necessarie e sufficienti per l'esattezza di una forma differenziale. Una condizione più generale è quella di \emph{semplice connessione}: un insieme $E \subseteq \mathbb{R}^2$ è semplicemente connesso se $E$ è connesso e ogni curva semplice e chiusa contenuta in $E$ è frontiera di un insieme limitato interamente contenuto in $E$. In generale, significa che due curve qualunque contenute in $E$ con gli stessi estremi sono ``deformabili con continuità''  l'una nell'altra senza uscire da $E$. Il concetto rigoroso è quello di \emph{omotopia} tra curve.

Siano $\gamma_1$ e $\gamma_2$ curve contenute in un aperto connesso $E \subseteq \mathbb{R}^3$ di equazioni $\vec{r}_1 = \vec{r}_1(t)$, $\vec{r}_2 = \vec{r}_2(t)$, $t \in [a, b]$ e tali che $\vec{r}_1(a) = \vec{r}_2(a)=\vec{p}_a$, $\vec{r}_1(b) = \vec{r}_2(b) = \vec{p}_b$.

	\begin{defn}
	Le due curve $\gamma_1$ e $\gamma_2$ si dicono omotope in $E$ se esiste una funzione continua $\vec{\phi} = \vec{\phi}(t, \lambda)$, $(t, \lambda) \in [a, b] \times [0, 1]$ tale che
	\begin{enumerate}
	\item $\vec{\phi}(t, 0) = \vec{r}_1(t), \quad \vec{\phi}(t, 1) = \vec{r}_2(t) \quad \forall t \in [a, b]$;
	\item $\vec{\phi}(a, \lambda) = \vec{p}_a, \quad \vec{\phi}(b, \lambda) = \vec{p}_b \quad \forall \lambda \in [0, 1] $,
	\end{enumerate}
e infine che per ogni $\lambda \in [0, 1]$ la curva $\gamma_{\lambda}$ di equazione $\vec{\phi} = \vec{\phi}(t, \lambda)$ sia contenuta in $E$. Se $\gamma_1$ e $\gamma_2$ sono chiuse, la $2.$ è sostituita dalla condizione
	\begin{equation*}
	\vec{\phi}(a, \lambda) = \vec{\phi}(b, \lambda) \quad \forall \lambda \in [0, 1].
	\end{equation*}
	\end{defn}

\begin{defn}
Un aperto connesso $E \subseteq \mathbb{R}^3$ si dice semplicemente connesso se due curve qualsiasi contenute in $E$ aventi gli stessi estremi sono omotope.
\end{defn}

La definizione si può dare in termini di curve chiuse: un aperto connesso $E$ è semplicemente connesso se ogni curva chiusa contenuta in $E$ è omotopa a una curva costante (cioè che si riduce a un solo punto).

Gli insiemi convessi e quelli stellati sono semplicemente connessi; non lo sono, ad esempio, una corona circolare o il piano privato di un punto (in $\mathbb{R}^2$) o una sfera privata di un diametro o l'interno di un toro (in $\mathbb{R}^3$). 

\begin{teor}
Siano $\omega = F_1dx + F_2 dy + F_3 dz$, $\vec{F} = F_1\vec{i} + F_2\vec{j} + F_3\vec{k}$ con $\vec{F} \in C^1(E)$, dove $E$ è un aperto semplicemente connesso in $\vec{R}^3$. Allora $\omega$ è esatta se e solo se $\rot{\vec{F}} = \vec{0}$ in $E$.
\end{teor}


\chapter{Ottimizzazione di funzioni di più variabili}
\section{Generalità sull'ottimizzazione, estremi liberi}
\subsection{Generalità sull'ottimizzazione}
Sia $f\colon \mathbb{R}^n \supseteq X \to \mathbb{R}$.
\begin{defn}
Un punto $\vec{x}_0 \in X$ si dice di \emph{massimo} (\emph{minimo}) \emph{locale} per $f$ se esiste un intorno $B_r(\vec{x}_0)$ tale che
	\begin{equation}
	\label{eqn:defestremi}
	f(\vec{x}) \le f(\vec{x}_0) 	\quad (f(\vec{x}) \ge f(\vec{x}_0))
	\end{equation}per ogni $\vec{x} \in X \cap B_r(\vec{x}_0)$. Si dice di massimo (minimo) \emph{globale} se la~\eqref{eqn:defestremi} vale per ogni $\vec{x} \in X$.
\end{defn}

Inoltre, un punto si dice di massimo (minimo) \emph{forte} se nella~\eqref{eqn:defestremi} la disuguaglianza vale in senso stretto.

Se $X$ è aperto gli estremi di $f$ (se esistono) si dicono \emph{liberi} e la loro ricerca consiste nell'individuare i punti di estremo interni al dominio $X$.

Se siamo interessati a studiare gli estremi di $f$ in una sua restrizione $f_{|U}$, ove $U \subset X$, si parla di \emph{estremi vincolati}. Studieremo i casi in cui $U$ è l'intersezione di $X$ con un insieme definito da un insieme di uguaglianze del tipo
	\begin{equation*}
	g_j(\vec{x}) = 0,
	\end{equation*}
per $j = 1, 2, \dots, m$ e con $m < n$. 

\subsection{Estremi liberi: condizioni necessarie}
Siano $f \colon \mathbb{R}^n \supseteq X \to \mathbb{R}$, con $X$ aperto, e $\vec{x_0}$ un punto di estremo locale per $f$.
\begin{teor}
Fissata una direzione $\vec{v}$ in $\mathbb{R}^n$, se $\vec{x_0}$ è punto di estremo locale per $f$ e $D_{\vec{v}}f(\vec{x_0})$ esiste allora $D_{\vec{v}}f(\vec{x}_0) = 0$.
\end{teor}

\begin{cor}
Se $f$ è differenziabile in $\vec{x}_0$, punto di estremo locale per $f$, allora $\nabla f(\vec{x}_0) = \vec{0}$ (ogni derivata direzionale in $\vec{x}_0$ è nulla).
\end{cor}

Si noti che tale condizione è solo \emph{necessaria}: possono esistere punti critici che non siano di massimo né di minimo. In particolare, un punto critico $\vec{x_0}$ si dice di \emph{sella} (o di \emph{colle}) se in ogni intorno di $\vec{x_0}$ esistono punti in cui $f$ è maggiore di $f(\vec{x_0})$ e punti in cui $f$ è minore di $f(\vec{x_0})$.

Si noti che per le funzioni convesse e concave non occorrono ulteriori analisi:
\begin{prop}
Se $\vec{x}_0$ è un punto critico per una funzione $f$ convessa (concava) e differenziabile, allora $\vec{x}_0$ è punto di minimo (massimo) globale. Se $f$ è strettamente convessa (concava) allora $\vec{x}_0$ è punto di minimo (massimo) unico e forte.
\end{prop}

\subsection{Forme quadratiche}
\label{subsec:formequadratiche}
Una \emph{forma quadratica} in $\mathbb{R}^n$ è un polinomio omogeneo di secondo grado del tipo
	\begin{equation}
	\label{eqn:formaquadratica}
	q(\vec{h}) = q(h_1, h_2, \dots, h_n) = \sum_{i,j=1}^na_{ij}h_ih_j,
	\end{equation}
dove gli $a_{ij}$ sono i numeri reali coefficienti della forma quadratica. Si può sempre supporre che una forma quadratica sia simmetrica; se così non fosse, basta sostituire per ogni $i \ne j$ i coefficienti $a_{ij}$ e $a_{ji}$ con la loro semisomma
	\begin{equation*}
	\frac{a_{ij} + a_{ji}}{2}.
	\end{equation*}
	
A ogni forma quadratica $q(\vec{h})$ risulta associata una matrice simmetrica $A = (a_{ij})_{i,j = 1, \dots, n}$ con una corrispondenza biunivoca; una forma quadratica può anche essere scritta come prodotto scalare, $q(\vec{h}) = (A\vec{h}, \vec{h})$, o in notazione matriciale, $q(\vec{h}) = \vec{h}^tA\vec{h}$.

\begin{defn}
Una forma quadratica $q(\vec{h})$, $\vec{h} \in \mathbb{R}^n$, si dice:
	\begin{enumerate}
	\item \emph{definita positiva} (\emph{negativa}) se $\forall \vec{h} \ne 0$, $q(\vec{h}) > 0$ $(q(\vec{h}) < 0)$;
	\item \emph{semidefinita positiva} (\emph{negativa}) se $\forall \vec{h} \ne 0$, $q(\vec{h}) \ge 0$ $(q(\vec{h}) \le 0)$;
	\item \emph{indefinita} se esistono $\vec{h}_1, \vec{h}_2 \in \mathbb{R}^n$ tali che $q(\vec{h}_1) > 0$, $q(\vec{h}_2) < 0$.
	\end{enumerate}
\end{defn}

Indichiamo con $A_k$, $k = 1, \dots, n$, le $n$ sottomatrici composte dalle prime $k$ righe e $k$ colonne di $A$:
\begin{equation*}
A_1 = (a_{11}), \ A_2 = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}, \ A_3 = \begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{pmatrix}, \dots, \ A_n = A.
\end{equation*}

\begin{teor}
Sia 
	\begin{equation*}
	q(\vec{h}) = \sum_{i,j = 1}^n a_{ij}h_ih_j,
	\end{equation*}
$\vec{h} \in \mathbb{R}^n$. Allora:
	\begin{enumerate}
	\item q è definita positiva se e solo se $\det{A_k} > 0$ per ogni $k = 1, 2, \dots, n$;
	\item q è definita negativa se e solo se $(-1)^k\det{A_k} > 0$ per ogni $k = 1, 2, \dots, n$.
	\end{enumerate}
\end{teor}

Il Teorema vale più in generale per ogni catena di \emph{sottomatrici principali} (cioè simmetriche rispetto alla diagonale principale di $A$) ottenute partendo da un elemento $a_{jj}$ della diagonale e aggiungendo ogni volta una riga e una colonna.

\begin{teor}
Sia 
	\begin{equation*}
	q(\vec{h}) = \sum_{i,j = 1}^na_{ij}h_ih_j,
	\end{equation*}
$\vec{h} \in \mathbb{R}^n$. Allora:
	\begin{enumerate}
	\item $q$ è semidefinita positiva se e solo se ogni sottomatrice principale ha determinante non negativo;
	\item $q$ è semidefinita negativa se e solo se ogni sottomatrice principale di ordine $k$ ha determinante non negativo se $k$ è pari, non positivo se $k$ è dispari.
	\end{enumerate}
\end{teor}
In ogni altro caso $q(\vec{h})$ è indefinita.

\begin{teor}
Sia $q(\vec{h}) = \vec{h}^tA\vec{h}$, $\vec{h} \in \mathbb{R}^n$. Allora:
	\begin{enumerate}
	\item $q$ è definita positiva (negativa) se e solo se tutti gli autovalori sono positivi (negativi);
	\item $q$ è semidefinita positiva (negativa) se e solo se tutti gli autovalori sono non negativi (non positivi) e almeno uno di essi è zero;
	\item $q$ è indefinita se e solo se esistono due autovalori di segno opposto.
	\end{enumerate}
\end{teor}

Si conclude con la seguente
\begin{oss}
Se $q = q(\vec{h})$ è una forma quadratica in $\mathbb{R}^n$, allora $q(\vec{0}) = 0$ e $\nabla q(\vec{0}) = \vec{0}$; la natura di $\vec{h} = \vec{0}$ dipende dal segno di $q$.

Se $q$ è definita positiva (negativa), $\vec{h} = \vec{0}$ è punto di minimo (massimo) globale forte.

Se $q$ è indefinita, $\vec{0}$ è di sella.

Se $q$ (non nulla) è semidefinita positiva (negativa), $\vec{0}$ è punto di minimo (massimo) globale debole.
\end{oss}

\subsection{Condizioni sufficienti per estremi liberi}
La matrice corrispondente a $d^2f(\vec{x}_0)$ è l'hessiana di $f$ in $\vec{x}_0$, ossia
	\begin{equation*}
	\mathbf{H}_f(\vec{x}_0) = (f_{x_ix_j}(\vec{x}_0))_{i,j = 1, \dots, n},
	\end{equation*}
che risulta simmetrica se $f_{x_ix_j}(\vec{x}_0) = f_{x_jx_i}(\vec{x}_0)$ per ogni $i,j = 1, 2, \dots, n$ (accade, per esempio, se $f \in C^2$).

\begin{teor}
Siano $f \in C^2(X)$ e $\vec{x}_0$ punto critico per $f$. Se $d^2f(\vec{x}_0)$ è:
	\begin{enumerate}
	\item definita positiva (negativa), $\vec{x}_0$ è punto di minimo (massimo) locale forte;
	\item indefinita, $\vec{x}_0$ è punto di sella.
	\end{enumerate}
\end{teor}

\begin{prop}
Sia $f \in C^2(X)$. Se $\vec{x}_0$ è punto di massimo (minimo), allora $d^2f(\vec{x}_0)$ è definita o semidefinita negativa (positiva). In particolare, $f_{x_jx_i} \le 0$ $(\ge 0)$ per ogni $i,j = 1, 2, \dots, n$.
\end{prop}

Una formulazione equivalente della Proposizione è la seguente: se $d^2f(\vec{x}_0)$ non è nulla ed è semidefinita positiva (negativa), allora $\vec{x}_0$ non può essere punto di massimo (minimo).

\begin{oss} Dalle considerazioni precedenti, si arriva alla seguente regola nel caso bidimensionale.

Siano $f \in C^2(X)$, $X$ aperto di $\mathbb{R}^2$ e $(x_0, y_0)$ punto critico per $f$. L'hessiana di $f$ in $(x_0, y_0)$ è
	\begin{equation*}
	\mathbf{H}_f(x_0, y_0) = \begin{pmatrix} f_{xx}(x_0, y_0) & f_{yx}(x_0, y_0) \\ f_{xy}(x_0, y_0) & f_{yy}(x_0, y_0) \end{pmatrix};
	\end{equation*}
\begin{enumerate}
\item se $\det{\mathbf{H}_f(x_0, y_0)} > 0$\footnote{Si noti che in questo caso $f_{xx}(x_0, y_0)$ e $f_{yy}(x_0, y_0)$ hanno lo stesso segno.} e 
	\begin{itemize}
	\item $f_{xx}(x_0, y_0) > 0$, allora $(x_0, y_0)$ è punto di minimo locale forte;
	\item $f_{xx}(x_0, y_0) < 0$, allora $(x_0, y_0)$ è punto di massimo locale forte;
	\end{itemize}
\item se $\det{\mathbf{H}_f(x_0, y_0)} < 0$, allora $(x_0, y_0)$ è punto di sella;
\item se $\det{\mathbf{H}_f(x_0, y_0)} = 0$ occorre uno studio più approfondito.
\end{enumerate}
Inoltre, se $\det{\mathbf{H}_f(x, y)} > 0$ per ogni $(x, y) \in X$ l'estremo è globale. Infine, se $\det{\mathbf{H}_f(x, y)} = 0$ e $f_{xx}(x,y) > 0$ ($< 0$) oppure $f_{yy}(x,y) > 0$ ($< 0$) in tutto $X$, allora $(x_0, y_0)$ è punto di minimo globale (massimo globale).
\end{oss}

\section{Estremi vincolati, vincoli di uguaglianza}
\subsection{Funzioni di due variabili}
Esaminiamo innanzitutto il caso più semplice.

Date due funzioni di due variabili $f$ e $g$ di classe $C^1(X)$, $X$ aperto di $\mathbb{R}^2$, si vogliono determinare gli estremi di $f$ (\emph{funzione obiettivo}) ristretta all'insieme (\emph{vincolo})
	\begin{equation*}
	E_0 = \{ (x, y) \in \mathbb{R}^2 \colon g(x, y) = 0 \}.
	\end{equation*}
	
La situazione più favorevole è quella in cui dall'equazione $g(x, y) = 0$ si può esplicitare $y = y(x)$ o $x = x(y)$ oppure, più in generale, quella in cui $E_0$ coincide con una curva $\gamma$ di equazioni parametriche $x = x(t)$ e $y = y(t)$, $t \in I$, con $I$ intervallo contenuto in $\mathbb{R}$.

Il problema si riconduce alla ricerca degli estremi della funzione reale di variabile reale
	\begin{equation*}
	\phi (t) = f(x(t), y(t)),
	\end{equation*}
con $t \in I$.

\begin{exmp}
Si determinino gli estremi di $f(x, y) = x^2 + 3y$, con il vincolo
	\begin{equation*}
	g(x,y) = \frac{x^2}{4} + \frac{y^2}{9} - 1 = 0.
	\end{equation*}
Si nota facilmente che $E_0$ è un'ellisse che ha per equazioni parametriche $x(t) = 2 \cos{t}$, $y(t) = 3\sin{t}$, $t \in [0, 2 \pi]$, e pertanto basta determinare gli estremi di
	\begin{equation*}
	\phi(t) = 4(\cos{t})^2 + 9\sin{t}
	\end{equation*} 
nell'intervallo $[0, 2\pi]$. Con gli strumenti classici dell'analisi differenziale, si trova facilmente che $t= \pi/2$ è un punto di massimo locale e $t = 3\pi/2$ un punto di minimo locale (e $9$ e $-9$ sono rispettivamente massimo e minimo globali).
\end{exmp}

In generale, non si potrà ridurre a una dimensione il problema; vediamo dunque come procedere.

Introduciamo innanzitutto il concetto di \emph{punto critico} (o \emph{stazionario}) \emph{vincolato}. Sia $(x_0, y_0)$ un punto \emph{regolare} di $E_0$, cioè
	\begin{equation*}
	g(x_0, y_0) = 0, \ \nabla g(x_0, y0) \ne \vec{0}.
	\end{equation*}
In tal caso, in un intorno di $(x_0, y_0)$ $E_0$ coincide con una curva $\gamma$ di equazioni parametriche $x = x(t)$ e $y = y(t)$; si può inoltre scegliere il parametro in modo che $t$ vari in un intorno $I_0$ di $t = 0$ e che $x(0) = x_0$, $y(0) = y_0$.

Dunque, il vettore $(x'(0), y'(0))$ è tangente al vincolo nel punto $(x_0, y_0)$. Si può allora considerare la funzione $\phi(t) = f(x(t), y(t))$ e definire $(x_0, y_0)$ \emph{punto critico vincolato} se $t = 0$ è \emph{punto critico} per $\phi$, ovvero se $\phi'(0) = 0$.

Essendo $f$ differenziabile, si ha
	\begin{equation*}
	\phi'(t) = f_x(x(t), y(t))x'(t) + f_y(x(t), y(t)) y'(t)
	\end{equation*}
e imponendo $\phi'(0) = 0$ si ha
	\begin{equation}
	\label{eqn:ptocritvinc}
	f_x(x_0, y_0)x'(0) + f_y(x_0, y_0)y'(0) = 0.
	\end{equation}
In altre parole, la derivata di $f$ in direzione tangente al vincolo in $(x_0, y_0)$ è nulla.
	
\begin{defn}
Siano $f, g \in C^1(X)$, $X$ aperto di $\mathbb{R}^2$. Un punto $(x_0, y_0) \in X$ si dice \emph{critico} o \emph{stazionario} condizionato al vincolo $g(x, y) = 0$ se:
	\begin{enumerate}
	\item $g(x_0, y_0) = 0$ e $\nabla g(x_0, y_0) \ne \vec{0}$, cioè $(x_0, y_0)$ è un punto regolare per $E_0$;
	\item la derivata di $f$ in direzione tangente al vincolo si annulla in $(x_0, y_0)$, cioè vale la~\eqref{eqn:ptocritvinc}.
	\end{enumerate} 
\end{defn}

\begin{teor}[Caratterizzazione dei punti critici vincolati]
\label{teor:carcritvinc}
Siano $f, g \in C^1(X)$, $X$ aperto di $\mathbb{R}^2$ e sia $(x_0, y_0)$ un punto regolare per $E_0 = \{ (x,y) \in \mathbb{R}^2 \colon g(x,y) = 0\}$. Allora $(x_0, y_0)$ è punto critico vincolato a $E_0$ se e solo se esiste un numero reale $\lambda_0$ tale che
	\begin{equation}
	\label{eqn:carcritvinc}
	\nabla f(x_0, y_0) = \lambda_0 \nabla g(x_0, y_0).
	\end{equation}
\end{teor}

\begin{teor}[Condizione necessaria per gli estremi vincolati]
Nelle ipotesi del precedente Teorema, se $(x_0, y_0)$ è un punto di estremo vincolato (con vincolo $g(x, y) = 0)$, allora è punto critico vincolato. In particolare, esiste $\lambda_0$ tale che valda la~\eqref{eqn:carcritvinc}.
\end{teor}

\begin{oss}
Il numero $\lambda_0$ la cui esistenza è asserita nel Teorema \ref{teor:carcritvinc} prende il nome di \emph{moltiplicatore di Lagrange}. 

Introducendo la funzione $\mathcal{L} = \mathcal{L}(x, y, \lambda)$, detta \emph{lagrangiana}, definita da
	\begin{equation*}
	\mathcal{L}(x, y, \lambda) \coloneqq f(x,y) - \lambda g(x,y),
	\end{equation*}
il Teorema \ref{teor:carcritvinc} afferma che $(x_0, y_0)$ è punto di critico vincolato se e solo se esiste $\lambda_0$ tale che il punto $(x_0, y_0, \lambda_0)$ sia punto critico libero per $\mathcal{L}$. Infatti, i punti critici di $\mathcal{L}$ sono soluzioni del sistema
	\begin{equation}
	\label{eqn:moltiplicatorilagrange}
	\begin{dcases}
	\mathcal{L}_x = f_x - \lambda g_x = 0 \\
	\mathcal{L}_y = f_y - \lambda g_y = 0 \\
	\mathcal{L}_{\lambda} = -g = 0.
	\end{dcases}
	\end{equation}
Si noti che le prime due equazioni coincidono con la~\eqref{eqn:carcritvinc}, mentre la terza esprime la condizione del vincolo.
\end{oss}

Abbiamo dunque sviluppato il seguente modo di procedere, detto \emph{metodo dei moltiplicatori di Lagrange}:
\begin{enumerate}
	\item si isolano gli eventuali punti non regolari di $E_0$, che vanno esaminati a parte;
	\item si cercano i punti critici vincolati di $f$ o equivalentemente quelli liberi della lagrangiana, cioè le soluzioni del sistema~\eqref{eqn:moltiplicatorilagrange};
	\item si determina la natura dei punti critici (si veda il prossimo paragrafo). 
\end{enumerate}

\subsection{Studio della natura dei punti critici vincolati}
Ci limiteremo al caso di un solo vincolo ($m = 1$).

Per lo studio della natura dei punti critici vincolati procederemo come nel caso dei punti critici liberi, studiando il differenziale secondo (rispetto a $\vec{x}$) della lagrangiana ristretto a incrementi tangenziali ai vincoli.

Sia $\vec{x}_0 \in \mathbb{R}^n$ un punto critico per $f$ condizionato al vincolo $g(\vec{x}) = 0$ e sia $\lambda_0$ il corrispondente moltiplicatore di Lagrange. 

Vale il seguente
\begin{teor}
Siano $f, g \colon X \to \mathbb{R}$, $X$ aperto di $\mathbb{R}^n$, di classe $C^2$. 

Se la forma quadratica
	\begin{equation}
	\label{eqn:naturacritvinc}
	\sum_{i,j=1}^n\bigl(f_{x_ix_j}(\vec{x}_0) - \lambda_0 g_{x_ix_j}(\vec{x}_0) \bigr) h_ih_j = \bigl( (\mathbf{H}_f(\vec{x}_0) - \lambda_0\mathbf{H}_g(\vec{x}_0))\vec{h}, \vec{h}\bigr),
	\end{equation}
ristretta all'insieme dei vettori $\vec{h} \in \mathbb{R}^n$ tangenziali al vincolo in $\vec{x}_0$ (cioè $(\nabla g(\vec{x_0}), \vec{h}) = 0$), è definita positiva (negativa), allora $\vec{x}_0$ è punto di massimo (minimo) locale forte vincolato.
\end{teor}

Può essere utile il seguente criterio di riconoscimento del segno di una forma quadratica soggetta a un vincolo lineare.

\begin{lem}
Sia $q(\vec{h}) = (A\vec{h}, \vec{h})$ una forma quadratica in $\mathbb{R}^n$ e sia $\vec{b} \in \mathbb{R}^n$, $\vec{b} = (b_1, b_2, \dots, b_n)$, $b_1 \ne 0$. Allora la forma quadratica $q(\vec{h})$ soggetta al vincolo lineare $(\vec{b}, \vec{h}) = 0$ è definita positiva se sono negativi tutti i minori principali di ``nord-ovest''\footnote{Sono chiamati così i determinanti delle sottomatrici principali di nord-ovest, quelle che nella sottosezione \ref{subsec:formequadratiche} abbiamo indicato con $A_k$.}, di ordine maggiore di 2, della matrice
	\begin{equation}
	\begin{pmatrix}
	0 & b_1 & \dots & b_n  \\
	b_1 \\
	\vdots &  &A \\
	b_n 
	\end{pmatrix}.
	\end{equation}
È definita negativa se i suddetti minori si susseguono a segni alterni a partire dal primo (di ordine 3) positivo.
\end{lem}

Nel nostro caso, il lemma si applica con $A = \mathbf{H}_f(\vec{x}_0) - \lambda_0 \mathbf{H}_g(\vec{x}_0)$ e $\vec{b} = \nabla g(\vec{x}_0)$; poiché $\nabla g(\vec{x}_0) \ne \vec{0}$, si può supporre sempre che $D_{x_1}g(\vec{x}_0) \ne 0$.
























\end{document}
