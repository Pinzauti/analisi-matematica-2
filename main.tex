\documentclass[a4paper]{book}
\usepackage[Bjornstrup]{fncychap}
\usepackage[usenames, dvipsnames]{xcolor}
\usepackage[greek.ancient,italian]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{mathrsfs}
\usepackage{xfrac}
\usepackage{tocloft}
\usepackage{enumitem}
\usepackage{epigraph}
\usepackage{pgfplots}
\usepackage[labelfont=bf]{caption}
\usepackage[labelfont=bf]{subfig}
\usepgfplotslibrary{fillbetween}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{etoolbox}

\usepackage{hyperref} %da caricare per ultimo!

\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\arctg}{arctg}
\DeclareMathOperator{\sgn}{sgn}

\DeclareMathOperator{\Epi}{Epi}

\DeclareMathOperator{\rot}{rot}
\DeclareMathOperator{\Vol}{Vol}
\DeclareMathOperator{\divg}{div}



\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\numberwithin{equation}{section}

\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\theta}{\vartheta}
\renewcommand{\rho}{\varrho}
\renewcommand{\phi}{\varphi}

\renewcommand{\ni}{\nu}




\pagestyle{plain}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norma{\lVert}{\rVert}%

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}

\let\oldnorm\norma
\def\norma{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\setlist{itemsep=0.5pt}

\setcounter{tocdepth}{2}

\theoremstyle{plain}
\newtheorem{teor}{Teorema}[section]
\newtheorem{cor}{Corollario}[section]

\theoremstyle{definition}
\newtheorem{defn}{Definizione}[section]
\newtheorem{prop}{Proposizione}[section]
\newtheorem{lem}{Lemma}[section]

\theoremstyle{remark}
\newtheorem{oss}{Osservazione}[section]

\renewcommand{\vec}{\boldsymbol}

\renewcommand*\thesection{\arabic{section}}

\newtheoremstyle{example}
{}
{}
{}
{}
{\scshape }
{}
{1em}
{}
\theoremstyle{example}
\newtheorem{exmp}{esempio}[section]

\AtBeginEnvironment{teor}{\setlist[enumerate,1]{label=\arabic*.,font=\upshape}}

\pgfplotsset{compat=1.15}
\begin{document}

\title{Analisi Matematica II}

\author{}
\date{}

\maketitle

\tableofcontents

\chapter{Brevi cenni agli spazi metrici}

\epigraph{
\begin{otherlanguage*}{greek}
	>Ae\`{i} <o je\`{o}s gewmetre~i.
\end{otherlanguage*}
}{Platone}
\section{Prodotto scalare, norma, distanza}
Per semplicità, considereremo solamente spazi vettoriali su $\mathbb{R}$.

Si ricorda che lo spazio $\mathbb{R}^n$ è definito come il prodotto cartesiano di $\mathbb{R}$ con se stesso $n$ volte, cioè
\begin{equation}
	\mathbb{R}^n = \underbrace{\mathbb{R}\times \mathbb{R} \times \dots \times \mathbb{R}}_{n \text{ volte}}.
\end{equation}
Un elemento $x \in \mathbb{R}^n$ (\emph{punto} o \emph{vettore}) è una $n$-upla ordinata $(x_1, x_2, \dots, x_n)$.

\subsection{Prodotto scalare}
S'introduce l'applicazione binaria $(\ ,\ )\colon\mathbb{R}^n\times\mathbb{R}^n \to \mathbb{R}$ che associa ad una coppia $(x,y)$, con $x,y\in\mathbb{R}^n$, il valore
\begin{equation}
	\label{eqn:prodscal}
	x\cdot y = \sum_{i=1}^nx_iy_i.
\end{equation}
Il prodotto $x\cdot y$ è detto \emph{prodotto scalare} di $x$ e $y$, e ha le seguenti proprietà:
\begin{enumerate}[label=$\mathcal{S}$\arabic*.]
	\item \label{s1} $x\cdot x \ge 0 \quad \forall x\in \mathbb{R}^n$; $\ x\cdot x = 0 \iff x = 0$ (definita positività);
	\item \label{s2} $x\cdot y = y \cdot x \quad \forall x, y \in \mathbb{R}^n$ (simmetria);
	\item \label{s3} $(\alpha x + \beta y, z) = \alpha(x, z) + \beta(y, z) \quad \forall x,y,z \in \mathbb{R}^n,\ \forall \alpha, \beta \in \mathbb{R}$ (bilinearità).
\end{enumerate}
\proof La dimostrazione è banale in tutti e tre i casi (si ricava facilmente dalla definizione).
\endproof

\begin{defn}
	Se $X$ è uno spazio vettoriale su $\mathbb{R}$, si dice \emph{prodotto scalare} un'operazione $(\ , \ )\colon X \times X \to \mathbb{R}$ con le proprietà suddette. Uno spazio dotato di prodotto scalare è detto \emph{pre-hilbertiano}.
\end{defn}

\subsection{Norma}
Il prodotto scalare introdotto in~\eqref{eqn:prodscal} induce una norma in $\mathbb{R}^n$ (più in generale, in $X$); si definisce l'applicazione \emph{norma}
\begin{equation}
	\label{eqn:norma}
	\norma{\ }\colon \mathbb{R}^n \ni x \mapsto \sqrt{x\cdot x} = \sqrt{\sum_{i=1}^n x_i^2}.
\end{equation}

È facile dimostrare che l'applicazione definita in~\eqref{eqn:norma} gode delle seguenti proprietà:
\begin{enumerate}[label=$\mathcal{N}$\arabic*.]
	\item \label{n1} $\norma{x} \ge 0 \quad \forall x \in \mathbb{R}^n; \ \norma{x} = 0 \iff x = 0$;
	\item \label{n2} $\norma{\lambda x} = \abs{\lambda}\, \norma{x} \quad \forall x \in \mathbb{R}^n, \ \forall \lambda \in \mathbb{R}$;
	\item \label{n3} $\norma{x+y} \le \norma{x} + \norma{y} \quad \forall x,y \in \mathbb{R}^n$ (disuguaglianza triangolare).
\end{enumerate}

\proof
La dimostrazione di~\ref{n1} e~\ref{n2} è banale e segue dalla definizione. La~\ref{n3} verrà provata dopo l'enunciazione del seguente teorema.

\begin{teor}[Disuguaglianza di Cauchy-Schwarz]
	\label{cauchy-schwarz}
	Siano $x, y \in \mathbb{R}^n$; allora
	\begin{equation}
		\label{eqn:cauchy-schwarz}
		\abs{(x,y)} \le \norma{x} \cdot \norma{y},
	\end{equation}
	ove l'uguaglianza è vera se $x = 0$ (o $y = 0$) oppure se $\exists \lambda \in \mathbb{R}$ tale che $x = \lambda y$.
\end{teor}

\proof
Se uno fra $x$ e $y$ è nullo la~\eqref{eqn:cauchy-schwarz} è ovvia. Sia $\lambda \in \mathbb{R}$; si ha
\begin{equation*}
	\begin{split}
		0 \le \norma{x + \lambda y}^2 &= (x + \lambda y, x + \lambda y) = (x, x) + 2 \lambda (x, y) + \lambda^2 (y, y) = \\ &= \norma{x}^2 + 2 \lambda(x, y) + \lambda^2 \norma{y}^2.
	\end{split}
\end{equation*}
Ponendo $\lambda = -(x,y)/\norma{y}^2$ si ha:
\begin{equation*}
	\norma{x}^2 - \frac{2(x,y)^2}{\norma{y}^2}+ \frac{(x,y)^2}{y^2} = \norma{x}^2 - \frac{(x,y)^2}{\norma{y}^2}.
\end{equation*}
Si ricava quindi:
\begin{equation*}
	\norma{x}^2\norma{y}^2 \ge (x, y)^2,
\end{equation*}
ed essendo ambo i membri maggiori o uguali a zero estraendo la radice quadrata si ritrova la~\eqref{eqn:cauchy-schwarz} (e facilmente si osserva che vale l'uguaglianza solo se uno dei due vettori è nullo o uno è multiplo dell'altro).
\endproof

\proof[Dimostrazione di~\ref{n3}]
Si ha:
\begin{equation*}
	\norma{x+y}^2 = \norma{x}^2 + 2(x,y) + \norma{y}^2 \overset{\eqref{eqn:cauchy-schwarz}}{\le} \norma{x}^2 + 2\norma{x}\cdot\norma{y} + \norma{y}^2 = (\norma{x}+\norma{y})^2
\end{equation*}
per ogni $x, y \in \mathbb{R}^n$; da ciò segue immediatamente la~\ref{n3} poiché $\norma{x+y} \ge 0$ e $\norma{x} + \norma{y} \ge 0$.
\endproof

\begin{defn}
	Sia $X$ uno spazio vettoriale su $\mathbb{R}$; è detta \emph{norma} una funzione $\norma{\cdot}\colon X\to \mathbb{R}$ che soddisfi le proprietà~\ref{n1},~\ref{n2},~\ref{n3}; si dice che $(X, \norma{\cdot})$ è uno spazio normato.
\end{defn}

\begin{oss}
	Uno spazio pre-hilbertiano è anche normato (con la norma naturalmente indotta dal prodotto scalare).
\end{oss}

Una puntualizzazione: non è vero neanche per $\mathbb{R}^n$ dotato della norma euclidea che $\norma{x-y} \le \norma{x} - \norma{y}$; si pensi al caso $n=1,\,x=0$ e $y=1$. Valgono invece le seguenti disuguaglianze, di facile dimostrazione:
\begin{enumerate}[label = \roman*)]
	\item $\norma{x-y} \le \norma{x} + \norma{y} \quad \forall x, y \in X$;
	\item $\norma{x-y} \ge \abs{\,\norma{x}-\norma{y}\,} \quad \forall x,y \in X.$
\end{enumerate}

Se $X$ è uno spazio pre-hilbertiano con prodotto scalare $(\ , \ )$ si ha:
\begin{equation}
	\label{eqn:par1}
	\norma{x+y}^2 = (x+y, x+y) = \norma{x}^2 + 2(x,y) + \norma{y}^2 \quad \forall x, y \in X;
\end{equation}
\begin{equation}
	\label{eqn:par2}
	\norma{x-y}^2 = \norma{x} - 2(x, y) + \norma{y}^2 \quad \forall x, y \in X.
\end{equation}
Sommando membro a membro~\eqref{eqn:par1} e~\eqref{eqn:par2}
si ottiene
\begin{equation}
	\label{eqn:par3}
	\norma{x+y}^2 + \norma{x-y}^2 = 2(\norma{x}^2 + \norma{y}^2) \quad \forall x,y \in X,
\end{equation}
che nel caso $X = \mathbb{R}^2$ corrisponde alla regola per cui in un parallelogramma la somma delle lunghezze al quadrato è uguale al doppio della somma dei quadrati costruiti sui lati.

\begin{exmp}
	Si consideri lo spazio $X = C_b (I)$ delle funzioni $f\colon I \subseteq \mathbb{R}\to \mathbb{R}$ continue e limitate in un intervallo $I$ di $\mathbb{R}$ con la norma
	\begin{equation*}
		\norma{f}_\infty \coloneqq \sup_{x\in I}\abs{f(x)}.
	\end{equation*}
	Si dimostra facilmente che $\norma{\cdot}_\infty$ è effettivamente una norma in $X$ e che non vale la~\eqref{eqn:par3}; ne segue che uno spazio normato non è necessariamente pre-hilbertiano (si può anche provare che la~\eqref{eqn:par3} caratterizza gli spazi normati la cui norma è indotta da un prodotto scalare).
\end{exmp}

\subsection{Distanza}
A questo punto, si può introdurre un'applicazione detta distanza che associa a due vettori di $\mathbb{R}^n$ il numero reale
\begin{equation}
	\label{distanza}
	d(x, y) = \norma{x - y}\quad \forall x, y \in \mathbb{R}^n.
\end{equation}

È facile verificare che valgono le seguenti proprietà:
\begin{enumerate}[label=$\mathcal{D}$\arabic*.]
	\item \label{d1} $d(x, y) \ge 0 \quad \forall x, y \in \mathbb{R}^n; \ d(x, y) = 0 \iff x = y;$
	\item \label{d2} $d(x, y) = d(y, x)$ (simmetria);
	\item \label{d3} $d(x, y) \le d(x, z) + d(z, y) \quad \forall x, y, z \in \mathbb{R}^n$ (disuguaglianza triangolare).
\end{enumerate}

\begin{defn}
	Se $X$ è un insieme e $d\colon X\times X \to \mathbb{R}$ è un'applicazione che soddisfa~\ref{d1},~\ref{d2} e~\ref{d3}, allora $(X, d)$ è detto \emph{spazio metrico}, e $d$ una \emph{metrica} o \emph{distanza} in $X$.
\end{defn}

La distanza definita in~\eqref{distanza}, cioè
\begin{equation*}
	d(x, y) = \norma{x - y} = \biggl( \sum_{i=1}^n (x_i - y_i)^2 \biggr) ^{1/2},
\end{equation*}
è detta \emph{distanza euclidea}, e la norma che la induce \emph{norma euclidea}.

\subsection{Ortogonalità}
Si consideri uno spazio pre-hilbertiano $(X, (\ , \ )) $ con la norma $\norma{\cdot}$ naturalmente indotta dal prodotto scalare. Se $x, y \in \mathbb{R}^n$ sono entrambi diversi da 0, da~\eqref{eqn:cauchy-schwarz} segue che
\begin{equation*}
	\abs[\bigg]{\frac{(x, y)}{\norma{x}\, \norma{y}}} \le 1.
\end{equation*}

Consideriamo adesso $X = \mathbb{R}^n$ con $\norma{\cdot}$ norma euclidea.
\begin{defn}
	Dati $x, y \in \mathbb{R}^n,$ entrambi diversi da zero, si definisce per $\theta \in [0, \pi]$
	\begin{equation*}
		\cos{\theta} \coloneqq \frac{(x,y)}{\norma{x}\, \norma{y}}.
	\end{equation*}
\end{defn}

Moltiplicando ambo i membri per $\norma{x}\, \norma{y}$ e leggendo da destra a sinistra si ottiene $(x,y) = \norma{x}\, \norma{y} \, \cos{\theta}$, in accordo con la definizione classica usata in Fisica.

\begin{defn}
	Due vettori $x$ e $y$ si dicono \emph{ortogonali} se $(x,y) = 0$.
\end{defn}

Si noti che la formula di Carnot diviene il teorema di Pitagora nel caso di vettori ortogonali in $\mathbb{R}^n$:
\begin{equation*}
	\norma{x+y}^2 = \norma{x}^2 + 2xy + \norma{y}^2 = \norma{x}^2 + \norma{y}^2 \quad \forall x, y \in X.
\end{equation*}

\section{Topologia}
\subsection{Intorni sferici}
\begin{defn}
	Dato uno spazio metrico $(X, d)$ con la metrica $d$, per ogni elemento $x_0 \in X$ e $r > 0$ l'insieme
	\begin{equation*}
		B(x_0, r) = \{x\in X \colon d(x, x_0) < r \}
	\end{equation*}
	è detto \emph{intorno sferico} o \emph{palla} di centro $x_0$ e raggio $r$.
\end{defn}

\begin{exmp}
	Nel caso di $\mathbb{R}^n$ dotato della metrica euclidea, gli intorni sferici sono:
	\begin{itemize}
		\item intervalli del tipo $(x_0 - r, x_0+r)$ per $n=1$;
		\item dischi di centro $x_0$ e raggio $r$ (privati del bordo) per $n=2$;
		\item palle di centro $x_0$ e raggio $r$ private della superficie per $n=3$.
	\end{itemize}
\end{exmp}



\begin{oss}
	In $\mathbb{R}^n$ possono essere introdotte metriche differenti, ad esempio quelle indotte da
	\begin{equation}
		\label{diffnorma}
		\norma{x} = \sum_{i=1}^n\abs{x_i}, \quad \forall x\in \mathbb{R}^n
	\end{equation}
	oppure da
	\begin{equation}
		\label{diffnorma2}
		\norma{x}= \max_i \abs{x_i}, \quad \forall x\in \mathbb{R}^n.
	\end{equation}
	È bene notare che in questi casi, a dispetto del nome, gli intorni sferici non hanno l'aspetto di dischi o sfere, come evidenziato in \figurename~\ref{fig:intornosferico}.
\end{oss}


\begin{figure}[ht]
	\centering
	\captionsetup{justification=centering}
	\subfloat[][\emph{Intorno sferico; la distanza è indotta da~\eqref{diffnorma}}]{
	\begin{tikzpicture}
		\begin{axis}[axis x line = center, axis y line = center, xmin = -2, xmax = 2, ymin = -2, ymax = 2, grid = major, xlabel = $x$, ylabel = $y$, scale = 0.82, axis equal image, ticklabel style = {font =\small}]
			\addplot [color = BrickRed, dashed, thick, fill = BrickRed, fill opacity = 0.2] coordinates
			{(1,0) (0, 1) (-1, 0) (0, -1) (1, 0)};
		\end{axis}
	\end{tikzpicture}
	} ~\quad \subfloat[][\emph{Intorno sferico; la distanza è indotta da~\eqref{diffnorma2}}]{
	\begin{tikzpicture}
		\begin{axis}[axis x line = center, axis y line = center, xmin = -2, xmax = 2, ymin = -2, ymax = 2, grid = major, xlabel = $x$, ylabel = $y$, scale = 0.82, axis equal image, ticklabel style = {font =\small}]
			\addplot [color = BrickRed, dashed, thick, fill = BrickRed, fill opacity = 0.2] coordinates
			{(1,1) (1, -1) (-1, -1) (-1, 1) (1, 1)};
		\end{axis}
	\end{tikzpicture}
	}
	\caption{Intorni sferici relativi a distanze indotte da norme diverse da quella euclidea; in entrambi i casi $n=2$ e $r=1$.}
	\label{fig:intornosferico}
\end{figure}

\subsection{Definizioni di base}
\begin{defn}
	Sia $D\in \mathbb{R}^n$. Si dice che $x_0 \in \mathbb{R}^n$ è \emph{interno} a $D$ se esiste un intorno sferico di $x_0$ interamente contenuto in $D$ (intrinsecamente, $x_0 \in D$).
\end{defn}
\begin{exmp}
	($n=1$) Se $D=(-1, +1]=\{x\in \mathbb{R}\colon -1 < x \le 1 \}$, tutti i suoi punti sono interni con l'eccezione di $1$.
\end{exmp}
\begin{exmp}
	($n=2$) Se $E= \dom\,f(x,y)$, ove $f(x,y) = \sqrt{x-y}$, si ha che $E = \{ (x,y) \in \mathbb{R}^2 \colon x- y \ge 0\}$, cioè i punti interni di $E$ sono tutti e soli quelli descritti da $y-x$ (cfr. \figurename~\ref{fig:esinterni}).
\end{exmp}


\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
		\begin{axis} [xmin=-2,xmax=2,ymin=-2,ymax=2,grid=major,xlabel=$x$,ylabel=$y$, axis equal image, axis x line = center, axis y line = center, ticklabel style = {font =\small}]
			\node[rotate=45] at (1, 1.3)   (a) {$y=x$};
			\addplot[color = BrickRed, dashed, thick]{x};

			\addplot+[draw = none, name path =A, no marks] {x}; % actual curve
			\addplot+[draw=none,name path=B, no marks] {-2};     % “fictional” curve
			\addplot+[BrickRed, opacity = 0.2] fill between[of=A and B,soft clip={domain=-4:4}];

		\end{axis}
	\end{tikzpicture}
	\caption{Il dominio di $f(x, y) = \sqrt{x-y}$}
	\label{fig:esinterni}
\end{figure}


\begin{defn}
	L'insieme dei punti interni a $D\in \mathbb{R}^n$ è detto \emph{parte interna} o \emph{interno} di $D$ e si indica con il simbolo $\overset{\circ}{D}$.
\end{defn}

\begin{defn}
	Sia $D\in \mathbb{R}^n$. Si dice che $x_0 \in \mathbb{R}^n$ è \emph{esterno} a $D$ se esiste un intorno sferico di $x_0$ interamente contenuto nel complementare $D^C$ di $D$.
\end{defn}

\begin{defn}
	Sia $D\in \mathbb{R}^n$. Si dice che $x_0 \in \mathbb{R}^n$ è \emph{di frontiera} per $D$ se non è né interno né esterno a $D$. L'insieme dei punti di frontiera per $D$ è detto \emph{frontiera} o \emph{bordo} di $D$; si indica col simbolo $\partial D$.
\end{defn}

\begin{oss}
	Tutte le definizioni date sono pertinenti al caso di un generico spazio metrico $(X, d)$.
\end{oss}

\begin{defn}
	Dato $D \in \mathbb{R}^n$, si dice che $x_0 \in \mathbb{R}^n$ è \emph{di accumulazione} per $D$ se in ogni intorno di $x_0$ cadono punti di $D$ diversi da $x_0$ (di conseguenza, tali punti sono infiniti).
\end{defn}

\begin{exmp}
	Se $I = (a, b)$, con $a<b$, allora $a$ e $b$ sono di accumulazione; i punti di $I$ sono tutti interni e in particolare di accumulazione. Infatti per ogni $r > 0$ si ha che $(b-r, b+r) \cap I = (\max \{ a, b-r \}, b)$.
\end{exmp}

\begin{defn}
	Sia $(X, d)$ uno spazio metrico e sia $A \subseteq X$. Si dice che $A$ è aperto se ogni suo punto è interno; in simboli,
	\begin{equation*}
		\forall x_0 \in A \ \exists R > 0\colon B(x_0, R) \subseteq A.
	\end{equation*}
\end{defn}

\begin{exmp}
	L'insieme $A = \{(x, y) \in \mathbb{R}^2 \colon y < x \}$ raffigurato in \figurename~\ref{fig:esinterni} è un insieme aperto.
\end{exmp}

\subsection{Limiti di successioni e funzioni tra spazi metrici}
\begin{defn}
	Dati uno spazio metrico $(X, d)$ e una successione $\{x_n\}_{n\in \mathbb{N}}$ a valori in $X$, si dice che il limite di $x_n$ è $x_0 \in X$ e si scrive
	\begin{equation*}
		\lim _{n\to \infty} x_n = x_0
	\end{equation*}
	(oppure $x_n \to x_0$) se
	\begin{equation*}
		\forall \varepsilon > 0 \ \exists \nu \in \mathbb{N}\colon n \in \mathbb{N}, n > \nu \implies d(x_n, x_0) < \varepsilon.
	\end{equation*}
\end{defn}

\begin{defn}
	Siano $(X, d_X)$ e $(Y, d_Y)$ due spazi metrici e $f\colon D\subseteq X \to Y$ una funzione. Sia $x_0 \in X$ un punto di accumulazione per $D$. Si dice che $f(x)$ ha limite $L \in Y$ per $x \to x_0$, e si scrive
	\begin{equation*}
		\lim_{x\to x_0} f(x) = L
	\end{equation*}
	(oppure $ f(x) \to L$ per $x \to x_0$), se
	\begin{equation*}
		\forall \varepsilon > 0 \ \exists \delta > 0 \colon x \in D, 0 < d_X(x, x_0) < \delta \implies d_Y(f(x), L) < \varepsilon.
	\end{equation*}
\end{defn}

Consideriamo adesso il caso $X=\mathbb{R}$, $Y=\mathbb{R}^n$, $n>1$, $d_X$ e $d_Y$ metriche euclidee.

\begin{defn}
	Sia $I \subseteq \mathbb{R}$ un intervallo e $r\colon I \to \mathbb{R}^n$ un'applicazione a valori vettoriali. Si dice che $r$ è \emph{continua} in $t_0 \in I$ se
	\begin{equation*}
		\lim _{t\to t_0} r(t) = r(t_0).
	\end{equation*}
	Se $r(t) = (r_1(t), r_2(t),\dots, r_n(t))$ dev'essere
	\begin{equation*}
		\forall \varepsilon > 0 \ \exists \delta > 0 \colon t \in I, \abs{t - t_0} < \delta \implies \norma{r(t) - r(t_0)} < \varepsilon,
	\end{equation*}
	ovvero
	\begin{equation*}
		\forall \varepsilon > 0 \exists \delta > 0 \colon t\in I, \abs{t- t_0} < \delta \implies \sqrt{\sum_{i=1}^n (r_i(t) - r_i(t_0))^2} < \varepsilon.
	\end{equation*}
\end{defn}
\chapter{Funzioni di più variabili}

\section{Funzioni da $\mathbb{R}^n$ in $\mathbb{R}$}

\subsection{Limiti e continuità in $\mathbb{R}^n$}

\begin{defn}
	Sia $f \colon \mathbb{R}^n \supseteq X \to \mathbb{R}$ e $\vec{x}_0 \in \overset{\cdot}{\mathbb{R}^n}$ un punto di accumulazione per $X$; sia inoltre $l$ un elemento di $\mathbb{R}*$ oppure di $\overset{\cdot}{\mathbb{R}}$. Diremo che il limite di $f(\vec{x})$ per $\vec{x}$ che tende a $\vec{x}_0$ è $l$ e scriveremo
		\begin{equation*}
			\lim_{\vec{x}\to\vec{x}_0}f(\vec{x}) = l,
		\end{equation*}
	oppure $f(\vec{}) \to l$ per $\vec{x} \to \vec{x}_0$, se, per ogni intorno $V$ di $l$, è possibile trovare un intorno $U$ di $\vec{x}_0$ tale che $f(\vec{x}) \in V$ se $\vec{x}_0 \ne \vec{x} \in U \cap X$.
\end{defn}

Al caso di funzioni di più variabili si estendono, oltre al teorema di unicità del limite, tutti i teoremi del calcolo dei limiti, sia quelli che riguardando l'ordinamento dell'insieme immagine (permanenza del segno e confronto), sia quelli che riguardando le operazioni algebriche.

\begin{oss}
	Nonostante la struttura della definizione di limite rimanga invariata, il calcolo dei limiti nel caso di funzioni di più variabili è in generale piuttosto complicato.

	Infatti, mentre in $\mathbb{R}*$ (o in $\overset{\cdot}{\mathbb{R}}$) $x$ tende a un punto $x_0$ lunga una direzione fissata, in $\overset{\cdot}{\mathbb{R}}^n$ $\vec{x}$ può tendere a $\vec{x}_0$ con gli $n$ gradi di libertà che lo spazio gli consente; la definizione richiede che $f(\vec{x})$ tenda a $l$ indipendentemente da come $\vec{x}$ si avvicina a $\vec{x}_0$.
\end{oss}

In $\mathbb{R}^2$, il calcolo dei limiti si può semplificare con l'uso delle coordinate polari. Indichiamo con $x, y$ le coordinate cartesiane di un punto di $\mathbb{R}^2$ e con $\rho, \theta$ le sue coordinate polari.

Sia $(x_0, y_0) \in \mathbb{R}^2$; poniamo
	\begin{equation*}
		x = x_0 + \rho \cos \theta, \quad y = y_0 + \rho \sin \theta.
	\end{equation*}
Si osserva che $(x, y) \to (x_0, y_0)$ equivale a $\rho \to 0_+$ indipendentemente da $\theta$. Precisamente si avrà
	\begin{equation*}
		\lim_{(x, y) \to (x_0, y_0)}f(x, y) = l
	\end{equation*}
se e solo se, posto
	\begin{equation*}
		\bar{f}(\rho, \theta) = f(x_0 + \rho \cos \theta, y_0 + \rho \sin \theta),
	\end{equation*}
risulta che, per ogni $\epsilon > 0$, esiste $\delta = \delta(\epsilon)$ tale che
	\begin{equation*}
		\abs{\bar{f}(\rho, \theta) - l} < \epsilon
	\end{equation*}
per $0 < \rho < \delta$ e $\theta \in \mathbb{R}$.

In questa situazione, si di che $f(\rho, \theta) \to l$ per $\rho \to 0$ uniformemente rispetto a $\theta$.

Per quanto riguarda la continuità, la definizione è (con le dovute modifiche) uguale a quella nel caso di funzioni di una variabile; analogamente si estendono le proprietà riguardanti la continuità di somma, prodotto e composizione di funzioni continue. Ci limitiamo dunque a fornire i principali risultati nei casi di funzioni continue su compatti e su connessi (particolarmente importanti in quanto compattezza e connessione sono proprietà che vengono preservate da una trasformazione continua).

\subsubsection*{Funzioni continue su un compatto}
\begin{defn}
	Un insieme $K \subset \mathbb{R}^n$  si dice compatto per successioni (o sequenzialmente compatto) se da ogni successione a valori in $K$ si può estrarre una sottosuccessione convergente a un elemento di $K$.
\end{defn}
\begin{teor}
	$K \in \mathbb{R}^n$ è compatto se e solo se è sequenzialmente compatto.
\end{teor}

\begin{teor}
	Sia $K \subset \mathbb{R}^n$ compatto e $\vec{f} \colon K \to \mathbb{R}^m$ continua. Allora $\vec{f}(K)$ è compatto.
\end{teor}

\begin{cor}[Teorema di Weierstrass]
	Sia $K \subset \mathbb{R}^n$ compatto e $\vec{f} \colon K \to \mathbb{R}$ continua. Allora $f$ ammette massimo e minimo.
\end{cor}

\begin{teor}
	Sia $K \subset \mathbb{R}^n$ compatto e $\vec{f} \colon K \to \mathbb{R}^m$ iniettiva e continua. Allora $\vec{f}^{-1} \colon \vec{f}(K) \to K$ è continua.
\end{teor}

Anche la definizione di uniforme continuità si estende al caso di più variabili: diremo che $\vec{f} \colon \mathbb{R}^n \supseteq X \to \mathbb{R}^m$ è uniformemente continua in $X$ se, $\forall \epsilon > 0$, $\exists \delta = \delta(\epsilon) > 0$ tale che, per ogni coppia di punti $\vec{x}, \vec{z} \in X$ con $\norma{\vec{x} - \vec{z}} < \delta$, risulti $\norma{\vec{f}(\vec{x})-\vec{f}(\vec{z})} < \epsilon$.

\begin{teor}[di Cantor-Heine]
	Sia $K \subset \mathbb{R}^n$ compatto e $\vec{f} \colon K \to \mathbb{R}^m$ continua. Allora $\vec{f}$ è uniformemente continua.
\end{teor}

\subsubsection*{Funzioni continue su un connesso}
\begin{teor}
	Sia $E \subseteq \mathbb{R}^n$ connesso e $\vec{f}\colon E \to \mathbb{R}^m$ continua. Allora $\vec{f}(E)$ è connesso.
\end{teor}

\begin{teor}
	Sia $I \subseteq \mathbb{R}$ un intervallo e $f \colon I \to \mathbb{R}$ continua e iniettiva. Allora $f^{-1}\colon f(I) \to I$ è continua. In particolare, se $I$ è aperto (chiuso) allora $f(I)$ è aperto (chiuso).
\end{teor}

Se due insiemi $X$ e $Y$ si possono porre in corrispondenza biunivoca tramite una funzione $f$ bicontinua (tale cioè che sia $f$ che $f^{-1}$ sono continue) essi si dicono \emph{omeomorfi} e $f$ (o $f^{-1}$) si chiama \emph{omeomorfismo} tra $X$ e $Y$.




\subsection{Derivate direzionali e parziali}

Siano $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$, $A$ aperto di $\mathbb{R}^n$, $\vec{x} \in A$; introduciamo una \emph{direzione}, cioè un versore $\vec{v} \in \mathbb{R}^n$ tale che $\norma{\vec{v}} = 1$. Consideriamo, per ogni $t \in \mathbb{R}$ tale che $\vec{x} + t\vec{v} \in A$, il \emph{rapporto incrementale di $f$ nella direzione $\vec{v}$}:

\begin{equation*}
	\frac{f(\vec{x} + t\vec{v}) - f(\vec{x})}{t}.
\end{equation*}

\begin{defn}
	Quando esiste finito, il

	\begin{equation}
		\lim_{t \to 0}\frac{f(\vec{x} + t\vec{v} - f(\vec{x})}{t}
	\end{equation}
	si chiama derivata nella direzione $\vec{v}$ di $f$ in $\vec{x}$ e si indica con $D_{\vec{v}}f(\vec{x})$; $f$ si dice derivabile nella direzione $\vec{v}$ in $\vec{x}$.
\end{defn}

Le derivate lungo i versori della base canonica $\vec{e}_1, \vec{e}_2, \dots, \vec{e}_n$ si chiamano \emph{derivate parziali} e si indicano con i simboli

\begin{equation*}
	\frac{\partial f}{\partial x_j}, \quad D_j f, \quad D_{x_j} f, \quad \partial_{x_j} f, \quad f_{x_j}.
\end{equation*}

Nella derivata parziale rispetto a $x_j$ viene incrementata solo quella variabile; dunque per il calcolo di $f_{x_j}$ si può pensare alle altre variabili come costanti e utilizzare le classiche regole di derivazioni per funzioni di una variabile.

Se una funzione $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$ ammette $n$ derivate parziali in un punto $\vec{x} \in A$ è definito il vettore \emph{gradiente} di $f$ in $\vec{x}$, le cui componenti sono le $n$ derivate parziali di $f$ in $\vec{x}$ e che si indica con $\nabla f(\vec{x})$:
\begin{equation*}
	\nabla f(\vec{x}) \coloneqq (f_{x_1}(\vec{x}), f_{x_2}(\vec{x}), \dots, f_{x_n}(\vec{x})).
\end{equation*}

È importante notare che l'esistenza di tutte le derivate direzionali in un punto non implica la continuità in quel punto, in dimensione maggiore di $1$; è quindi necessario introdurre un concetto più forte di quello di derivabilità.

\subsection{Differenziale}

L'idea è quella di approssimare l'incremento $\Delta f = f(\vec{x} + \vec{h}) - f(\vec{x})$ con una funzione lineare in $\vec{h}$ a meno di infinitesimi di ordine superiore a $\norma{\vec{h}}$.

Ricordiamo che ogni funzione lineare $L \colon \mathbb{R}^n \to \mathbb{R}$ è identificata da un unico vettore $\vec{a} \in \mathbb{R}^n$, nel senso che
\begin{equation*}
	L(\vec{x}) = (\vec{a}, \vec{h})
\end{equation*}
per ogni $\vec{h} \in \mathbb{R}^n$.

\begin{defn}
	Sia $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$, $A$ aperto; $f$ si dice differenziabile in $\vec{x} \in A$ se esiste un vettore $\vec{a} \in \mathbb{R}^n$ tale che
	\begin{equation}
		\label{eqn:diff}
		f(\vec{x} + \vec{h}) - f(\vec{x}) = (\vec{a}, \vec{h}) + o(\norma{h})
	\end{equation}
	per $\norma{h} \to 0$, per ogni $\vec{h} \in \mathbb{R}^n$ con $\vec{x} + \vec{h} \in A$.

	L'applicazione lineare da $\mathbb{R}^n$ in $\mathbb{R}$ data da
	\begin{equation*}
		\vec{h} \mapsto (\vec{a}, \vec{h})
	\end{equation*}
	si chiama differenziale di $f$ in $\vec{x}$ e si indica col simbolo $df(\vec{x}). $
\end{defn}

\begin{teor}
	Sia $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$, $A$ aperto; se $f$ è differenziabile in $\vec{x} \in A$, allora:
	\begin{enumerate}[series = teorenum]
		\item $f$ è continua in $\vec{x}$;
		\item $f$ è derivabile in $\vec{x}$ lungo ogni direzione; in particolare esistono tutte le derivate parziali di $f$ in $\vec{x}$ e, se $\vec{a}$ è il vettore in~\eqref{eqn:diff}, si ha $\vec{a} = \nabla f(\vec{x})$. Inoltre vale la formula
		\begin{equation}
			\label{eqn:gradiente}
			D_{\vec{v}}f(\vec{x}) = (\nabla f(\vec{x}), \vec{v}).
		\end{equation}
	\end{enumerate}
\end{teor}

Si può dunque scrivere
\begin{equation}
	df(\vec{x})(\vec{h}) = (\nabla f(\vec{x}), \vec{h})
\end{equation}
per ogni $\vec{h} \in \mathbb{R}^n$.

La~\eqref{eqn:gradiente} permette di individuare le direzioni di massima e minima crescita di una funzione differenziabile. Si può scrivere infatti
\begin{equation*}
	D_{\vec{v}}f(\vec{x}) = \norma{\nabla f(\vec{x})} \cos{\beta},
\end{equation*}
ove $\beta$ è l'angolo formato dai vettori $\vec{v}$ e $\nabla f(\vec{x})$; ciò significa che $D_{\vec{v}}f(\vec{x})$ è massima quando $\beta = 0$ e minima quando $\beta = \pi$, quindi
\begin{equation*}
	\vec{v}_\textup{max} =\frac{\nabla f(\vec{x})}{\norma{\nabla f(\vec{x})}}, \quad \vec{v}_\textup{min} =- \frac{\nabla f(\vec{x})}{\norma{\nabla f(\vec{x})}};
\end{equation*}
in conclusione,
\begin{equation*}
	\max_{\norma{\vec{v}} = 1} D_{\vec{v}} f(\vec{x}) = \norma{\nabla f(\vec{x})}, \quad \min_{\norma{\vec{v}} = 1} D_{\vec{v}} f(\vec{x}) = - \norma{\nabla f(\vec{x})}.
\end{equation*}

L'aspetto geometrico della differenziabilità è legato all'esistenza del piano tangente.
Sia $f$ differenziabile in un punto $\vec{x}_0$; ponendo $\vec{h} = \vec{x} - \vec{x}_0$ scriviamo la~\eqref{eqn:diff} nella forma
\begin{equation}
	\label{eqn:iperpiano}
	f(\vec{x}) = f(\vec{x}_0) + (\nabla f(\vec{x}_0), \vec{x} - \vec{x}_0) + o(\norma{\vec{x} - \vec{x}_0}).
\end{equation}
La funzione $z =  f(\vec{x}_0) + (\nabla f(\vec{x}_0), \vec{x} - \vec{x}_0)$ ha come grafico un iperpiano e la~\eqref{eqn:iperpiano} equivale ad affermare che essa è la funzione lineare (affine) che meglio approssima $f$ in un intorno di $\vec{x}_0$; tale piano si chiama piano tangente.

In dimensione $2$, se $\vec{x}_0 = (x_0, y_0)$, $\vec{x} = (x, y)$ e $z_0 = f(x_0, y_0)$, la sua equazione si scrive esplicitamente come
\begin{equation}
	\label{eqn:pianotangente}
	z - z_0 - f_x(x_0, y_0)(x - x_0) - f_y(x_0, y_0)(y - y_0) = 0.
\end{equation}
La~\eqref{eqn:pianotangente} indica che il vettore $\vec{n} = (-f_x(x_0, y_0), -f_y(x_0, y_0)) \in \mathbb{R}^3$ è un vettore normale al piano tangente nel punto $P_0$ di coordinate $(x_0, y_0, z_0)$, dunque, per definizione, normale al grafico di $f$ nello stesso punto.

\begin{teor}
	Sia $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$, $A$ aperto; se in un intorno di $\vec{x} \in A$ esistono tutte le derivate parziali di $f$ ed esse sono continue in $\vec{x}$ allora $f$ è differenziabile in $\vec{x}$.
\end{teor}

La condizione è solo sufficiente: esistono funzioni differenziabili con derivata non continua, ad esempio
\begin{equation*}
	f(x) = \begin{dcases*}
	x^2 \sin{\frac{1}{x}} &se $x \ne 0$, \\
	0 &se $x = 0$.
\end{dcases*}
\end{equation*}
Dunque la classe delle funzioni differenziabili in un aperto $A$ contiene strettamente quella delle funzioni con derivate parziali continue in $A$ (differenziabili con continuità); quest'ultima classe di funzioni si indica col simbolo $C^1(A)$ ed è uno spazio vettoriale su $\mathbb{R}$. Il precedente Teorema si può enunciare nel seguente modo: \emph{se $f \in C^1(A)$, allora $f$ è differenziabile in A}.

\subsection{Derivate e differenziali di ordine superiore}

Sia $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$, $A$ aperto. Se, fissata la direzione $\vec{v} \in \mathbb{R}^n$, esiste $D_{\vec{v}}f$ in un intorno $U(\vec{x})$ di $\vec{x} \in A$, è definita la funzione
\begin{equation*}
	D_{\vec{v}}f \colon U(\vec{x}) \to \mathbb{R}.
\end{equation*}
Se $\vec{w} \in \mathbb{R}^n$ è un altro versore, è lecito chiedersi se esista $D_{\vec{w}} D_{\vec{v}} f(\vec{x})$, cioè la derivata seconda di $f$ lungo le direzioni $\vec{v}$ e $\vec{w}$ (nell'ordine), che si indica con $D_{\vec{w}\vec{v}}^2f(\vec{x})$.

Nel caso in cui $\vec{v} = \vec{e}_j$ e $\vec{w} = \vec{e}_k$ si ha la derivata parziale seconda rispetto a $x_j$ e $x_k$, indicata con uno dei seguenti simboli:
\begin{equation*}
	\frac{\partial^2 f}{\partial x_k x_j}(\vec{x}), \quad f_{x_k x_j}(\vec{x}), \quad D_{x_k x_j}f(\vec{x}), \quad D_{kj}^2f(\vec{x}), \quad \partial_{x_kx_j}f(\vec{x}).
\end{equation*}
Se $k\ne j$ le derivate si chiamano miste; se $k=j$ si chiamano pure e il primo simbolo si semplifica in
\begin{equation*}
	\frac{\partial^2 f}{\partial x_j^2}(\vec{x}).
\end{equation*}

In generale, non è vero che, per una funzione due volte derivabile lungo $\vec{v}$ e $\vec{w}$, $D^2_{\vec{w}\vec{v}}f = D^2_{\vec{v}\vec{w}}$; il prossimo teorema indica una condizione sufficiente per l'uguaglianza delle derivate miste.

\begin{teor}[di Schwarz]
	Se $f_{x_kx_j}$ e $f_{x_jx_k}$ esistono in un intorno di $\vec{x}$ e sono continue in $\vec{x}$ allora
	\begin{equation*}
		f_{x_kx_j}(\vec{x}) = f_{x_jx_k}(\vec{x}).
	\end{equation*}
\end{teor}

\begin{oss}
	Il Teorema vale per le derivate direzionali seconde qualunque, non solo per le derivate seconde miste. Inoltre, si può dimostrare che se $f_{x_k}$, $f_{x_j}$, $f_{x_kx_j}$ esistono in un intorno di $\vec{x}$ e $f_{x_kx_j}$ è continua in $\vec{x}$, allora esiste anche $f_{x_jx_k}(\vec{x})$ ed è uguale a $f_{x_kx_j}(\vec{x})$.
\end{oss}

In maniera del tutto analoga, si possono considerare derivate di ordine superiore.

Sia ora $f\colon \mathbb{R}^n \supseteq A \to \mathbb{R}$ differenziabile in $A$. Allora per ogni $\vec{x} \in A$ esistono le derivate parziali $f_{x_j}(\vec{x})$, $j = 1, \dots, n$; se queste derivate sono a loro volta differenziabili in $\vec{x}$ diremo che $f$ è due volte differenziabile in $\vec{x}$ e si chiama differenziale secondo di $f$ in $\vec{x}$ la forma quadratica nell'incremento $\vec{h} = (h_1, \dots, h_n)$ data da
\begin{equation*}
	d^2f(\vec{x}) \colon\! \vec{h} \mapsto \sum_{i,j = 1}^n f_{x_ix_j}(\vec{x})h_ih_j;
\end{equation*}
in altri termini,
\begin{equation}
	\label{eqn:diffsecondo}
	d^2f(\vec{x}) \coloneqq \sum_{i,j=1}^n f_{x_ix_j}(\vec{x})dx_idx_j.
\end{equation}

La matrice quadrata di ordine $n$ i cui elementi sono $f_{x_ix_j}(\vec{x})$ si chiama matrice hessiana di $f$ in $\vec{x}$ e si indica col simbolo $\mathbf{H}_f(\vec{x})$, cioè
\begin{equation*}
	\mathbf{H}_f(\vec{x}) \coloneqq \begin{pmatrix} f_{x_1x_1}(\vec{x}) & f_{x_1x_2}(\vec{x}) & \dots & f_{x_1x_n}(\vec{x}) \\
	f_{x_2x_1}(\vec{x}) & f_{x_2x2}(\vec{x}) & \dots & f_{x_2x_n}(\vec{x}) \\
	\vdots & \vdots & \ddots & \vdots \\
	f_{x_nx_1}(\vec{x}) & f_{x_nx_2}(\vec{x}) & \dots & f_{x_nx_n}(\vec{x})
	\end{pmatrix}.
\end{equation*}

Si può dunque scrivere
\begin{equation*}
	d^2f(\vec{x}) = (\mathbf{H}_f(\vec{x})d\vec{x}, d\vec{x}).\footnote{$\mathbf{H}_f(\vec{x})d\vec{x}$ indica il prodotto righe per colonne della matrice hessiana per il vettore $d\vec{x}$.}
\end{equation*}

Se $f$ è due volte differenziabile in $\vec{x}$ esistono le derivate $D_{\vec{v}\vec{w}}^2f(\vec{x})$ per ogni coppia di versori $\vec{v}, \vec{w} \in \mathbb{R}^n$; inoltre vale la formula

\begin{equation}
	D_{\vec{v}\vec{w}}^2f(\vec{x}) = \sum_{i,j=1}^nf_{x_ix_j}(\vec{x})v_iv_j = (\mathbf{H}_f(\vec{x})\vec{w}, \vec{v}).
\end{equation}

\begin{teor}
	Se $f$ è due volte differenziabile in $\vec{x}$, l'ordine di derivazione delle derivate miste è invertibile.
\end{teor}

Terminiamo il paragrafo menzionando un importante operatore differenziale, l'operatore di Laplace (o \emph{laplaciana}):
\begin{equation*}
	\Delta \colon \! f \mapsto \Delta f \coloneqq \frac{\partial^2 f}{\partial x_1^2} + \frac{\partial^2 f}{\partial x_2^2} + \dots + \frac{\partial^2 f}{\partial x_n^2}.
\end{equation*}
Le funzioni $f \in C^2(A)$ tali che $\Delta f = 0$ in $A$ si dicono armoniche.

\subsection{Formula di Taylor}

\begin{teor}[Formula di Taylor con resto di Lagrange]
	Sia $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$ e supponiamo che il segmento chiuso $[\vec{x}, \vec{x} + d\vec{x}]$ sia contenuto in $A$. Se $f$ è differenziabile con continuità $k-1$ volte nel segmento chiuso e $k$ volte nel segmento aperto, allora esiste $\theta \in (0, 1)$ tale che
	\begin{equation}
		\label{eqn:taylor}
		\begin{split}
			&f(\vec{x}+d\vec{x})-f(\vec{x}) = \\ &= df(\vec{x}) + \frac{1}{2}d^2f(\vec{x})+\dots+\frac{1}{(k-1)!}d^{k-1}f(\vec{x})+\frac{1}{k!}d^kf(\vec{x}+\theta d \vec{x}).
		\end{split}
	\end{equation}
\end{teor}

Il caso particolare $k=1$ nella~\eqref{eqn:taylor} è l'estensione al caso di funzioni reali di $n$ variabili del teorema del valor medio di Lagrange:
\begin{equation}
	\label{eqn:valmedlagrange}
	f(\vec{x}+d\vec{x})-f(\vec{x}l) = df(\vec{x}+\theta d \vec{x}),
\end{equation}
con $\theta \in (0, 1)$ opportuno.

Come nel caso unidimensionale, tramite la~\eqref{eqn:valmedlagrange} possiamo caratterizzare le funzioni costanti in un aperto connesso.

\begin{prop}
	Sia $A$ aperto connesso e $df(\vec{x}) = 0$ $\forall \vec{x} \in A$. Allora $f$ è costante.
\end{prop}

\begin{teor}[Formula di Taylor con resto di Peano]
	Sia $f\colon \mathbb{R}^n \supseteq A \to \mathbb{R}$, $k$ volte differenziabile in $\vec{x}$. Allora
	\begin{equation}
		f(\vec{x} + d\vec{x}) - f(\vec{x}) = df(\vec{x}) + \frac{1}{2}d^2f(\vec{x}) + \dots + \frac{1}{k!}d^k(\vec{x}) + o(\norma{d\vec{x}}^k)
	\end{equation}
	per $\norma{d\vec{x}} \to 0$.
\end{teor}


\subsection{Funzioni omogenee, funzioni convesse e concave}

Le funzioni positivamente omogenee di grado $\mu$, $\mu \in \mathbb{R}$ sono definite in coni con vertice in $\vec{0}$; cioè, se sono definite in un punto $\vec{x}$, allora sono definite su tutta la semiretta $\rho \vec{x}$ per ogni $\rho > 0$. Un cono non è necessariamente convesso e ovviamente può coincidere con tutto $\mathbb{R}^n$.
\begin{defn}
	Sia $\mathcal{C} \subseteq \mathbb{R}^n$ un cono e $f \colon \mathcal{C} \to \mathbb{R}$; $f$ si dice positivamente omogenea di grado $\mu$, $\mu \in \mathbb{R}$, se $\forall \vec{x} \in \mathcal{C}$ e $\forall \rho > 0$ risulta
	\begin{equation}
		f(\rho \vec{x}) = \rho^{\mu}f(\vec{x}).
	\end{equation}
\end{defn}

\begin{exmp}
	Qualunque polinomio omogeneo di grado $\mu$, $\mu \in \mathbb{N}$, in $k$ variabili è omogeneo dello stesso grado nel senso della definizione precedente.
\end{exmp}

\begin{exmp}
	La norma di un vettore $\vec{x} \in \mathbb{R}^n$ è positivamente omogenea di grado $1$.
\end{exmp}

Il seguente teorema caratterizza le funzioni positivamente omogenee differenziabili in insiemi aperti.

\begin{teor}[di Eulero]
	Sia $\mathcal{C} \subseteq \mathbb{R}^n$ un cono aperto e $f \colon \mathcal{C} \to \mathbb{R}$ differenziabile in $\mathcal{C}$. Allora $f$ è positivamente omogenea di grado $\mu$ se e solo se, per ogni $\vec{x} \in \mathcal{C}$, risulta
	\begin{equation}
		(\vec{x}, \nabla f(\vec{x})) = \mu f(\vec{x}).
	\end{equation}
\end{teor}

Sia $f \colon \mathbb{R}^n \supseteq A \to \mathbb{R}$. Indichiamo con $\Epi(f)$ l'epigrafico di $f$, cioè
\begin{equation*}
	\Epi(f) \coloneqq \{ (\vec{x}, z) \in \mathbb{R}^{n+1} \colon z \ge f(\vec{x}), \vec{x} \in A \}.
\end{equation*}

\begin{defn}
	Sia $f$ definita su un insieme convesso $A \subseteq \mathbb{R}^n$; $f$ si dice convessa in $A$ se $\Epi(f)$ è convesso in $\mathbb{R}^{n+1}$; $f$ si dice concava se $-f$ è convessa.
\end{defn}

Alternativamente, $f$ risulta convessa in $A$ se per ogni coppia di punti $\vec{x}, \vec{y} \in A$ e per ogni $t \in (0,1)$ risulta
\begin{equation}
	f(t\vec{y} + (1-t)\vec{x}) \le tf(\vec{y}) + (1-t)f(\vec{x}).
\end{equation}

\begin{teor}
	Sia $f$ convessa in $A$, aperto convesso in $\mathbb{R}^n$. Allora:
	\begin{enumerate}
		\item $f$ è continua in $A$;
		\item in ogni punto di $A$, $f$ ammette derivate parziali destre e sinistre;
		\item nei punti in cui esistono tutte le derivate parziali, $f$ è differenziabile.
	\end{enumerate}
\end{teor}

\begin{teor}
	Se $f$ è differenziabile in $A$ aperto convesso in $\mathbb{R}^n$, $f$ è convessa in $A$ se e solo se per ogni $\vec{x}, \vec{y} \in A$
	\begin{equation}
		\label{eqn:convessa}
		f(\vec{y}) \ge f(\vec{x})+df(\vec{x}).
	\end{equation}
\end{teor}

La~\eqref{eqn:convessa} scritta esplicitamente per una funzione di due variabili diviene
\begin{equation*}
	f(y_1, y_2) \ge f(x_1, x_2) + f_{x_1}(x_1, x_2)(y_1 - x_1) + f_{x_2}(x_1, x_2)(y_2 - x_2).
\end{equation*}

\begin{teor}
	Sia $f$ due volte differenziabile in $A$ aperto convesso di $\mathbb{R}^n$; $f$ è strettamente convessa in $A$ se, per ogni $\vec{x} \in A$, $d^2f(\vec{x})$ è una forma quadratica definita positiva (ovvero per ogni $d\vec{x} \ne \vec{0}$, $d^2f(\vec{x}) > 0$).
\end{teor}

Per le funzioni di due variabili, se $z = f(x_1, x_2)$ è due volte differenziabile è di sicuro strettamente convessa.

\section{Funzioni a valori vettoriali}

\subsection{Derivate e differenziali}
Sia $\vec{f} \colon \mathbb{R}^n \supseteq A \to \mathbb{R}^m$, $A$ aperto.

Per ogni $\vec{x} \in A$, $\vec{f}(\vec{x})$ è un vettore $(f_1(\vec{x}), f_2(\vec{x}), \dots, f_m(\vec{x}))$ le cui componenti $f_j$, $j=1, \dots, m$ sono funzioni da $A$ in $\mathbb{R}$.

Fissato un versore $\vec{v} \in \mathbb{R}^n$, $\vec{f}$ è derivabile lungo la direzione $\vec{v}$ nel punto $\vec{x}$ se e solo se esistono $D_{\vec{v}}f_j(\vec{x})$ per ogni $j = 1, \dots, m$ e
\begin{equation*}
	D_{\vec{v}}\vec{f} \coloneqq (D_{\vec{v}}f_1, D_{\vec{v}}f_2, dots, D_{\vec{v}}f_m).
\end{equation*}

\begin{defn}
	Si dice che $\vec{f}$ è differenziabile in $\vec{x}$ se esiste una matrice $\mathbf{M}$ di ordine $m \times n$ tale che
	\begin{equation}
		\vec{f}(\vec{x}+\vec{h}) - \vec{f}(\vec{x}) = \mathbf{M}\vec{h} + o(\norma{\vec{h}})
	\end{equation}
	per $\norma{\vec{h}} \to 0$, per ogni $\vec{h} \in \mathbb{R}^n$ con $\vec{x}+\vec{h} \in A$. L'applicazione lineare da $\mathbb{R}^n$ in $\mathbb{R}^m$ data da
	\begin{equation*}
		\vec{h} \mapsto \mathbf{M}\vec{h}
	\end{equation*}
	si chiama differenziale di $\vec{f}$ in $\vec{x}$ e si indica col simbolo $d\vec{f}(\vec{x})$.
\end{defn}

Si ha che
\begin{equation}
	\mathbf{M} = \begin{pmatrix}
	\nabla f_1 (\vec{x}) \\
	\nabla f_2 (\vec{x}) \\
	\vdots \\
	\nabla f_m (\vec{x})
\end{pmatrix} = \begin{pmatrix}
D_{x_1}f_1(\vec{x}) & D_{x_2}f_1(\vec{x}) & \dots & D_{x_n}f_1(\vec{x}) \\
D_{x_1}f_2(\vec{x}) & D_{x_2}f_2(\vec{x}) & \dots & D_{x_n}f_2(\vec{x}) \\
\vdots & \vdots & \ddots & \vdots \\
D_{x_1}f_m(\vec{x}) & D_{x_2}f_m(\vec{x}) & \dots & D_{x_n}f_m(\vec{x}) \\
\end{pmatrix}
\end{equation}

\begin{defn}
	Per i campi vettoriali derivaili è definito un operatore differenziale che si chiama \textit{divergenza} e si indica con "div" e che in coordinate cartesiane è assegnato dalla formula

	\begin{equation}
		div \vec{f} \coloneqq \frac{\partial{f_1}}{\partial{x_1}} + \frac{\partial{f_2}}{\partial{x_2}}+ ... + \frac{\partial{f_n}}{\partial{x_n}}
	\end{equation}
\end{defn}
\begin{defn}
	Ai i campi vettoriali f derivabili in $\mathbb{R}^3$ si può associare un altro vettore, il \textit{rotore} di $\vec{f}$, denotato $rot \vec{f}$, le  cui componenti in coordinate cartesiane sono le seguenti:
	\begin{equation}
		rot \vec{f} \coloneqq (\frac{\partial{f_3}}{\partial{x_2}}-\frac{\partial{f_2}}{\partial{x_3}},\frac{\partial{f_1}}{\partial{x_3}}-\frac{\partial{f_3}}{\partial{x_1}},\frac{\partial{f_2}}{\partial{x_1}}-\frac{\partial{f_1}}{\partial{x_2}})
	\end{equation}
\end{defn}

\begin{teor}(di inversione locale)
	Sia $\vec{f}:\mathbb{R}^n \supseteq  A \rightarrow  \mathbb{R}^n$, A aperto. Se:
	\begin{enumerate}
		\item $\vec{f} \in C^1(A)$;
		\item $\vec{x_0}$ è un punto di A tale che $\vec{J_f(x_0)}$ è non singolare;
		allora esistono un intorno V di $\vec{x_0}$ e un intorno W di $\vec{y_0} = \vec{f(x_0)}$ tali che:
		\begin{enumerate}
			\item $\vec{f}$ è una corrispondenza biunivoca tra $V$ e $W$;
			\item detta $\vec{g}:\rightarrow V$ la funzione inversa di $\vec{f}$ (ristretto a $V$), $\vec{g} \in C^1(W)$ e se $\vec{x} = \vec{g(y)}$, vale la formula
			\begin{equation}
				\vec{J_g(y)} = (\vec{J_f(x)})^{-1}
			\end{equation}
		\end{enumerate}
	\end{enumerate}
\end{teor}

\section{Funzioni implicite}

\begin{defn}
	Sia $g:\mathbb{R}^2 \supseteq A \rightarrow \mathbb{R}$; la funzione $y=f(x)$, $x \in I \subseteq \mathbb{R}$ (risp. $x = h(y)$), $y \in I \subseteq \mathbb{R}$) si dice definita implicitamente dall'equazione $g(x,y) = 0$ se
	\begin{equation}
		graf(f) \supset A
	\end{equation}
	e
	\begin{equation*}
		g(x,f(x)) = 0 \quad \forall x \in I \quad \quad (g(h(y)),y) = 0 \quad \forall x \in I)
	\end{equation*}
\end{defn}

\begin{teor} (di Dini) Sia $g:\mathbb{R}^2 \supseteq A \rightarrow \mathbb{R}$, $A$ aperto. Supponiamo che:
	\begin{enumerate}
		\item $g$ e $g_y$ siano continue in $A$;
		\item nel punto $(x_0,y_0) \in A$ si abbia $g(x_0,y_0) = 0$ e $g_y(x_0,y_0) \neq 0$. \\
		Allora esistono un intorno $U$ di $x_0$ e un'unica funzione $f : U \rightarrow \mathbb{R}$, continua in $U$, tale che $y_0 = f(x_0)$ e che $g(x,f(x)) = 0$ per ogni $x \in U$. Se Inoltre
		\item $g_x$ è continua in $A$ (dunque $g \in C^1(A)$), \\
		allora $f \in C^1(U)$ e vale la formula
	\end{enumerate}
	\begin{equation}
		f'(x) = - \frac{g_x(x,f(x))}{g_y(x,f(x))} \quad \forall x \in U.
	\end{equation}

\end{teor}









\chapter{Curve e integrali curvilinei}

\section{Curve in $\mathbb{R}^3$}
\subsection{Definizioni principali}
Sia $\gamma$ un sottoinsieme di $\mathbb{R}^3$ ed esista una funzione continua $\vec{r}\colon \!I \to \mathbb{R}^3$, dove $I \subseteq \mathbb{R}$ è un intervallo, di cui $\gamma$ è l'immagine; diremo che $\vec{r}$ è una parametrizzazione di $\gamma$.\footnote{È evidente che uno stesso insieme $\gamma$ può avere diverse parametrizzazioni.}

\begin{defn}
	Si dice curva in $\mathbb{R}^3$ un insieme $\gamma \subseteq \mathbb{R}^3$ (detto sostegno della curva) con una sua parametrizzazione $\vec{r}(t)$, $t \in I \subseteq \mathbb{R}$.
\end{defn}

Più esplicitamente, una parametrizzazione è assegnata mediante l'equazione
\begin{equation*}
	\vec{r}(t) = (x(t), y(t), z(t)),
\end{equation*}
oppure, in forma vettoriale,
\begin{equation*}
	\vec{r}(t) = x(t)\vec{i} + y(t)\vec{j} + z(t)\vec{k},
\end{equation*}
con $t \in I$.

Se $I = [a, b]$ e $\vec{r}(a) = \vec{r}(b)$ la curva si dice \emph{chiusa}; se $\vec{r}(t_1) \ne \vec{r}(t_2)$ per ogni $t_1, t_2 \in I$ con almeno uno fra $t_1$ e $t_2$ interni a $I$ la curva si dice \emph{semplice} (cioè una curva semplice non chiusa non ha autointersezioni). Se il sostegno $\gamma$ di una curva è contenuto in un piano la curva si dice \emph{piana}; si può assegnare una curva piana mediante una funzione continua $\vec{r} \colon \! I \to \mathbb{R}^2$.

Le curve piane, semplici e chiuse si chiamano \emph{curve di Jordan}; un importante teorema afferma che il sostegno di una curva di Jordan è frontiera di due aperti nel piano, uno limitato (\emph{interno} della curva) e uno illimitato (\emph{esterno} della curva).

Si noti che, dato che $t \in I \subseteq \mathbb{R}$, essendo $\mathbb{R}$ orientato è automaticamente assegnato su $\gamma$ un verso di percorrenza, ovvero un'orientazione della curva.

\begin{exmp}
	Sia $f \colon I \to \mathbb{R}$ una funzione reale di variabile reale, continua. Il suo grafico definisce una curva piana semplice di equazione $\vec{r}(t) = (t, f(t))$, detta \emph{curva cartesiana}.
\end{exmp}

\begin{exmp}
	L'equazione $g(x, y) = 0$, con $g$ di classe $C^1$, definisce in un intorno di ogni punto $(x_0, y_0)$ non singolare per $g$ (in cui cioè $\nabla g \ne \vec{0}$) una curva piana. Infatti, per il teorema di Dini, se $g_x(x_0, y_0) \ne 0$ ($g_y(x_0, y_0) \ne 0$), l'equazione $g(x, y) = 0$ definisce implicitamente una funzione $x = f(y)$ ($y = f(x)$) in un intorno di $y_0$ ($x_0$).
\end{exmp}

\begin{exmp}
	L'equazione $\rho = f(\theta)$, $\theta \in [\theta_0, \theta_1]$, dove $\rho$ e $\theta$ sono coordinate polari nel piano ed $f$ è continua, definisce una curva piana (in generale non semplice) mediante le equazioni parametriche
	\begin{equation*}
		x(\theta) = f(\theta)\cos\theta, \quad y(\theta) = f(\theta)\sin(\theta).
	\end{equation*}
\end{exmp}

\subsection{Curve regolari}
\begin{defn}
	Una curva $\gamma$ di equazione $\vec{r} = \vec{r}(t)$ si dice regolare se $\vec{r} \in C^1(I)$ e se $\vec{r}'(t) \ne \vec{0}$ per ogni $t \in \overset{\circ}{I}$. Si dice regolare a tratti se $I$ si può suddividere nell'unione di un numero finito di intervalli su ciascuno dei quali $\gamma$ è regolare.
\end{defn}

Per una curva regolare è ben definito e diverso da $\vec{0}$ il vettore tangente
\begin{equation*}
	\vec{r}'(t) = x'(t)\vec{i} + y'(t)\vec{j} + z'(t)\vec{k}.
\end{equation*}
La retta di equazioni parametriche
\begin{equation*}
	\begin{split}
		\xi &= x(t_0) + \alpha x'(t_0) \\
		\eta &= y(t_0) + \alpha y'(t_0) \\
		\zeta &= z(t_0) + \alpha z'(t_0)
	\end{split}
	\quad \alpha \in \mathbb{R}
\end{equation*}
o di equazione vettoriale
\begin{equation*}
	\vec{\xi}(\alpha) = \vec{r}(t_0) + \alpha \vec{r}'(t_0)
\end{equation*}
si chiama retta tangente alla curva nel punto $\vec{r}(t_0)$; per una curva piana cartesiana definita dalla funzione $y = f(x)$ le equazioni parametriche della retta tangente in un punto $(x_0, f(x_0))$ si riducono a
\begin{equation*}
	\begin{split}
		x &= x_0 + \alpha \\
		y &= f(x_0) + \alpha f'(x_0).
	\end{split}
\end{equation*}

Dal punto di vista cinematico, $\vec{r}'(t)$ rappresenta il vettore velocità, indicato anche con $\vec{v}(t)$. La velocità scalare $v(t)$ è definita come
\begin{equation*}
	v(t) \coloneqq \norma{\vec{r}'(t)} = \sqrt{x'(t)^2 + y'(t)^2 + z'(t)^2}.
\end{equation*}
Se la curva è regolare, $v(t) \ne 0$ per ogni $t \in I$. Risulta quindi ben definito il versore
\begin{equation*}
	\vec{T}(t) \coloneqq \frac{\vec{r}'(t)}{\norma{\vec{r}'(t)}} = \frac{\vec{v}(t)}{v(t)},
\end{equation*}
detto versore tangente.

\begin{exmp}
	Se $f \colon I \to \mathbb{R}$ è di classe $C^1$, la curva di equazione $\vec{r}(t) = t\vec{i} + f(t)\vec{j}$ è piana e regolare. Si ha:
	\begin{equation*}
		\vec{r}'(t) = \vec{i} + f'(t)\vec{j}, \quad v(t) = \sqrt{1 + f'(t)^2}.
	\end{equation*}
\end{exmp}

\begin{exmp}
	La curva di equazione polare $\rho = f(\theta)$, $\theta \in [\theta_0, \theta_1]$ è regolare se $f$ è di classe $C^1$ e $f'(\theta)^2 + f(\theta)^2 \ne 0$ per ogni $\theta$.
\end{exmp}

\subsection{Curve equivalenti}
Siano $(\gamma, \vec{r})$, $\vec{r} \colon \! I \to \mathbb{R}^3$ una curva regolare e $\phi \colon I_1 \to I$, con $I_1$ intervallo di $\mathbb{R}$, una funzione di classe $C^1(I_1)$ tale che $\phi'(\alpha) \ne 0$ per ogni $\alpha \in I_1$ e che realizzi una corrispondenza biunivoca tra $I_1$ e $I$. La funzione composta
\begin{equation*}
	\vec{r}_1(\alpha) = \vec{r} \circ \phi(\alpha) \colon \! I_1 \to \mathbb{R}^3
\end{equation*}
è una nuova parametrizzazione di $\gamma$.

Poiché $\vec{r}_1'(\alpha) = \vec{r}'(\phi(\alpha))\,\phi'(\alpha)$, la coppia $(\gamma, \vec{r}_1)$ è ancora una curva regolare.

Passando da $\vec{r}$ a $\vec{r}_1 = \vec{r} \circ \phi$, l'orientazione non muta se $\phi'(\alpha) > 0$ per ogni $\alpha \in I_1$, è opposta se $\phi'(\alpha) < 0$; due curve si dicono \emph{equivalenti} se possono essere ottenute l'una dall'altra con un cambio di parametro che non muti l'orientazione.

\subsection{Curve rettificabili, lunghezza di una curva}

Sia $\gamma$ una curva di equazione $\vec{r} \colon \! [a, b] \to \mathbb{R}^3$.

Fissiamo una suddivisione $\mathcal{D} = \{t_0 = a, t_!, \dots, t_{n-1}, t_n = b\}$ di $[a, b]$ e poniamo, per $j = 0, \dots, n$, $\vec{r}(t_j) = \vec{p}_j$; tali punti individuano una poligonale inscritta nella curva. La lunghezza della poligonale è:
\begin{equation*}
	l(\Gamma_{\mathcal{D}}) = \sum_{j= 0}^{n-1} \norma{\vec{p}_{j+1} - \vec{p}_j} = \sum_{j=0}^{n-1}\norma{\vec{r}(t_{j+1})-\vec{r}(t_j)}.
\end{equation*}
Sia ora $L \coloneqq \sup l(\Gamma_{\mathcal{D}})$, dove l'estremo superiore è cercato al variare di tutte le possibili suddivisioni di $[a, b]$.

\begin{defn}
	Se $L < +\infty$ si dice che la curva $(\gamma, \vec{r})$ è rettificabile e che $L$ è la sua lunghezza, indicata con $l(\gamma, \vec{r})$.
\end{defn}

\begin{teor}
	Se $\gamma$ è una curva regolare di equazione $\vec{r} \colon \![a, b] \to \mathbb{R}^3$, allora è rettificabile e vale la formula
	\begin{equation}
		\label{eqn:lunghezza}
		L = l(\gamma, \vec{r}) = \int_a^b \norma{\vec{r}'(t)}\, dt.
	\end{equation}
\end{teor}

\begin{exmp}
	Per le curve piane che sono grafico di una funzione $y = f(x)$, $x \in [a, b]$ la~\eqref{eqn:lunghezza} diventa
	\begin{equation}
		L = \int_a^b \sqrt{1 + f'(t)^2} \, dt.
	\end{equation}
\end{exmp}

\begin{exmp}
	Per una curva piana regolare di equazione polare $\rho = f(\theta)$, $\theta \in [\theta_1, \theta_2]$ la lunghezza si trova con la formula
	\begin{equation}
		L = \int_{\theta_1}^{\theta_2} \sqrt{f'(\theta)^2 + f(\theta)^2}\, dt.
	\end{equation}
\end{exmp}

È importante notare che la lunghezza è invariante per cambi di parametrizzazione; in particolare, è identica per curve equivalenti e non dipende dall'orientazione.

\begin{prop}
	Se le curve $\gamma_j$ per $j = 1, \dots, N$ sono rettificabili, anche $\gamma = \gamma_1 \cup \gamma_2 \cup \dots \cup \gamma_N$ è rettificabile e, se $\vec{r}$ è la parametrizzazione di $\gamma$,
	\begin{equation}
		l(\gamma, \vec{r}) = \sum_{j=1}^N l(\gamma_j, \vec{r}_j).
	\end{equation}
\end{prop}

\subsection{Ascissa curvilinea}

Sia $\vec{r} \colon \! [a, b] \to \mathbb{R}^3$ la parametrizzazione di una curva $\gamma$ regolare con lunghezza $L$. Per ogni $t \in [a, b]$ è definita la funzione
\begin{equation*}
	s(t) = \int_a^t v(u)\, du
\end{equation*}
che rappresenta cinematicamente lo spazio percorso al tempo $t$ partendo da $\vec{r}(a)$.

Per il teorema fondamentale del calcolo integrale, essendo $v$ continua in $[a, b]$, $s$ è derivabile e $s'(t) = v(t)$; poiché $v(t) \ne 0$ per ogni $t \in [a, b]$, $s$ risulta una funzione strettamente crescente e realizza una corrispondenza biunivoca tra $[a, b]$ e $[0, L]$. Anche la funzione inversa $t = t(s)$ è strettamente crescente e derivabile con derivata
\begin{equation*}
	\frac{dt}{ds} = \frac{1}{v(t)}
\end{equation*}
continua in $[0, L]$.

Segue che le curve di equazione $\vec{r} = \vec{r}(t)$ e $\vec{r_1} = \vec{r}(t(s))$ sono equivalenti; il parametro $s$ si chiama ascissa curvilinea (o lunghezza d'arco) e individua un sistema di coordinate ``intrinseco'' alla curva.

\section{Integrali curvilinei}
\subsection{Integrali curvilinei di prima specie}

Siano $f \colon \mathbb{R}^3 \supseteq E \to \mathbb{R}$, con $E$ aperto connesso, una funzione scalare e $\gamma \subset E$ una curva regolare a tratti di equazione $\vec{r} = \vec{r}(t)$, $t \in [a, b]$.

\begin{defn}
	L'integrale di $f$ rispetto alla lunghezza d'arco lungo $\gamma$ è definito dalla formula
	\begin{equation}
		\label{eqn:intcurv1}
		\int_{\gamma}f\, ds \coloneqq \int_a^b f \circ \vec{r}(t)s'(t)\, dt
	\end{equation}
	quando $f \circ \vec{r}(t)s'(t)$ è integrabile in $[a, b]$.
\end{defn}

Più esplicitamente, se $\vec{r}(t) = (x(t), y(t), z(t))$:
\begin{equation*}
	\int_{\gamma} f \, ds = \int_a^b f(x(t), y(t), z(t)) \sqrt{x'(t)^2 + y'(t)^2 + z'(t)^2} \, dt.
\end{equation*}

L'integrale~\eqref{eqn:intcurv1} è utile nel calcolo di baricentri e momenti d'inerzia (rispetto a un asse) di fili composti da materiali di cui si conosca la densità lineare di massa $\delta = \delta(x, y, z)$.

Se il filo coincide con una curva $\gamma$ regolare a tratti di equazione $\vec{r} \colon\! [a, b] \to \mathbb{R}^3$, allora
\begin{equation*}
	\int_{\gamma} \delta \, ds = m,
\end{equation*}
ove $m$ è la massa totale del filo. Le coordinate del baricentro sono date dalle formule:
\begin{equation}
	x_b = \frac{1}{m} \int_{\gamma}x\delta \, ds, \quad y_b = \frac{1}{m} \int_{\gamma} y\delta \, ds, \quad z_b = \frac{1}{m} \int_{\gamma} z\delta \, ds.
\end{equation}
Il momento d'inerzia del filo rispetto a un asse è dato dalla formula
\begin{equation}
	I = \int_{\gamma} d^2 \delta \, ds,
\end{equation}
ove $d = d(x, y, z)$ indica la distanza del punto di coordinate $(x, y, z)$ dall'asse.

\subsection{Forme differenziali lineari, integrali curvilinei di seconda specie}
Sia $\vec{F}(x, y, z) = F_1(x, y, z,)\vec{i} + F_2(x, y, z)\vec{j} + F_3(x, y, z)\vec{k}$ un campo vettoriale di classe $C^1(E)$, con $E$ aperto connesso di $\mathbb{R}^3$. Associamo a $\vec{F}$ l'espressione formale
\begin{equation}
	\omega = F_1 dx + F_2dy + F_3dz,
\end{equation}
detta \emph{forma differenziale lineare} con coefficienti $F_1, F_2, F_3$.

Se pensiamo al vettore $d\vec{r} = dx\vec{i} + dy\vec{j} + dz\vec{k}$ come a un vettore ``spostamento infinitesimo'', $\omega = (\vec{F}, d\vec{r})$ rappresenta il lavoro effettuato da $\vec{F}$ in relazione a tale spostamento.

\begin{defn}
	L'integrale curvilineo di $\omega$ lungo $\gamma$ è definito dalla formula
	\begin{equation}
		\begin{split}
			\label{eqn:intcurv2}
			\int_{\gamma}\omega \coloneqq &\int_a^b(F_1(x(t), y(t), z(t))x'(t) + F_2(x(t), y(t), z(t))y'(t)+  \\
			&+ F_3(x(t), y(t), z(t))z'(t))\, dt.
		\end{split}
	\end{equation}
\end{defn}

Introducendo il vettore posizione $\vec{r}(t) = x(t)\vec{i} + y(t)\vec{j} + z(t)\vec{k}$ si ha
\begin{equation*}
	\int_{\gamma} \omega = \int_a^b (\vec{F}, \vec{r}') \, dt
\end{equation*}
e, moltiplicando e dividendo per $s'(t) = \norma{\vec{r}'(t)}$,
\begin{equation}
	\int_{\gamma} \omega = \int_{\gamma} (\vec{F}, \vec{T})\, ds,
\end{equation}
dove
\begin{equation*}
	\vec{T} = \frac{\vec{r}'}{\norma{\vec{r}'}}.
\end{equation*}

\begin{defn}
	Se, data una forma differenziale $\omega$ di classe $C^1(E)$, esiste una funzione $U \colon \! E \to \mathbb{R}$ di classe $C^2(E)$ tale che $dU = \omega$ in $E$, allora $\omega$ si dice esatta e $U$ si chiama funzione potenziale.
\end{defn}

Più esplicitamente, $dU = \omega$ significa che
\begin{equation}
	\label{eqn:potenziale}
	\frac{\partial U}{\partial x} = F_1, \quad \frac{\partial U}{\partial y} = F_2, \quad \frac{\partial U}{\partial z} = F_3,
\end{equation}
o più sinteticamente $\nabla U = \vec{F}$ in $E$. Se $U$ è una funzione potenziale per $\omega$ in $E$ lo è anche $U+c$, $c \in \mathbb{R}$. Essendo $E$ connesso, tutte le possibili funzioni potenziale per $\omega$ hanno questa forma.

\begin{lem}
	Sia $\omega$ esatta in $E$ con funzione potenziale $U$. Sia $\gamma$ una curva regolare, contenuta in $E$, di equazione $\vec{r} = \vec{r}(t)$, $t \in [a, b]$. Allora
	\begin{equation}
		\int_{\gamma}\omega = U(\vec{r}(b) - \vec{r}(a)).
	\end{equation}
\end{lem}

Se $\omega$ è esatta il campo associato è uguale, per la~\eqref{eqn:potenziale}, al gradiente di un potenziale. In tal caso il campo vettoriale si dice \emph{conservativo}.

\begin{teor}
	Sia $\omega = F_1dx + F_2dy + F_3dz$ una forma differenziale lineare di classe $C^1(E)$, $E$ aperto connesso di $\mathbb{R}^3$. Le seguenti affermazioni sono equivalenti:
	\begin{enumerate}
		\item per ogni coppia di curve regolari a tratti $\gamma_1, \gamma_2$ contenute in $E$ e aventi stessi punti iniziale e finale
		\begin{equation*}
			\int_{\gamma_1}\omega = \int_{\gamma_2}\omega;
		\end{equation*}
		\item per ogni curva chiusa $\gamma$ regolare a tratti contenuta in $E$,
		\begin{equation*}
			\oint_{\gamma} \omega = 0;
		\end{equation*}
		\item $\omega$ è esatta in $E$.
	\end{enumerate}
\end{teor}

\subsection{Riconoscimento delle forme differenziali esatte. Costruzione della funzione potenziale}
Sia data la forma differenziale
\begin{equation*}
	\omega = F_1dx + F_2 dy + F_3dz,
\end{equation*}
con coefficienti di classe $C^1(E)$, dove $E$ è un aperto connesso di $\mathbb{R}^3$.

\begin{prop}
	Se $\omega$ è esatta in $E$ ed $\vec{F}$ è il campo vettoriale associato, allora $\rot{\vec{F}} = \vec{0}$ in $E$, ovvero $\vec{F}$ è irrotazionale in $E$.
\end{prop}

Più esplicitamente, si devono verificare in ogni punto di $E$ le relazioni
\begin{equation}
	\label{eqn:irrot}
	\frac{\partial F_3}{\partial y} = \frac{\partial F_2}{\partial z}, \quad \frac{\partial F_1}{\partial z} = \frac{\partial F_3}{\partial x}, \quad \frac{\partial F_2}{\partial x} = \frac{\partial F_1}{\partial y}.
\end{equation}

In $\mathbb{R}^2$ le~\eqref{eqn:irrot} si riducono a
\begin{equation}
	\frac{\partial F_2}{\partial x} = \frac{\partial F_1}{\partial y}.
\end{equation}

Vi sono opportune condizioni topologiche su $E$ sotto le quali le~\eqref{eqn:irrot} diventano anche condizioni sufficienti; per adesso ci limitiamo a dare la seguente
\begin{defn}
	Si dice che $E \subseteq \mathbb{R}^3$ è stellato se esiste un punto $\vec{p}_0 \in E$ tale che, per ogni punto $\vec{p} \in E$, il segmento di retta $[\vec{p}_0. \vec{p}]$ è tutto contenuto in $E$.
\end{defn}

Ogni insieme convesso è stellato; un insieme stellato è ovviamente connesso per segmenti e perciò connesso.

\begin{teor}
	Siano $\omega = F_1dx + F_2 dy + F_3 dz$, $\vec{F} = F_1\vec{i} + F_2\vec{j} + F_3\vec{k}$ con $\vec{F} \in C^1(E)$, dove $E$ è un aperto stellato in $\vec{R}^3$. Allora $\omega$ è esatta se e solo se $\rot{\vec{F}} = \vec{0}$ in $E$.
\end{teor}

La dimostrazione del Teorema fornisce una formula per la costruzione della funzione potenziale; supponendo che $E$ sia stellato rispetto all'origine (a meno di una traslazione degli assi), dato un punto $\vec{p} = (x, y, z) \in E$ indichiamo con $\Gamma$ la curva di equazioni parametriche
\begin{equation*}
	x(t) = tx, \quad y(t) = ty, \quad z(t) = tz \quad t \in [0, 1]
\end{equation*}
il cui sostegno è il segmento di retta $[\vec{0}, \vec{p}]$ che risulta pertanto contenuto in $E$. Poniamo:
\begin{equation}
	\begin{split}
		U(x, y, z) \coloneqq \int_{\Gamma}\omega = &\int_0^1 (F_1(tx, ty, tz)x + F_2(tx, ty, tz)y + \\ &+ F_3(tx, ty, tz)z) dt.
	\end{split}
\end{equation}

Si dimostra che questa è una funzione potenziale; naturalmente, si può costruire $U$ come integrale di $\omega$ lungo una qualunque curva regolare a tratti con sostegno in $E$.

\subsection{Insiemi semplicementi connessi}
Abbiamo visto che per gli insiemi stellati le~\eqref{eqn:irrot} sono necessarie e sufficienti per l'esattezza di una forma differenziale. Una condizione più generale è quella di \emph{semplice connessione}: un insieme $E \subseteq \mathbb{R}^2$ è semplicemente connesso se $E$ è connesso e ogni curva semplice e chiusa contenuta in $E$ è frontiera di un insieme limitato interamente contenuto in $E$. In generale, significa che due curve qualunque contenute in $E$ con gli stessi estremi sono ``deformabili con continuità''  l'una nell'altra senza uscire da $E$. Il concetto rigoroso è quello di \emph{omotopia} tra curve.

Siano $\gamma_1$ e $\gamma_2$ curve contenute in un aperto connesso $E \subseteq \mathbb{R}^3$ di equazioni $\vec{r}_1 = \vec{r}_1(t)$, $\vec{r}_2 = \vec{r}_2(t)$, $t \in [a, b]$ e tali che $\vec{r}_1(a) = \vec{r}_2(a)=\vec{p}_a$, $\vec{r}_1(b) = \vec{r}_2(b) = \vec{p}_b$.

\begin{defn}
	Le due curve $\gamma_1$ e $\gamma_2$ si dicono omotope in $E$ se esiste una funzione continua $\vec{\phi} = \vec{\phi}(t, \lambda)$, $(t, \lambda) \in [a, b] \times [0, 1]$ tale che
	\begin{enumerate}
		\item $\vec{\phi}(t, 0) = \vec{r}_1(t), \quad \vec{\phi}(t, 1) = \vec{r}_2(t) \quad \forall t \in [a, b]$;
		\item $\vec{\phi}(a, \lambda) = \vec{p}_a, \quad \vec{\phi}(b, \lambda) = \vec{p}_b \quad \forall \lambda \in [0, 1] $,
	\end{enumerate}
	e infine che per ogni $\lambda \in [0, 1]$ la curva $\gamma_{\lambda}$ di equazione $\vec{\phi} = \vec{\phi}(t, \lambda)$ sia contenuta in $E$. Se $\gamma_1$ e $\gamma_2$ sono chiuse, la $2.$ è sostituita dalla condizione
	\begin{equation*}
		\vec{\phi}(a, \lambda) = \vec{\phi}(b, \lambda) \quad \forall \lambda \in [0, 1].
	\end{equation*}
\end{defn}

\begin{defn}
	Un aperto connesso $E \subseteq \mathbb{R}^3$ si dice semplicemente connesso se due curve qualsiasi contenute in $E$ aventi gli stessi estremi sono omotope.
\end{defn}

La definizione si può dare in termini di curve chiuse: un aperto connesso $E$ è semplicemente connesso se ogni curva chiusa contenuta in $E$ è omotopa a una curva costante (cioè che si riduce a un solo punto).

Gli insiemi convessi e quelli stellati sono semplicemente connessi; non lo sono, ad esempio, una corona circolare o il piano privato di un punto (in $\mathbb{R}^2$) o una sfera privata di un diametro o l'interno di un toro (in $\mathbb{R}^3$).

\begin{teor}
	Siano $\omega = F_1dx + F_2 dy + F_3 dz$, $\vec{F} = F_1\vec{i} + F_2\vec{j} + F_3\vec{k}$ con $\vec{F} \in C^1(E)$, dove $E$ è un aperto semplicemente connesso in $\vec{R}^3$. Allora $\omega$ è esatta se e solo se $\rot{\vec{F}} = \vec{0}$ in $E$.
\end{teor}


\chapter{Ottimizzazione di funzioni di più variabili}
\section{Generalità sull'ottimizzazione, estremi liberi}
\subsection{Generalità sull'ottimizzazione}
Sia $f\colon \mathbb{R}^n \supseteq X \to \mathbb{R}$.
\begin{defn}
	Un punto $\vec{x}_0 \in X$ si dice di \emph{massimo} (\emph{minimo}) \emph{locale} per $f$ se esiste un intorno $B_r(\vec{x}_0)$ tale che
	\begin{equation}
		\label{eqn:defestremi}
		f(\vec{x}) \le f(\vec{x}_0) 	\quad (f(\vec{x}) \ge f(\vec{x}_0))
		\end{equation}per ogni $\vec{x} \in X \cap B_r(\vec{x}_0)$. Si dice di massimo (minimo) \emph{globale} se la~\eqref{eqn:defestremi} vale per ogni $\vec{x} \in X$.
	\end{defn}

	Inoltre, un punto si dice di massimo (minimo) \emph{forte} se nella~\eqref{eqn:defestremi} la disuguaglianza vale in senso stretto.

	Se $X$ è aperto gli estremi di $f$ (se esistono) si dicono \emph{liberi} e la loro ricerca consiste nell'individuare i punti di estremo interni al dominio $X$.

	Se siamo interessati a studiare gli estremi di $f$ in una sua restrizione $f_{|U}$, ove $U \subset X$, si parla di \emph{estremi vincolati}. Studieremo i casi in cui $U$ è l'intersezione di $X$ con un insieme definito da un insieme di uguaglianze del tipo
	\begin{equation*}
		g_j(\vec{x}) = 0,
	\end{equation*}
	per $j = 1, 2, \dots, m$ e con $m < n$.

	\subsection{Estremi liberi: condizioni necessarie}
	Siano $f \colon \mathbb{R}^n \supseteq X \to \mathbb{R}$, con $X$ aperto, e $\vec{x_0}$ un punto di estremo locale per $f$.
	\begin{teor}
		Fissata una direzione $\vec{v}$ in $\mathbb{R}^n$, se $\vec{x_0}$ è punto di estremo locale per $f$ e $D_{\vec{v}}f(\vec{x_0})$ esiste allora $D_{\vec{v}}f(\vec{x}_0) = 0$.
	\end{teor}

	\begin{cor}
		Se $f$ è differenziabile in $\vec{x}_0$, punto di estremo locale per $f$, allora $\nabla f(\vec{x}_0) = \vec{0}$ (ogni derivata direzionale in $\vec{x}_0$ è nulla).
	\end{cor}

	Si noti che tale condizione è solo \emph{necessaria}: possono esistere punti critici che non siano di massimo né di minimo. In particolare, un punto critico $\vec{x_0}$ si dice di \emph{sella} (o di \emph{colle}) se in ogni intorno di $\vec{x_0}$ esistono punti in cui $f$ è maggiore di $f(\vec{x_0})$ e punti in cui $f$ è minore di $f(\vec{x_0})$.

	Si noti che per le funzioni convesse e concave non occorrono ulteriori analisi:
	\begin{prop}
		Se $\vec{x}_0$ è un punto critico per una funzione $f$ convessa (concava) e differenziabile, allora $\vec{x}_0$ è punto di minimo (massimo) globale. Se $f$ è strettamente convessa (concava) allora $\vec{x}_0$ è punto di minimo (massimo) unico e forte.
	\end{prop}

	\subsection{Forme quadratiche}
	\label{subsec:formequadratiche}
	Una \emph{forma quadratica} in $\mathbb{R}^n$ è un polinomio omogeneo di secondo grado del tipo
	\begin{equation}
		\label{eqn:formaquadratica}
		q(\vec{h}) = q(h_1, h_2, \dots, h_n) = \sum_{i,j=1}^na_{ij}h_ih_j,
	\end{equation}
	dove gli $a_{ij}$ sono i numeri reali coefficienti della forma quadratica. Si può sempre supporre che una forma quadratica sia simmetrica; se così non fosse, basta sostituire per ogni $i \ne j$ i coefficienti $a_{ij}$ e $a_{ji}$ con la loro semisomma
	\begin{equation*}
		\frac{a_{ij} + a_{ji}}{2}.
	\end{equation*}

	A ogni forma quadratica $q(\vec{h})$ risulta associata una matrice simmetrica $A = (a_{ij})_{i,j = 1, \dots, n}$ con una corrispondenza biunivoca; una forma quadratica può anche essere scritta come prodotto scalare, $q(\vec{h}) = (A\vec{h}, \vec{h})$, o in notazione matriciale, $q(\vec{h}) = \vec{h}^tA\vec{h}$.

	\begin{defn}
		Una forma quadratica $q(\vec{h})$, $\vec{h} \in \mathbb{R}^n$, si dice:
		\begin{enumerate}
			\item \emph{definita positiva} (\emph{negativa}) se $\forall \vec{h} \ne 0$, $q(\vec{h}) > 0$ $(q(\vec{h}) < 0)$;
			\item \emph{semidefinita positiva} (\emph{negativa}) se $\forall \vec{h} \ne 0$, $q(\vec{h}) \ge 0$ $(q(\vec{h}) \le 0)$;
			\item \emph{indefinita} se esistono $\vec{h}_1, \vec{h}_2 \in \mathbb{R}^n$ tali che $q(\vec{h}_1) > 0$, $q(\vec{h}_2) < 0$.
		\end{enumerate}
	\end{defn}

	Indichiamo con $A_k$, $k = 1, \dots, n$, le $n$ sottomatrici composte dalle prime $k$ righe e $k$ colonne di $A$:
	\begin{equation*}
		A_1 = (a_{11}), \ A_2 = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}, \ A_3 = \begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{pmatrix}, \dots, \ A_n = A.
	\end{equation*}

	\begin{teor}
		Sia
		\begin{equation*}
			q(\vec{h}) = \sum_{i,j = 1}^n a_{ij}h_ih_j,
		\end{equation*}
		$\vec{h} \in \mathbb{R}^n$. Allora:
		\begin{enumerate}
			\item q è definita positiva se e solo se $\det{A_k} > 0$ per ogni $k = 1, 2, \dots, n$;
			\item q è definita negativa se e solo se $(-1)^k\det{A_k} > 0$ per ogni $k = 1, 2, \dots, n$.
		\end{enumerate}
	\end{teor}

	Il Teorema vale più in generale per ogni catena di \emph{sottomatrici principali} (cioè simmetriche rispetto alla diagonale principale di $A$) ottenute partendo da un elemento $a_{jj}$ della diagonale e aggiungendo ogni volta una riga e una colonna.

	\begin{teor}
		Sia
		\begin{equation*}
			q(\vec{h}) = \sum_{i,j = 1}^na_{ij}h_ih_j,
		\end{equation*}
		$\vec{h} \in \mathbb{R}^n$. Allora:
		\begin{enumerate}
			\item $q$ è semidefinita positiva se e solo se ogni sottomatrice principale ha determinante non negativo;
			\item $q$ è semidefinita negativa se e solo se ogni sottomatrice principale di ordine $k$ ha determinante non negativo se $k$ è pari, non positivo se $k$ è dispari.
		\end{enumerate}
	\end{teor}
	In ogni altro caso $q(\vec{h})$ è indefinita.

	\begin{teor}
		Sia $q(\vec{h}) = \vec{h}^tA\vec{h}$, $\vec{h} \in \mathbb{R}^n$. Allora:
		\begin{enumerate}
			\item $q$ è definita positiva (negativa) se e solo se tutti gli autovalori sono positivi (negativi);
			\item $q$ è semidefinita positiva (negativa) se e solo se tutti gli autovalori sono non negativi (non positivi) e almeno uno di essi è zero;
			\item $q$ è indefinita se e solo se esistono due autovalori di segno opposto.
		\end{enumerate}
	\end{teor}

	Si conclude con la seguente
	\begin{oss}
		Se $q = q(\vec{h})$ è una forma quadratica in $\mathbb{R}^n$, allora $q(\vec{0}) = 0$ e $\nabla q(\vec{0}) = \vec{0}$; la natura di $\vec{h} = \vec{0}$ dipende dal segno di $q$.

		Se $q$ è definita positiva (negativa), $\vec{h} = \vec{0}$ è punto di minimo (massimo) globale forte.

		Se $q$ è indefinita, $\vec{0}$ è di sella.

		Se $q$ (non nulla) è semidefinita positiva (negativa), $\vec{0}$ è punto di minimo (massimo) globale debole.
	\end{oss}

	\subsection{Condizioni sufficienti per estremi liberi}
	La matrice corrispondente a $d^2f(\vec{x}_0)$ è l'hessiana di $f$ in $\vec{x}_0$, ossia
	\begin{equation*}
		\mathbf{H}_f(\vec{x}_0) = (f_{x_ix_j}(\vec{x}_0))_{i,j = 1, \dots, n},
	\end{equation*}
	che risulta simmetrica se $f_{x_ix_j}(\vec{x}_0) = f_{x_jx_i}(\vec{x}_0)$ per ogni $i,j = 1, 2, \dots, n$ (accade, per esempio, se $f \in C^2$).

	\begin{teor}
		Siano $f \in C^2(X)$ e $\vec{x}_0$ punto critico per $f$. Se $d^2f(\vec{x}_0)$ è:
		\begin{enumerate}
			\item definita positiva (negativa), $\vec{x}_0$ è punto di minimo (massimo) locale forte;
			\item indefinita, $\vec{x}_0$ è punto di sella.
		\end{enumerate}
	\end{teor}

	\begin{prop}
		Sia $f \in C^2(X)$. Se $\vec{x}_0$ è punto di massimo (minimo), allora $d^2f(\vec{x}_0)$ è definita o semidefinita negativa (positiva). In particolare, $f_{x_jx_i} \le 0$ $(\ge 0)$ per ogni $i,j = 1, 2, \dots, n$.
	\end{prop}

	Una formulazione equivalente della Proposizione è la seguente: se $d^2f(\vec{x}_0)$ non è nulla ed è semidefinita positiva (negativa), allora $\vec{x}_0$ non può essere punto di massimo (minimo).

	\begin{oss} Dalle considerazioni precedenti, si arriva alla seguente regola nel caso bidimensionale.

		Siano $f \in C^2(X)$, $X$ aperto di $\mathbb{R}^2$ e $(x_0, y_0)$ punto critico per $f$. L'hessiana di $f$ in $(x_0, y_0)$ è
		\begin{equation*}
			\mathbf{H}_f(x_0, y_0) = \begin{pmatrix} f_{xx}(x_0, y_0) & f_{yx}(x_0, y_0) \\ f_{xy}(x_0, y_0) & f_{yy}(x_0, y_0) \end{pmatrix};
		\end{equation*}
		\begin{enumerate}
			\item se $\det{\mathbf{H}_f(x_0, y_0)} > 0$\footnote{Si noti che in questo caso $f_{xx}(x_0, y_0)$ e $f_{yy}(x_0, y_0)$ hanno lo stesso segno.} e
			\begin{itemize}
				\item $f_{xx}(x_0, y_0) > 0$, allora $(x_0, y_0)$ è punto di minimo locale forte;
				\item $f_{xx}(x_0, y_0) < 0$, allora $(x_0, y_0)$ è punto di massimo locale forte;
			\end{itemize}
			\item se $\det{\mathbf{H}_f(x_0, y_0)} < 0$, allora $(x_0, y_0)$ è punto di sella;
			\item se $\det{\mathbf{H}_f(x_0, y_0)} = 0$ occorre uno studio più approfondito.
		\end{enumerate}
		Inoltre, se $\det{\mathbf{H}_f(x, y)} > 0$ per ogni $(x, y) \in X$ l'estremo è globale. Infine, se $\det{\mathbf{H}_f(x, y)} = 0$ e $f_{xx}(x,y) > 0$ ($< 0$) oppure $f_{yy}(x,y) > 0$ ($< 0$) in tutto $X$, allora $(x_0, y_0)$ è punto di minimo globale (massimo globale).
	\end{oss}

	\section{Estremi vincolati, vincoli di uguaglianza}
	\subsection{Funzioni di due variabili}
	Esaminiamo innanzitutto il caso più semplice.

	Date due funzioni di due variabili $f$ e $g$ di classe $C^1(X)$, $X$ aperto di $\mathbb{R}^2$, si vogliono determinare gli estremi di $f$ (\emph{funzione obiettivo}) ristretta all'insieme (\emph{vincolo})
	\begin{equation*}
		E_0 = \{ (x, y) \in \mathbb{R}^2 \colon g(x, y) = 0 \}.
	\end{equation*}

	La situazione più favorevole è quella in cui dall'equazione $g(x, y) = 0$ si può esplicitare $y = y(x)$ o $x = x(y)$ oppure, più in generale, quella in cui $E_0$ coincide con una curva $\gamma$ di equazioni parametriche $x = x(t)$ e $y = y(t)$, $t \in I$, con $I$ intervallo contenuto in $\mathbb{R}$.

	Il problema si riconduce alla ricerca degli estremi della funzione reale di variabile reale
	\begin{equation*}
		\phi (t) = f(x(t), y(t)),
	\end{equation*}
	con $t \in I$.

	\begin{exmp}
		Si determinino gli estremi di $f(x, y) = x^2 + 3y$, con il vincolo
		\begin{equation*}
			g(x,y) = \frac{x^2}{4} + \frac{y^2}{9} - 1 = 0.
		\end{equation*}
		Si nota facilmente che $E_0$ è un'ellisse che ha per equazioni parametriche $x(t) = 2 \cos{t}$, $y(t) = 3\sin{t}$, $t \in [0, 2 \pi]$, e pertanto basta determinare gli estremi di
		\begin{equation*}
			\phi(t) = 4(\cos{t})^2 + 9\sin{t}
		\end{equation*}
		nell'intervallo $[0, 2\pi]$. Con gli strumenti classici dell'analisi differenziale, si trova facilmente che $t= \pi/2$ è un punto di massimo locale e $t = 3\pi/2$ un punto di minimo locale (e $9$ e $-9$ sono rispettivamente massimo e minimo globali).
	\end{exmp}

	In generale, non si potrà ridurre a una dimensione il problema; vediamo dunque come procedere.

	Introduciamo innanzitutto il concetto di \emph{punto critico} (o \emph{stazionario}) \emph{vincolato}. Sia $(x_0, y_0)$ un punto \emph{regolare} di $E_0$, cioè
	\begin{equation*}
		g(x_0, y_0) = 0, \ \nabla g(x_0, y0) \ne \vec{0}.
	\end{equation*}
	In tal caso, in un intorno di $(x_0, y_0)$ $E_0$ coincide con una curva $\gamma$ di equazioni parametriche $x = x(t)$ e $y = y(t)$; si può inoltre scegliere il parametro in modo che $t$ vari in un intorno $I_0$ di $t = 0$ e che $x(0) = x_0$, $y(0) = y_0$.

	Dunque, il vettore $(x'(0), y'(0))$ è tangente al vincolo nel punto $(x_0, y_0)$. Si può allora considerare la funzione $\phi(t) = f(x(t), y(t))$ e definire $(x_0, y_0)$ \emph{punto critico vincolato} se $t = 0$ è \emph{punto critico} per $\phi$, ovvero se $\phi'(0) = 0$.

	Essendo $f$ differenziabile, si ha
	\begin{equation*}
		\phi'(t) = f_x(x(t), y(t))x'(t) + f_y(x(t), y(t)) y'(t)
	\end{equation*}
	e imponendo $\phi'(0) = 0$ si ha
	\begin{equation}
		\label{eqn:ptocritvinc}
		f_x(x_0, y_0)x'(0) + f_y(x_0, y_0)y'(0) = 0.
	\end{equation}
	In altre parole, la derivata di $f$ in direzione tangente al vincolo in $(x_0, y_0)$ è nulla.

	\begin{defn}
		Siano $f, g \in C^1(X)$, $X$ aperto di $\mathbb{R}^2$. Un punto $(x_0, y_0) \in X$ si dice \emph{critico} o \emph{stazionario} condizionato al vincolo $g(x, y) = 0$ se:
		\begin{enumerate}
			\item $g(x_0, y_0) = 0$ e $\nabla g(x_0, y_0) \ne \vec{0}$, cioè $(x_0, y_0)$ è un punto regolare per $E_0$;
			\item la derivata di $f$ in direzione tangente al vincolo si annulla in $(x_0, y_0)$, cioè vale la~\eqref{eqn:ptocritvinc}.
		\end{enumerate}
	\end{defn}

	\begin{teor}[Caratterizzazione dei punti critici vincolati]
		\label{teor:carcritvinc}
		Siano $f, g \in C^1(X)$, $X$ aperto di $\mathbb{R}^2$ e sia $(x_0, y_0)$ un punto regolare per $E_0 = \{ (x,y) \in \mathbb{R}^2 \colon g(x,y) = 0\}$. Allora $(x_0, y_0)$ è punto critico vincolato a $E_0$ se e solo se esiste un numero reale $\lambda_0$ tale che
		\begin{equation}
			\label{eqn:carcritvinc}
			\nabla f(x_0, y_0) = \lambda_0 \nabla g(x_0, y_0).
		\end{equation}
	\end{teor}

	\begin{teor}[Condizione necessaria per gli estremi vincolati]
		Nelle ipotesi del precedente Teorema, se $(x_0, y_0)$ è un punto di estremo vincolato (con vincolo $g(x, y) = 0)$, allora è punto critico vincolato. In particolare, esiste $\lambda_0$ tale che valda la~\eqref{eqn:carcritvinc}.
	\end{teor}

	\begin{oss}
		Il numero $\lambda_0$ la cui esistenza è asserita nel Teorema \ref{teor:carcritvinc} prende il nome di \emph{moltiplicatore di Lagrange}.

		Introducendo la funzione $\mathcal{L} = \mathcal{L}(x, y, \lambda)$, detta \emph{lagrangiana}, definita da
		\begin{equation*}
			\mathcal{L}(x, y, \lambda) \coloneqq f(x,y) - \lambda g(x,y),
		\end{equation*}
		il Teorema \ref{teor:carcritvinc} afferma che $(x_0, y_0)$ è punto di critico vincolato se e solo se esiste $\lambda_0$ tale che il punto $(x_0, y_0, \lambda_0)$ sia punto critico libero per $\mathcal{L}$. Infatti, i punti critici di $\mathcal{L}$ sono soluzioni del sistema
		\begin{equation}
			\label{eqn:moltiplicatorilagrange}
			\begin{dcases}
				\mathcal{L}_x = f_x - \lambda g_x = 0 \\
				\mathcal{L}_y = f_y - \lambda g_y = 0 \\
				\mathcal{L}_{\lambda} = -g = 0.
			\end{dcases}
		\end{equation}
		Si noti che le prime due equazioni coincidono con la~\eqref{eqn:carcritvinc}, mentre la terza esprime la condizione del vincolo.
	\end{oss}

	Abbiamo dunque sviluppato il seguente modo di procedere, detto \emph{metodo dei moltiplicatori di Lagrange}:
	\begin{enumerate}
		\item si isolano gli eventuali punti non regolari di $E_0$, che vanno esaminati a parte;
		\item si cercano i punti critici vincolati di $f$ o equivalentemente quelli liberi della lagrangiana, cioè le soluzioni del sistema~\eqref{eqn:moltiplicatorilagrange};
		\item si determina la natura dei punti critici (si veda il prossimo paragrafo).
	\end{enumerate}

	\subsection{Studio della natura dei punti critici vincolati}
	Ci limiteremo al caso di un solo vincolo ($m = 1$).

	Per lo studio della natura dei punti critici vincolati procederemo come nel caso dei punti critici liberi, studiando il differenziale secondo (rispetto a $\vec{x}$) della lagrangiana ristretto a incrementi tangenziali ai vincoli.

	Sia $\vec{x}_0 \in \mathbb{R}^n$ un punto critico per $f$ condizionato al vincolo $g(\vec{x}) = 0$ e sia $\lambda_0$ il corrispondente moltiplicatore di Lagrange.

	Vale il seguente
	\begin{teor}
		Siano $f, g \colon X \to \mathbb{R}$, $X$ aperto di $\mathbb{R}^n$, di classe $C^2$.

		Se la forma quadratica
		\begin{equation}
			\label{eqn:naturacritvinc}
			\sum_{i,j=1}^n\bigl(f_{x_ix_j}(\vec{x}_0) - \lambda_0 g_{x_ix_j}(\vec{x}_0) \bigr) h_ih_j = \bigl( (\mathbf{H}_f(\vec{x}_0) - \lambda_0\mathbf{H}_g(\vec{x}_0))\vec{h}, \vec{h}\bigr),
		\end{equation}
		ristretta all'insieme dei vettori $\vec{h} \in \mathbb{R}^n$ tangenziali al vincolo in $\vec{x}_0$ (cioè $(\nabla g(\vec{x_0}), \vec{h}) = 0$), è definita positiva (negativa), allora $\vec{x}_0$ è punto di massimo (minimo) locale forte vincolato.
	\end{teor}

	Può essere utile il seguente criterio di riconoscimento del segno di una forma quadratica soggetta a un vincolo lineare.

	\begin{lem}
		Sia $q(\vec{h}) = (A\vec{h}, \vec{h})$ una forma quadratica in $\mathbb{R}^n$ e sia $\vec{b} \in \mathbb{R}^n$, $\vec{b} = (b_1, b_2, \dots, b_n)$, $b_1 \ne 0$. Allora la forma quadratica $q(\vec{h})$ soggetta al vincolo lineare $(\vec{b}, \vec{h}) = 0$ è definita positiva se sono negativi tutti i minori principali di ``nord-ovest''\footnote{Sono chiamati così i determinanti delle sottomatrici principali di nord-ovest, quelle che nella sottosezione \ref{subsec:formequadratiche} abbiamo indicato con $A_k$.}, di ordine maggiore di 2, della matrice
		\begin{equation}
			\begin{pmatrix}
				0 & b_1 & \dots & b_n  \\
				b_1 \\
				\vdots &  &A \\
				b_n
				\end{pmatrix}.
			\end{equation}
			È definita negativa se i suddetti minori si susseguono a segni alterni a partire dal primo (di ordine 3) positivo.
		\end{lem}

		Nel nostro caso, il lemma si applica con $A = \mathbf{H}_f(\vec{x}_0) - \lambda_0 \mathbf{H}_g(\vec{x}_0)$ e $\vec{b} = \nabla g(\vec{x}_0)$; poiché $\nabla g(\vec{x}_0) \ne \vec{0}$, si può supporre sempre che $D_{x_1}g(\vec{x}_0) \ne 0$.

		\chapter{Spazi funzionali e approssimazione di funzioni}
		\section{Spazi funzionali}
		\subsection{Metrica e topologia}

		\begin{defn}
		Un insieme $X$ si dice spazio metrico se è definita un'applicazione (distanza o metrica) $d \colon\! X \times X \to \mathbb{R}$ tale che, $\forall\, x, y, z \in X$, valgano le proprietà
			\begin{enumerate}[$\mathcal{D}$\arabic*.]
			\item	$d(x, y) \ge 0, \quad d(x, y) = 0 \iff x = y0;$
			\item $d(x, y) = d(y, x)0:$
			\item $d(x, y) \le d(x, z) + d(z, y)$.	\end{enumerate}
		\end{defn}

		Con spazio metrico s'intende la \emph{coppia} $(X, d)$; su uno stesso insieme potrebbero definirsi più metriche diverse, $d_1, d_2, \dots$; in tal caso, $(X, d_1)$ e $(X, d_2)$ sono spazi metrici diversi.

		\begin{exmp}
		Qualunque insieme può diventare spazio metrico con la distanza
			\begin{equation}
				d(x, y) \coloneqq \begin{dcases*}
				0 &se $x = y$ \\
				1 &se $x \ne y$,
			\end{dcases*}
			\end{equation}
		detta \emph{metrica discreta}.
		\end{exmp}

		\begin{exmp}
		Gli spazi euclidei $\mathbb{R}^n$ e $\mathbb{C}^n$ sono metrici con la consueta definizione di distanza
			\begin{equation}
				d(\vec{x}, \vec{y}) \coloneqq \norma{x-y} \coloneqq \Biggl(\sum_{i = 1}^n \abs{x_i -y_i}^2 \Biggr)^{1/2}
			\end{equation}
		(\emph{metrica pitagorica}). Altre metriche possibili in $\mathbb{R}^n$ e $\mathbb{C}^n$ sono ad esempio
			\begin{equation*}
				d_1(\vec{x}, \vec{y}) \coloneqq \sum_{i=1}^n \abs{x_i - y_i}, \quad d_{\infty}(\vec{x}, \vec{y}) \coloneqq \max_{i=1, \dots, n}\{\abs{x_i, y_i}\}.
			\end{equation*}
		\end{exmp}

		\begin{exmp}
		L'insieme $\mathbb{R}* = \mathbb{R} \cup \{+\infty\} \cup \{-\infty\}$ si può metrizzare con la definizione di distanza ($x, y \in \mathbb{R}$)
			\begin{align*}
				&d(x, y) \coloneqq \abs{\arctg x - \arctg y}, \\
				&d(x, +\infty) = d(+\infty, x) \coloneqq \frac{\pi}{2} - \arctg x, \\
				&d(x, -\infty) = d(-\infty, x) \coloneqq \arctg x + \frac{\pi}{2}, \\
				&d(+\infty, +\infty) = d(-\infty, - \infty) \coloneqq 0, \\
				&d(+\infty, -\infty) = d(-\infty, + \infty) \coloneqq \pi.
			\end{align*}
		\end{exmp}

		\begin{exmp}
		Sia $I \subseteq \mathbb{R}$ un intervallo; indichiamo con $C^0(I)$ la classe della funzioni continue su $I$. Supponiamo che che $I$ sia compatto su $\mathbb{R}$; in $C^0(I)$ si può introdurre una metrica nel modo seguente: se $f(t)$ e $g(t)$ sono due elementi dello spazio, poniamo
			\begin{equation}
				\label{eqn:lagrangiana}
				d(f, g) \coloneqq \max_{t\in I} \abs{f(t) - g(t)}
			\end{equation}
		(\emph{metrica lagrangiana}); senza difficoltà si può estendere a funzioni di più variabili continue in un sottoinsieme $A \subseteq \mathbb{R}^n$.
		\end{exmp}

		\begin{exmp}
		Se (riferendoci all'esempio precedente) $I$ non è compatto, non è più valida la metrica~\eqref{eqn:lagrangiana}. Tuttavia, se consideriamo l'insieme $C^0_b(I)$ delle funzioni continue e limitate su $I$ possiamo definire una nuova metrica ponendo
			\begin{equation}
				d(f, g) \coloneqq \sup_{t \in I} \abs{f(t) - g(t)}.
			\end{equation}
		Più in generale, questa sarà una metrica per lo spazio $\mathcal{B}(I)$ di tutte le funzioni limitate in $I$.
		\end{exmp}

		\begin{exmp}
		Sia ancora $I$ compatto; su $C^k(I)$, $k \ge 0$, si pone la metrica
			\begin{equation}
				d(f,g) \coloneqq \sum_{j=0}^k \max_{t \in I} \abs{f^{(j)}(t) - g^{(j)}(t)}
			\end{equation}
		(\emph{metriche lagrangiane di ordine $k$}).
		\end{exmp}

		\begin{exmp}
		In $\overset{~}{C}(a, b)$, insieme delle funzioni continue e assolutamente integrabili in senso generalizzato sull'intervallo $(a, b)$, possiamo definire una metrica ponendo
			\begin{equation}
				d(f, g) \coloneqq \int_a^b \abs{f(t) - g(t)}\,dt
			\end{equation}
		(\emph{metrica integrale di ordine $1$}).
		\end{exmp}

		Ricordiamo la definizione di intorno sferico.
			\begin{defn}
				Se $(X, d)$ è uno spazio metrico e $x \in X$, chiameremo intorno sferico di $x$ di raggio $r$ l'insieme
					\begin{equation*}
						B(x, r) \coloneqq \{ y \in X \colon d(x, y) < r \}.
					\end{equation*}
			\end{defn}

		Riprendiamo la definizione di successione convergente e di successione di Cauchy (o fondamentale).
			\begin{defn}
				Sia $(X, d)$ uno spazio metrico e $\{x_n\}$ una successione a valori in $X$; essa si dirà convergente a un punto $x \in X$ se vale una delle seguenti affermazioni (equivalenti):
					\begin{enumerate}
						\item fissato un qualunque intorno $V$ di $x$, per $n \to +\infty$, $x_n$ appartiene definitivamente a $V$;
						\item $\forall \epsilon > 0$, esiste un intero positivo $N$ tale che, per $n > N$, risulta $d(x_n, x) < \epsilon$;
						\item $\lim_{n\to+\infty} d(x_n, x) = 0.$
					\end{enumerate}
			\end{defn}

				\begin{defn}
					Sia $(X, d)$ no spazio metrico e $\{ x_n\}$ una successione a valori in $X$; essa si dirà fondamentale o di Cauchy se
						\begin{equation}
							\lim_{n, m \to +\infty} d(x_n, x_m) = 0.
						\end{equation}
				\end{defn}

				Se una successione è convergente, allora essa è fondamentale; l'affermazione inversa è vera negli spazi euclidei, ma non lo è in tutti gli spazi metrici. Uno spazio metrico in cui ogni successione fondamentale è convergente si dice \emph{completo}.

				\subsection{Successioni di funzioni, convergenza puntuale e uniforme}

				Sia $I \subseteq \mathbb{R}$ un intervallo, $f_n \colon I \to \mathbb{R}$ una successione di funzioni e sia $t_0 \in I$. La successione $\{ f_n(t_0)\}$ è dunque una successione di numeri reali e potremo determinarne il carattere; possiamo ripetere questa operazione per tutti i punti $t \in I$. Se, per ogni $t \in I$, la corrispondente successione numerica $\{ f_n(t)\}$ è convergente e $f(t)$ è il suo limite, risulta definita una funzione $f \colon I \to \mathbb{R}$ che sarà naturale chiamare \emph{limite} della successione di funzioni $\{ f_n\}$.

				Più precisamente, diamo la seguente
					\begin{defn}
						Una successione di funzioni $f_n \colon I \to \mathbb{R}$ si dirà convergente in $I$ con limite $f \colon I \to \mathbb{R}$ se, fissato $\epsilon > 0$, $\forall t \in I$, $\exists N = N(\epsilon, t)$ tale che
							\begin{equation*}
								\abs{f_n(t) - f(t)} < \epsilon
							\end{equation*}
						se $n > N$.
					\end{defn}

			Questa definizione è una semplice estensione dell'analoga definizione di convergenza delle successioni numeriche; essa viene detta anche convergenza puntuale (o ordinaria).

			Secondo un altro punto di vista, potremmo considerare le funzioni come elementi di un dato spazio metrico $X$; il limite è allora già definito dalla metrica di questo spazio.

			Sia $X = C^0(I)$, con $I$ compatto, dotato della metrica lagrangiana. Una successione $\{f_n \}\subset C^0(I)$ converge a $f \in C^0(I)$ se accade che $d(f_n, f)$ tende a zero, cioè
				\begin{equation*}
					\lim_{n \to \infty} \max_{t \in I} \abs{f_n(t) - f(t)} = 0.
				\end{equation*}

		Ciò significa che, fissato $\epsilon > 0$, $\exists N = N(\epsilon)$ per cui
			\begin{equation}
				\max_{t \in I} \abs{f_n(t) - f(t)} > \epsilon
			\end{equation}
		se $n > N$.

		Ma, se per $n > N(\epsilon)$, $\abs{f_n(t) - f(t)} < \epsilon$ nei punti di massimo, lo stesso accadrà per ogni altro $t \in I$; perciò, senz'altro $f_n$ converge a $f$ puntualmente.

		\begin{defn}
			Una successione di funzioni $f_n \colon I \to \mathbb{R}$ si dice convergente uniformemente in $I$, con limite $f \colon I \to \mathbb{R}$, se, fissato $\epsilon > 0$, $\exists N = N(\epsilon)$ tale che
				\begin{equation}
					\label{eqn:uniforme}
					\abs{f_n(t) - f(t)} < \epsilon \quad \forall t \in I,
				\end{equation}
				se $n > N$.
		\end{defn}

		Confrontando le definizioni precedenti, si osserva che nel primo caso prima si fissa $t \in I$ e poi si trova $N$, che dipenderà dunque (oltre che dal prefissato $\epsilon$) anche dal punto $t$ scelto; nel secondo, siamo invece in grado di trovare un $N$ indipendente da $t$ per cui la~\eqref{eqn:uniforme} è verificata. Ovviamente, la convergenza uniforme  implica quella puntuale, ma non vale il viceversa.

		Osserviamo inoltr che, per le successioni in $C^0(I)$, la convergenza ``naturale'' indotta dalla metrica lagrangiana è proprio quella uniforme; anche la metrica dell'estremo superiore adottata sullo spazio $\mathcal{B}(I)$ conduce alla convergenza uniforme.

		\begin{exmp}
		Sia $f_n \colon [0, 1] \to \mathbb{R} \colon t \to t^n$ e sia $f(t) = 0$ se $0 \le t < 1$, $f(1) = 1$. La successione $f_n$ converge puntualmente a $f$: se $t = 1$ oppure $t = 0$ l'affermazione è ovvia, se $0 < t < 1$, fissato $\epsilon > 0$, basta prendere
			\begin{equation*}
				N = \frac{\log{\epsilon}}{\log{t}}
			\end{equation*}
		per essere certi che $t^n < \epsilon$ se $n > N$. Scegliendo $t$ prossimo a $1$, $N$ diventa sempre più grande: dunque non esiste un $N$ adatto per tutti i $t \in [0, 1]$ e la successione non converge uniformemente. In alternativa,
			\begin{equation*}
				\sup_{t\in[0,1]} \abs{f_n(t) - f(t)} = 1,
			\end{equation*}
		che non tende a $0$ per $n \to + \infty$, quindi $f_n$ non converge uniformemente a $f$ in $[0, 1]$.
		\end{exmp}

		\begin{exmp}
		La successione $f_n \colon \mathbb{R} \to \mathbb{R} \colon t \to \sin{(nt)}/n$ converge uniformemente a $0$; infatti risulta
			\begin{equation*}
				\frac{1}{n} \abs{\sin{(nt)}} < \epsilon
			\end{equation*}
		se $ n > 1/\epsilon$.
		\end{exmp}

		Rimane valido il criterio di convergenza di Cauchy:
			\begin{prop}[Criterio di convergenza puntuale]
				Condizione necessaria e sufficiente affinché la successione di funzioni $f_n \colon I \to \mathbb{R}$ converga puntualmente in $I$ è che, fissato $\epsilon > 0$, $\forall t \in I$, esista $N = N(\epsilon, t)$ tale che
					\begin{equation*}
						\abs{f_n(t) - f_m(t)} < \epsilon
					\end{equation*}
				se $n, m > N$.
			\end{prop}

			\begin{prop}[Criterio di convergenza uniforme]
				Condizione necessaria e sufficiente affinché la successione di funzioni $f_n \colon I \to \mathbb{R}$ converga uniformemente in $I$ è che, fissato $\epsilon > 0$, esista $N = N(\epsilon)$ tale che
					\begin{equation*}
						\abs{f_n(t) - f_m(t)} < \epsilon \quad \forall t \in I,
					\end{equation*}
				se $n, m > N$.
			\end{prop}

		\subsection{Scambio di limiti; di limite e derivata; di limite e integrale}

		\begin{prop}
			Il limite $f$ di una successione di funzioni limitate $f_n \colon I \to \mathbb{R}$, convergente uniformemente, è una funzione limitata.
		\end{prop}

		\begin{teor}[dello scambio dei limiti]
			Siano $l_n \in \mathbb{R}$, $f_n$ e $f$ fuznioni da $I \subseteq \mathbb{R}$ in $\mathbb{R}$, $t_0 \in I$. Se
				\begin{enumerate}
					\item $f_n(t) \to f(t)$ per $n \to \infty$ uniformemente in $I$;
					\item $f_n(t) \to l_n$ per $t \to t_0$,
				\end{enumerate}
			allora esistono finiti i limiti
				\begin{equation*}
					\lim_{t\to t_0}f(t), \quad \lim_{n\to\infty} l_n
				\end{equation*}
			e sono uguali, cioè
				\begin{equation}
					\lim_{t\to t_0} \lim_{n \to \infty} f_n(t) = \lim_{n \to \infty} \lim_{t \to t_0} f_n(t).
				\end{equation}
		\end{teor}

		\begin{cor}
			\label{cor:contlim}
			Se la successione di funzioni continue $f_n \colon I \to \mathbb{R}$ converge uniformemente, il suo limite $f \colon I \to \mathbb{R}$ è una funzione continua.
		\end{cor}

		In generale, tale risultato è falso se la convergenza della successione $\{ f_n\}$ è puntuale, ma non uniforme; tale corollario è spesso utilizzato al negativo, per mostrare che se una successione di funzioni continue converge a una funzione discontinua, la convergenza non può essere uniforme.

		\begin{teor}[dello scambio del limite con la derivata]
			\label{teor:limder}
			Sia data una successione di funzioni $f_n \colon (a, b) \to \mathbb{R}$ derivabili; se
				\begin{enumerate}
					\item la successione delle derivate $\{ f'_n\}$ converge uniformemente in $(a, b)$ con limite $g;$
					\item la successione delle funzioni $\{f_n \}$ converge almeno in un punto $t_0 \in (a, b)$,
				\end{enumerate}
			allora anche la successione $\{f_n\}$ converge uniformemente in $(a, b)$. Sia $f$ il suo limite; risulta $f$ derivabile e $f' = g$, cioè
				\begin{equation}
					\lim_{n \to \infty} \frac{d}{dt}f_n(t) = \frac{d}{dt} \lim_{n \to \infty} f_n(t).
				\end{equation}
		\end{teor}

		\begin{cor}
			Nelle stesse ipotesi del teorema precedente, se $f_n \in C^1(a, b)$ anche il limite $f \in C^1(a, b)$.
		\end{cor}

		\begin{teor}[dello scambio del limite con l'integrale]
			Sia data una successione $\{ f_n\} \subset \mathcal{R}(a, b)$ di funzione limitate integrabili sull'intervallo $[a, b]$, convergente uniformemente con limite $f$; allora risulta $f \in \mathcal{R}(a, b)$ e
			 	\begin{equation}
						\label{eqn:intlim}
					\lim_{n\to \infty} \int_a^b f_n(t)\,dt = \int_a^b f(t)\,dt.
				\end{equation}
		\end{teor}

		In particolare, se $\{f_n\} \subset C^0([a, b])$, allora $f \in C^0([a, b])$ (per il Corollario~\ref{cor:contlim}) e vale la~\eqref{eqn:intlim}.

		\subsection{Applicazioni tra spazi metrici, teorema delle contrazioni}

		Siano $(X, d_X)$ e $(Y, d_Y)$ due spazi metrici, dotati ognuno della propria metrica $d_X$ e $d_Y$.
			\begin{defn}
				Un'applicazione $F \colon X \to Y$ si dice limitata se l'immagine $F(X)$ è un insieme limitato (nella metrica di $Y$); si dice continua in $x_0 \in X$ se, per ogni successione $\{x_n\}$ a valori in $X$ convergente a $x_0$ risulta
					\begin{equation*}
						\lim_{n\to \infty} F(x_n) = F(x_0);
					\end{equation*}
				si dice continua in $X$ se è continua in ogni punto di $X$.
			\end{defn}

			Più esplicitamente, diremo che $F \colon X \to Y$ è continua in $x_0 \in X$ se, fissato $\epsilon > 0$, è possibile trovare $\delta = \delta(x_0, \epsilon)$ tale che
				\begin{equation*}
					d_Y(F(x), F(x_0)) < \epsilon
				\end{equation*}
			se $d_X(x, x_0) < \delta$.

			Occupiamoci di una classe particolare di applicazioni continue.

				\begin{defn}
					Siano $(X, d_X)$ e $(Y, d_Y)$ spazi metrici. Un'applicazione $F \colon X \to Y$ si dice lipschitziana se esiste un numero $\rho > 0$ tale che, per ogni coppia $x_1, x_2$ di punti di $X$, si abbia
						\begin{equation*}
							d_Y(F(x_1), F(x_2)) \le \rho \, d_X(x_1, x_2).
						\end{equation*}
					Se risulta $\rho < 1$, $F$ si dirà contrazione di costante $\rho$.
				\end{defn}

				\begin{exmp}
					Sia $f \colon \mathbb{R}^n \to \mathbb{R}$ differenziabile. Se $\norma{\nabla f(\vec{x})} \le \rho < 1$, $\forall \vec{x} \in \mathbb{R}^n$, $f$ è una contrazione. Infatti, per il teorema del valor medio di Lagrange, si può scrivere
						\begin{equation*}
							f(\vec{x}) - f(\vec{y}) = (\nabla f (\vec{\xi}), \vec{x} - \vec{y})
						\end{equation*}
					con $\vec{\xi}$ opportuno sul segmento $[\vec{x}, \vec{y}]$. Poiché
						\begin{equation*}
							\abs{(\nabla f(\vec{\xi}), \vec{x} - \vec{y})} \le \norma{\nabla f(\vec{\xi})} \cdot \norma{\vec{x}- \vec{y}} \le \rho \, \norma{\vec{x} - \vec{y}}
						\end{equation*}
						si conclude che
							\begin{equation*}
								\abs{f(\vec{x}) - f(\vec{y})} \le \rho \, \norma{\vec{x}-\vec{y}}
							\end{equation*}
							e cioè che $f$ è una contrazione.
				\end{exmp}


			Particolarmente significativo è il caso $X = Y$.

			Data un'applicazione $F \colon X \to X$, spesso è importante stabilire se essa ammette dei \emph{punti fissi}, cioè punti che vengono trasformati in se stessi da $F$; in altre parole, $\bar{x}$ è punto fisso per $F$ se $F(\bar{x}) = \bar{x}$.

				\begin{teor}[di Banach-Caccioppoli, delle contrazioni]
					Sia $X$ uno spazio metrico completo ed $F \colon X \to X$ una contrazione di costante $\rho$. Allora $F$ ammette un unico punto fisso $\bar{x}$. Inoltre, dato un qualunque $x_0 \in X$, la successione definita ricorsivamente ponendo
						\begin{equation}
							x_{n+1} = F(x_n), \quad n \ge 0,
						\end{equation}
					converge a $\bar{x}$.
				\end{teor}

		\section{Serie di funzioni}
		\subsection{Generalità}

		Sia $\{ x_n\}$ una successione di funzioni che, per semplicità, supporremo reali di variabile reale ($x_n \colon I \to \mathbb{R}$ dove $I \subseteq \mathbb{R}$ è un intervallo). La successione delle \emph{somme parziali} $\{s_n\}$ così definita
			\begin{align*}
				s_0(t) &= x_0(t) \\
				s_1(t) &= x_0(t) + x_1(t) \\
				\vdots \\
				s_n(t) &= x_0(t) + x_1(t) + \dots + x_n(t) \\
				\vdots
			\end{align*}
		si dice serie di termini $x_n(t)$ e si indica col simbolo
			\begin{equation*}
				\sum_{n=0}^{\infty} x_n(t);
			\end{equation*}
		la serie si dirà convergente (puntualmente, uniformemente) in $I$ se converge (puntualmente, uniformemente) la successione delle somme parziali.

		\begin{exmp}
		Consideriamo la successione di funzioni $\{ t^n\}$, $t \in \mathbb{R}$; la corrispondente serie è la serie geometrica $\sum_{n=0}^{\infty}t^n$, che converge $\forall t \in (-1, +1)$ e ha come somma la funzione $s(t) = (1 - t)^{-1}$; diremo quindi che la serie geometrica converge puntualmente in $(-1, +1)$. Mostriamo che converge uniformemente in $[-r, r]$, con $0 < r < 1$: la successione delle somme parziali è
			\begin{equation*}
				s_n(t) = \frac{1 - t^{n+1}}{1 -t},
			\end{equation*}
		ed essendo $\abs{t} \le r$
			\begin{equation*}
				\abs{s(t) - s_n(t)} = \frac{\abs{t}^{n+1}}{1-t} \le \frac{r^{n+1}}{1-r}.
			\end{equation*}
		Poiché la successione numerica $\{ r^n / (1 -r)\}$ tende a $0$ per $n \to \infty$, fissatp $\epsilon > 0$ possiamo determinare $N = N(\epsilon)$ ($N$ indipendente da $t$) tale per cui risulti
			\begin{equation*}
				\abs{s(t) - s_n(t)} < \epsilon
			\end{equation*}
		se $n > N$, $\forall t \in [-r, r]$; ciò assicura la convergenza uniforme della serie data in $[-r, r]$.
		\end{exmp}

		Anche per le serie di funzioni vale il criterio di Cauchy: la serie
			\begin{equation*}
				\sum_{n = 0}^{\infty}x_n(t)
			\end{equation*}
		di funzioni $x_n \colon I \to \mathbb{R}$ converge puntualmente in $I$ se e solo se, fissato $\epsilon > 0$, $\forall t \in I$ si può determinare un indice $N = N(\epsilon, t)$ tale che, $\forall p \ge N$ e $\forall q \ge 0$, si abbia
			\begin{equation}
				\label{eqn:critcauchy}
				\abs{x_p(t) + x_{p+1}(t) + \dots + x_{p+q}(t)} < \epsilon.
			\end{equation}
		La convergenza è uniforme se e solo se esiste un indice $N = N(\epsilon)$ (indipendente da $t$) tale che la~\eqref{eqn:critcauchy} sia valida $\forall t \in I$, $\forall p \ge N$, $\forall q \ge 0$.

		Come corollario immediato si ricava che, se $\sum x_n(t)$ converge puntualmente (uniformemente) in $I$ allora $x_n(t) \to 0$ puntualmente (uniformemente) in $I$.

		\begin{prop}[Criterio di Weierstrass]
			Data la successione di funzioni $x_n \colon I \to \mathbb{R}$, se esistono costanti positive $c_n$ tali che
				\begin{enumerate}
					\item $\abs{x_n(t)} \le c_n$, $\forall t \in I$, definitivamente;
					\item $\sum c_n$ è convergente,
				\end{enumerate}
			allora la serie di funzioni $\sum x_n(t)$ converge uniformemente in $I$.
		\end{prop}

		In molte applicazioni, le funzioni $x_n$ sono elementi di uno spazio di Banach\footnote{Cioè uno spazio metrico normato e completo.} e in tal caso il criterio di Cauchy (necessario e sufficiente per la convergenza di $\sum x_n$) prende la forma seguente: fissato $\epsilon > 0$, esiste $N = N(\epsilon)$ tale che
			\begin{equation*}
				\abs{x_p + x_{p+1} + \dots + x_{p+q}} < \epsilon
			\end{equation*}
		$\forall p \ge N$, $\forall q \ge 0$.

		C'è anche un criterio sufficiente di convergenza simile al criterio di Weierstrass:
			\begin{teor}[della convergenza totale]
		Sia $\{ x_n\}$ una successione a valori in uno spazio di Banach. Se la serie delle norme $\sum \norma{x_n}$ è convergente, anche la serie $\sum x_n$ è convergente, e inoltre
			\begin{equation}
				\abs{\sum_{n=0}^{\infty} x_n} \le \sum_{n= 0}
		^{\infty} \norma{x_n}.
			\end{equation}
			\end{teor}

		Una serie per la quale converge la serie delle norme si dice \emph{totalmente convergente}.

		\begin{teor}[limite di una serie]
			Siano $f_n \colon I \to \mathbb{R}$ e $t_0 \in I$. Se
				\begin{enumerate}
					\item $\sum_{n=0}^{\infty} f_n(t)$ converge uniformemente in $I$ con somma $F(t)$;
					\item esistono finiti i limiti $\lim_{t \to t_0} f_n(t) = l_n$,
				\end{enumerate}
			allora la serie $\sum_{n=0}^{\infty} l_n$ converge e, detta $L$ la sua somma, risulta $\lim_{t \to t_0} F(t) = L$, cioè
			 	\begin{equation}
					\lim_{t \to t_0} \sum_{n=0}^{\infty} f_n(t) = \sum_{n=0}^{\infty} \lim_{t \to t_0} f_n(t).
				\end{equation}
		\end{teor}

		\begin{cor}
		La somma di una serie uniformemente convergente di funzioni continue è una funzione continua.
		\end{cor}

		\begin{teor}[derivata di una serie]
		Sia data una successione di funzioni $(a, b) \to \mathbb{R}$ derivabili; se
			\begin{enumerate}
				\item la serie delle derivate $\sum f'_n(t)$ è uniformemente convergente in $(a, b)$, con somma $G(t)$;
				\item la serie delle funzioni converge almeno in un punto $t_0 \in (a, b)$,
			\end{enumerate}
		allora anche la serie $\sum f_n(t)$ converge uniformemente in $(a, b)$; detta $F(t)$ la sua somma, risulta $F$ derivabile e $F' = G$, cioè
			\begin{equation}
				\frac{d}{dt} \sum_{n=0}^{\infty} f_n(t) = \sum_{n=0}^{\infty} f'_n(t).
			\end{equation}
		\end{teor}

		\begin{cor}
			Se le funzioni $f_n$ sono di classe $C^k$ in $(a, b)$ e le serie $\sum f_n$, $\sum f'_n$, $\dots$, $\sum f^{(k)}_n$ convergono uniformemente in $(a, b)$ con somma $F, G_1, \dots, G_k$ rispettivamente, allora anche $F$ è di classe $C^k$ in $(a, b)$ e risulta $F^{(i)} = G_i$, $i = 1, 2, \dots, k$.
		\end{cor}

		\begin{teor}[integrale di una serie]
			Sia data una successione $f_n$ di funzioni limitate e integrabili sull'intervallo $(a, b)$; se la serie $\sum f_n(t)$ converge uniformemente in $(a, b)$ con somma $F(t)$, allora $F$ è integrabile e si ha
				\begin{equation}
					\label{eqn:intserie}
					\int_a^b \sum_{n=0}^{\infty}f_n(t)\,dt = \sum_{n=0}^{\infty} \int_a^b f_n(t)\,dt.
				\end{equation}
		\end{teor}
		In particolare, se le $f_n$ sono continue su $[a, b]$, anche la somma $F$ è continua, perciò integrabile su $[a, b]$ e vale la~\eqref{eqn:intserie}.

		\subsection{Serie di potenze}
		Una classe particolare di serie di funzioni è formata dalle cosiddette \emph{serie di potenze}, cioè serie della forma
			\begin{equation*}
				\sum_{n=0}^{\infty} a_n(x - x_0)^n
			\end{equation*}
		dove $a_n$ sono numeri reali assegnati, $x$ varia in un insieme $A \subseteq \mathbb{R}$ e $x_0 \in A$ è dato.

		Tali serie trovano la loro collocazione più naturale nel campo complesso; considereremo fin da ora serie della forma
		 	\begin{equation*}
				\sum_{n=0}^{\infty} a_n(z- z_0)^n \quad a_n, z_0 \in \mathbb{C}
			\end{equation*}
		dove $z$ varia in un insieme $A \subseteq \mathbb{C}$. Si può assumere (tramite un cambiamento di variabile) $z_0 = 0$ e ricondursi allo studio di serie del tipo
			\begin{equation}
				\label{eqn:seriepotenze}
				\sum_{n=0}^{\infty}a_nz^n = a_0 + a_1z+a_2z^2 + \dots + a_n z^n + \dots
			\end{equation}
		È evidente che tutte le serie del tipo~\eqref{eqn:seriepotenze} convergono almeno in un punto, $z = 0$; si tratta di studiare la convergenza in altri punti.

			\begin{lem}
				Se una serie di potenze converge in un punto $z_0 \in \mathbb{C}$, allora converge (assolutamente) in ogni punto $z$ con $\abs{z} < \abs{z_0}$.
			\end{lem}

			Il lemma non ci dice solo che se la serie~\eqref{eqn:seriepotenze} converge in $z_0$ converge anche nel cerchio $\abs{z} < \abs{z_0}$, ma anche che se non converge in $z_0$ non può convergere neppure per $\abs{z} > \abs{z_0}$; perciò l'insieme dei punti $z \in \mathbb{C}$ in cui una serie del tipo~\eqref{eqn:seriepotenze} converge è un cerchio con centro nell'origine e un certo raggio $r$: tale cerchio si chiama \emph{cerchio di convergenza} ed $r$ si dice \emph{raggio di convergenza}.

			Diamo una definizione più precisa: sia $A \subseteq \mathbb{C}$ l'insieme di convergenza per la serie~\eqref{eqn:seriepotenze} e sia $E = \{ \abs{z} \colon z \in A \}$ l'insieme dei numeri reali non negativi che coincidono con i moduli dei numeri che stanno in $A$; tale insieme non è vuoto, perché contiene almeno lo zero. Definiamo il raggio di convergenza $r$ ponendo
				\begin{equation*}
					r \coloneqq \sup E.
				\end{equation*}
			Può essere:
				\begin{enumerate}
					\item $r = 0$; la serie converge solo per $z = 0$;
					\item $r = + \infty$; la serie converge per ogni $z \in \mathbb{C}$;
					\item $0 < r < +\infty$; la serie converge per $\abs{z} < r$, non converge per $\abs{z} > r$; non abbiamo informazioni per $\abs{z} = r$.
				\end{enumerate}

			All'interno del cerchio di convergenza, la serie converge assolutamente; possiamo quindi determinare il raggio di convergenza applicando i criteri di convergenza assoluta, ovvero studiando la convergenza della serie $\sum \abs{a_n} \, \abs{z}^n$.

			\begin{teor}[Criterio della radice]
				Data la serie~\eqref{eqn:seriepotenze}, sia
					\begin{equation}
						l = \varlimsup_{n\to\infty} \sqrt[n]{\abs{a_n}}.
					\end{equation}
				Allora $r = 1/l$ è il raggio di convergenza (s'intende che $r = 0$ se $l = + \infty$, $r = +\infty$ se $l = 0$).
			\end{teor}

			\begin{prop}[Criterio del rapporto]
				Data la serie~\eqref{eqn:seriepotenze}, se esiste il limite
					\begin{equation*}
						\lim_{n\to\infty} \frac{\abs{a_{n+1}}}{\abs{a_n}} = l
					\end{equation*}
				allora $r = 1/l$ è il raggio di convergenza.
			\end{prop}

			Consideriamo adesso le serie con raggio di convergenza $r > 0$; esse definiscono, per $\abs{z} < r$, una funzione $f(z)$ detta la loro \emph{somma}:
				\begin{equation}
					\label{eqn:sommaserie}
					f(z) = \sum_{n=0}^{\infty} a_n z^n, \quad \abs{z} < r.
				\end{equation}

				\begin{prop}
					\begin{enumerate}
						\item La serie~\eqref{eqn:sommaserie} converge uniformemente in ogni cerchio $\abs{z} \le r'$, con $r' < r$;
						\item la somma $f(z)$ è continua in $\abs{z} < r$.
					\end{enumerate}
				\end{prop}

				\begin{prop}
					\begin{enumerate}
						\item La serie delle derivate della serie~\eqref{eqn:sommaserie} è ancora una serie di potenze che ha lo stesso cerchio di convergenza della~\eqref{eqn:sommaserie};
						\item la somma $f(z)$ della serie~\eqref{eqn:sommaserie} è derivabile (in senso complesso) con derivata continua in $\abs{z} < r$; la sua derivata $f'(z)$ è la somma della serie delle derivate.
					\end{enumerate}
				\end{prop}

				\begin{prop}
					\begin{enumerate}
						\item La somma $f(z)$ della serie~\eqref{eqn:sommaserie} è di classe $C^{\infty}$ in $\abs{z} < r$;
						\item $\forall k \in \mathbb{N}$, la derivata $f^{(k)}(z)$ è la somma della serie ottenuta derivando la~\eqref{eqn:sommaserie} termine a termine $k$ volte;
						\item tra i coefficienti $a_k$ della serie~\eqref{eqn:sommaserie} e le derivate della somma $f(z)$ sussistono le seguenti relazioni:
							\begin{equation}
								a_k = \frac{1}{k!}f^{(k)}(0) \quad \forall k \in \mathbb{N}.
							\end{equation}
					\end{enumerate}
				\end{prop}

\chapter{Equazioni differenziali ordinarie}
\section{Concetti e teoremi fondamentali}
\subsection{Definizioni e terminologia}

Si chiama equazione differenziale ordinaria di ordine $n$ una relazione del tipo
	\begin{equation}
		\label{eqn:edo}
		F(t, y(t), y'(t), y''(t), \dots, y^{(n)}(t)) = 0,
	\end{equation}
con $F \colon \mathbb{R}^{n+2} \supseteq U \to \mathbb{R}$.

Nella~\eqref{eqn:edo} compare la funzione incognita $y = y(t)$ insieme alle sue derivate fino all'ordine $n$ incluso calcolate \emph{nello stesso punto}.

L'ordine di un'equazione è l'ordine massimo di derivazione che vi compare; l'aggettivo ordinaria si riferisce al fatto che l'incognita è funzione di una variabile.

Se nella~\eqref{eqn:edo} si può esplicitare la derivata di ordine massimo,
	\begin{equation}
		\label{eqn:edonorm}
		y^{(n)}(t) = f(t, y(t), y'(t), \dots, y^{(n-1)}(t)),
	\end{equation}
con $f \colon \mathbb{R}^{n-1} \supseteq D \to \mathbb{R}$, l'equazione si dice in forma normale.

Se nella~\eqref{eqn:edo} $F$ è un polinomio di primo grado in $y, y', \dots, y^{(n)}$ l'equazione si dice \emph{lineare}; la forma generale è
	\begin{equation}
		\label{eqn:edolin}
		a_0(t)y^{(n)}(t) + a_1(t)y^{(n-1)}(t) + \dots + a_{n-1}y'(t) + a_n(t)y(t) = b(t);
	\end{equation}
se $a_0(t) \ne 0$ la~\eqref{eqn:edolin} si può scrivere in forma normale.

Se $F$ nella~\eqref{eqn:edo} o $f$ nella~\eqref{eqn:edonorm} non dipendono esplicitamente da $t$ l'equazione si dice \emph{autonoma}.

D'ora in poi, ci riferiremo a equazioni in forma normale; diamo la nozione di soluzione locale di un'equazione differenziale.
	\begin{defn}
		Si dice soluzione (o integrale) dell'equazione~\eqref{eqn:edonorm} una funzione $\phi = \phi(t)$, definita e differenziabile $n$ volte in un intervallo $I \subseteq \mathbb{R}$, tale che $(t, \phi(t), \dots, \phi^{(n-1)}(t)) \in D$ e
			\begin{equation}
				\phi^{(n)}(t) = f(t, \phi(t), \phi'(t), \dots, \phi^{(n-1)}(t)) \quad \forall t \in I.
			\end{equation}
	\end{defn}

	Più generalmente, si può parlare di \emph{sistemi di equazioni differenziali ordinarie}, in più funzioni incognite, tutte di una sola variabile.

	Noi affronteremo il caso di sistemi di $n$ equazioni del 1° ordine in forma normale, in $n$ funzioni incognite; la forma generica è
		\begin{equation}
			\label{eqn:sistedo}
			\begin{dcases}
				\dot{y}_1 &= f_1 (t, y_1, y_2, \dots, y_n) \\
				\dot{y}_2 &= f_2 (t, y_1, y_2, \dots, y_n) \\
				\vdots \\
				\dot{y}_n &= f_n(t, y_1, y_2, \dots, y_n),
			\end{dcases}
		\end{equation}
	dove le funzioni $f_j$, $j = 1, 2, \dots, n$ sono definite in una stessa regione $D$ di $\mathbb{R}^{n+1}$ e $y_1 = y_1(t), \dots, y_n = y_n(t)$ sono le funzioni incognite.

	Introducendo i vettori $\vec{y}(t) = (y_1(t), \dots, y_n(t))$ e $\vec{f} = (f_1, f_2, \dots, f_n)$, il sistema~\eqref{eqn:sistedo} si può scrivere come
		\begin{equation}
			\dot{y}(t) = \vec{f}(t, \vec{y}(t)).
		\end{equation}
	Se ogni componente di $\vec{f}$ è lineare in $\vec{y}$ si dice che il sistema è lineare; se $\vec{f}$ non dipende esplicitamente da $t$ (cioè $\vec{f} = \vec{f}(\vec{y})$) il sistema si dice autonomo.

	\begin{defn}
		Si dice soluzione (o integrale) del sistema~\eqref{eqn:sistedo} una funzione $\vec{\phi} \colon I \to \mathbb{R}^n$, con $I$ intervallo di $\mathbb{R}$, differenziabile in $I$ tale che $(t, \phi(t)) \in D$ e
			\begin{equation*}
				\dot{\vec{\phi}}(t) = \vec{f}(t, \vec{\phi}(t)) \quad \forall t \in I.
			\end{equation*}
	\end{defn}

	Un'equazione differenziabile di ordine $n$ si può sempre ricondurre a un sistema di $n$ equazioni del prim'ordine.

	\subsubsection*{L'integrale generale}
	La ricerca delle primitive di una funzione $f$ continua su un intervallo equivale alla risoluzione dell'equazione differenziale
		\begin{equation*}
			\dot{y} = f(t),
		\end{equation*}
	che ha infinite soluzioni del tipo
		\begin{equation*}
			\int f(t) \, dt + c, \quad c \in \mathbb{R}.
		\end{equation*}

Anche un'equazione del tipo $\dot{y} = ky$ ammette una famiglia infinita di soluzioni dipendente da un parametro reale $c$, data da $y(t) = ce^{kt}$; è naturale pensare che, per un'equazione generica del tipo $\dot{y} = f(t,y)$, l'insieme delle soluzioni sia rappresentato da una famiglia a un parametro di funzioni $y = \phi (t, c)$.

Tale congettura è avvalorata dal fatto che a una famiglia di questo tipo, sotto ragionevoli ipotesi di regolarità per $\phi$, si può sempre associare un'equazione differenziale soddisfatta da \emph{ogni funzione della famiglia}.

Siano infatti $\phi \in C^1$ e $\partial \phi / \partial c \ne 0$; si consideri il sistema
\begin{equation*}
	\begin{dcases}
		&y = \phi(t,c) \\
		&\dot{y} = \frac{\partial \phi}{\partial t}(t, c),
	\end{dcases}
\end{equation*}
ove la seconda equazione è ottenuta derivando la prima rispetto a $t$. Per il teorema di Dini, si può ricavare dalla prima equazione $c = c(t, y)$, che sostituita nella seconda fornisce
	\begin{equation*}
		\dot{y} = \frac{\partial \phi}{\partial t}(t, c(t, y)) \coloneqq f(t, y).
	\end{equation*}

	Più generalmente, si può pensare che l'insieme delle soluzioni di un'equazione differenziale di ordine $n$ sia rappresentato da una famiglia di funzioni dipendente da $n$ parametri. Tale famiglia prende il nome di \emph{integrale generale}.

	\subsubsection*{Problema di Cauchy e problema ai limiti}

	Solitamente si preferisce focalizzarsi su soluzioni che soddisfano condizioni aggiuntive. Particolarmente importante è il \emph{problema di Cauchy} che ha la forma seguente:
		\begin{enumerate}
			\item per le equazioni scalari di ordine $n$: trovare $y \in C^n$ tale che
				\begin{equation*}
					\begin{dcases}
						&y^{(n)}(t) = f(t, y(t), y'(t), \dots, y^{(n-1)}(t)) \\
						&y(\tau) = \xi_0 \\
						&y'(\tau) = \xi_1 \\
						&\vdots \\
						&y^{(n-1)}(\tau) = \xi_{n-1}
					\end{dcases}
				\end{equation*}
				con $\tau, \xi_0, \dots, \xi_{n-1}$ costanti assegnate;
				\item per i sistemi: trovare un vettore $\vec{y} \in C^1$ tale che
				 	\begin{equation*}
						\begin{dcases}
							&\dot{\vec{y}} = \vec{f}(t, \vec{y}(t)) \\
							&\vec{y}(\tau) = \vec{\xi},
						\end{dcases}
					\end{equation*}
					con $\tau \in \mathbb{R}$ e $\vec{\xi} \in \mathbb{R}^n$ assegnati.
		\end{enumerate}

Le equazioni $y^{(j)}(\tau) = \xi_j$, $(j = 0, \dots, n-1)$ e $\vec{y}(\tau) = \vec{\xi}$ prendono il nome di \emph{condizioni iniziali}; le soluzioni si intendono definite localmente, cioè in un intorno (a priori non precisato) dell'istante iniziale $\tau$.

Vi sono anche altri tipi di condizioni aggiuntive; ne sono un esempio i \emph{problemi ai limiti}, che prescrivono condizioni agli estremi di un intervallo come ad esempio
	\begin{equation*}
		\begin{dcases}
			&\ddot{y} = f(t, y, \dot{y}) \\
			&y(a) = A \\
			&y(b) = B.
		\end{dcases}
	\end{equation*}
La seconda e la terza equazione richiedono al grafico della soluzione di ``passare'' per i due punti $(a, A)$ e $(b, B)$ del piano $(t, y)$, e perciò la soluzione deve essere definita in tutto l'intervallo $[a,b]$.

\subsection{Esistenza e unicità locale}
In questo paragrafo ci occuperemo di uno dei risultati fondamentali riguardanti l'esistenza e l'unicità della soluzione di un problema di Cauchy; poiché ogni equazione differenziale di ordine $n$ in forma normale si può ricondurre a un sistema di $n$ equazioni del prim'ordine, è sufficiente sviluppare la teoria per questi ultimi.

Sia $\vec{f} \colon \mathbb{R}^{n+1} \supseteq D \to \mathbb{R}^n$, con $D$ aperto, e si consideri il problema
	\begin{equation}
		\label{eqn:problemacauchy}
		\begin{dcases}
			&\dot{\vec{y}} = \vec{f}(t, \vec{y})\\
			&\vec{y}(\tau) = \vec{\xi}.
		\end{dcases}
	\end{equation}

Senza opportune ipotesi di regolarità su $\vec{f}$ non ci si possono aspettare, in generale, né esistenza né unicità delle soluzioni; un'ipotesi ragionevole è la continuità rispetto a entrambe le variabili $t$ e $\vec{y}$. Peano ha dimostrato che questa solo ipotesi è sufficiente a garantire l'esistenza di almeno una soluzione di~\eqref{eqn:problemacauchy}. L'unicità non è comunque assicurata; noi tratteremo simultaneamente esistenza e unicità introducendo un'ulteriore ipotesi su $\vec{f}$.

	\begin{defn}
Si dice che $\vec{f} = \vec{f}(t, \vec{y})$ è lipschitziana in $D$ rispetto a $\vec{y}$, uniformemente in $t$, se esiste una costante $L$ tale che
	\begin{equation}
		\label{eqn:lipschitziana}
		\norma{\vec{f}(t, \vec{y}) - \vec{f}(t, \vec{z})} \le L\norma{\vec{y} - \vec{z}}
	\end{equation}
	per ogni coppia di punti $(t, \vec{y})$ e $(t, \vec{z})$ in $D$. Si dice che $\vec{f} = \vec{f}(t, \vec{y})$ è localmente lipschitziana in $D$ rispetto a $\vec{y}$ uniformemente in $t$ se ogni punto di $D$ ha un intorno in cui vale la~\eqref{eqn:lipschitziana} (in tal caso, la costante $L$ può dipendere dall'intorno).
	\end{defn}

	Si verifica facilmente che $\vec{f}$ è localmente lipschitziana in $D$ se e solo se la~\eqref{eqn:lipschitziana} vale in ogni compatto contenuto in $D$. Inoltre $\vec{f} = (f_1, f_2, \dots, f_n)$ è lipschitziana (risp. localmente lipschitziana) se e solo se lo sono tutte le sue componenti $f_1, \dots, f_n$.

	La~\eqref{eqn:lipschitziana} sostanzialmente afferma che i rapporti incrementali di $\vec{f}$ rispetto alle variabili $y_1, \dots, y_n$ si mantenfono limitati, uniformemente rispetto a $t$.

		\begin{prop}
			Se $\vec{f}$ e tutte le derivate $\partial f_i / \partial y_s$ sono continue in $D$, allora $\vec{f}$ è localmente lipschitziana rispetto a $\vec{y}$, uniformemente in $t$.
		\end{prop}

		\begin{teor}[di esistenza e unicità locale]
			\label{teor:esistunicloc}
			 Sia $\vec{f} \colon D \to \mathbb{R}^n$, con $D$ aperto di $\mathbb{R}^{n+1}$. Se:
				\begin{enumerate}
					\item $\vec{f}$ è continua in $D$;
					\item $\vec{f}$ è localmente lipschitziana in $D$ rispetto a $\vec{y}$, uniformemente in $t$,
				\end{enumerate}
		allora, per ogni punto $(\tau, \vec{\xi}) \in D$ esiste un intorno $I_{\delta}$ di $\tau$, $I_{\delta} = [\tau - \delta, \tau + \delta]$, nel quale è definita una soluzione $\vec{\phi}$ del problema di Cauchy~\eqref{eqn:problemacauchy}. Tale soluzione è unica nel senso che ogni altra soluzione coincide con $\vec{\phi}$ nell'intervallo comune di definizione.
			\end{teor}

			Si noti che, essendo $\vec{f}$ continua in $D$, la soluzione $\vec{\phi}$ risulta non solo differenziabile in $I_{\delta}$ ma anche di classe $C^1(I_{\delta})$. Infatti, dall'equazione differenziale si ha $\dot{\vec{\phi}}=\vec{f}(t, \vec{\phi}(t))$ ed $\vec{f}(t, \vec{\phi}(t))$ è continua in $I_{\delta}$ come composta di funzioni continue.

			Dalla dimostrazione del teorema si ricava un'informazione riguardante l'intervallo di esistenza della soluzione: essa esiste almeno in $I_{\delta} = [\tau - \delta, \tau + \delta]$, con $\delta < \min\{a, b/M, 1/L\}$; in seguito affronteremo la questione della prolungabilità di una soluzione a un intervallo massimale di esistenza.

			\begin{prop}
				Nelle ipotesi del precedente Teorema, la seguente successione definita pe rricorrenza converge uniformemente in $I_{\delta}$ alla soluzione del problema di Cauchy~\eqref{eqn:problemacauchy}:
					\begin{equation}
						\vec{\phi}_0(t) = \vec{\xi}, \quad \vec{\phi}_{n+1} = \vec{F}[\vec{\phi}_n] = \vec{\xi} + \int_{\tau}^t \vec{f}(s, \vec{\phi}_n(s))\,ds,
					\end{equation}
				con $n \ge 0$.
			\end{prop}

			Per la costruzione delle soluzioni si può quindi usare tale successione; questo metodo prende il nome di \emph{metodo delle approssimazioni successive} (anche se il suo interesse è soprattutto teorico, date le difficoltà date legate al calcolo degli integrali a ogni passo).

			\begin{prop}[regolarità delle soluzioni] Se $\vec{f} \in C^k(D)$, $k$ intero non negativo, allora una soluzione $\vec{\phi}$ dell'equazione $\dot{\vec{y}} = \vec{f}(t, \vec{y})$ è di classe $C^{k+1}$ nel suo dominio; dunque se $\vec{f} \in C^{\infty}(D)$, anche $\vec{\phi}$ è di classe $C^{\infty}$. Infine, se $\vec{f}$ è analitica in $D$, anche $\vec{\phi}$ lo è.

			\end{prop}

\subsection{Prolungamento delle soluzioni, esistenza e unicità globale}

Dalla dimostrazione del Teorema~\ref{teor:esistunicloc}, risulta che l'esistenza della soluzione $\vec{\phi}$ del problema di Cauchy~\eqref{eqn:problemacauchy} è garantita nell'intervallo $[\tau - \delta, \tau + \delta]$, con $\delta < \min\{a, b/M, 1/L\}$.

$M$ ed $L$ sono rispettivamente il massimo di $\norma{\vec{f}(t, \vec{y})}$ e la costante di Lipschitz per $\vec{f}$ relativamente al cilindro $\Gamma = \{(t, \vec{y}) \in \mathbb{R}^{n+1} \colon \abs{t-\tau} \le a, \norma{\vec{y}-\vec{\xi}} \le b\}$. Raffinando la dimostrazione, si può arrivare a
 	\begin{equation}
		\delta = \min \Biggl{\{}a, \frac{b}{M} \Biggr{\}}.
	\end{equation}

	In ogni caso, l'intervallo di esistenza della soluzione è più ampio di $[\tau - \delta, \tau + \delta]$. Consideriamo infatti il punto $(\tau_1, \vec{\xi}_1) = (\tau + \delta, \vec{\phi}(\tau + \delta))$. Essendo un punto di $D$, esiste un cilindro
		\begin{equation*}
			\Gamma_1 = \{(t, \vec{y}) \in \mathbb{R}^{n+1} \colon \abs{t - \tau_1} \le a_1, \norma{\vec{y} - \vec{\xi}_1} \le b_1 \} \in D.
		\end{equation*}
	In base al Teorema~\ref{teor:esistunicloc} il problema di Cauchy con dati iniziali $(\tau_1, \vec{\xi}_1)$ ha una soluzione $\vec{\phi}_1$ definita in un intervallo $I_{\delta_1} = [\tau_1 - \delta_1, \tau_1 + \delta_1]$ dove
		\begin{equation*}
			\delta_1 = \min \{a_1, b_1/M_1 \}, \quad M_1 = \max_{(t, \vec{y}) \in \Gamma_1} \norma{\vec{f}(t, \vec{y})}.
		\end{equation*}

		Per l'unicità delle soluzioni, si deve avere $\vec{\phi} = \vec{\phi}_1$ in $I_{\delta} \cap I_{\delta_1}$; dunque la nuova soluzione appare come un \emph{prolungamento} (a destra) della vecchia, che possiamo quindi ritenere definita in $[\tau, \tau + \delta + \delta_1]$.

		Continuando a ripetere il procedimento, si prolunga (a destra) $\vec{\phi}$ a un intervallo ogni volta più ampio e analogamente si potrà prolungare $\vec{\phi}$ sulla sinistra; in casi particolari (non è vero in generale) si arriverà a contenere l'intervallo $[\tau - a, \tau + a]$ relativo al cilindro $\Gamma$.

		In generale, data una soluzione $\vec{\phi}_t(\tau, \vec{\xi})$\footnote{Utilizziamo questa notazione per evidenziare la condizione iniziale.}, esiste un intervallo massimale destro $J^+_{\vec{\phi}}$ alla destra del quale $\vec{\phi}$ non è prolungabile, definito come
			\begin{equation*}
				J_{\vec{\phi}}^+ \coloneqq [\tau, T_\textup{max}),
			\end{equation*}
		dove $T_\textup{max}$ è l'estremo superiore dei $t$ tali che $\vec{\phi}$ sia prolungabile in $[\tau, t]$.

		Analogamente, per il prolungamento a sinistra esiste un intervallo massimale sinistro
			\begin{equation*}
				J_{\vec{\phi}}^- \coloneqq (T_\textup{min}, \tau]
			\end{equation*}
		alla sinistra del quale $\vec{\phi}$ non si può prolungare. Porremo $J_{\vec{\phi}} \coloneqq J_{\vec{\phi}}^+ \cup J_{\vec{\phi}}^-$.

		Poiché per il teorema di esistenza e unicità il prolungamento di una soluzione è unico, si conclude che per ogni punto $(\tau, \vec{\xi})$ esiste un'unica soluzione non prolungabile del problema di Cauchy~\eqref{eqn:problemacauchy} il cui grafico sia contenuto in $D$. In altri termini, $\vec{\phi}_t(\tau, \vec{\xi})$ è prolungabile in modo unico a un intervallo massimale di esistenza $J_{\vec{\phi}} = (T_\textup{min}, T_\textup{max})$.

		Occupiamoci dell'intervallo massimale di esistenza per la soluzione del problema di Cauchy~\eqref{eqn:problemacauchy}; un primo risultato riguarda l'esistenza della soluzione in un intervallo prefissato $[\tau_1, \tau_2]$.

		\begin{teor}[di esistenza globale]
Sia $S \coloneqq (\tau_1, \tau_2) \times \mathbb{R}^n$. Supponiamo che $\vec{f}$ sia definita in $\overline{S}$ e che in $S$ valgano le ipotesi del Teorema~\ref{teor:esistunicloc}. Se inoltre esistono due costanti $k_1 \ge 0$ e $k_2 \ge 0$ tali che
	\begin{equation}
		\label{eqn:esistglob}
		\norma{\vec{f}(t, \vec{y})} \le k_1 + k_2\norma{y} \quad \forall (t, \vec{y}) \in \overline{S},
	\end{equation}
allora, per ogni $(\tau, \vec{\xi}) \in S$, $\vec{\phi}_t(\tau, \vec{\xi})$ è definita in $[\tau_1, \tau_2]$.
		\end{teor}

La~\eqref{eqn:esistglob} è soddisfatta in particolare se vale una delle condizioni elencate:
	\begin{enumerate}
		\item $\vec{f}$ è limitata in $\overline{S}$;
		\item $\vec{f}(t, \vec{0})$ è limitata e $\vec{f}$ è lipschitziana in $\overline{S}$ rispetto a $\vec{y}$ uniformemente in $\vec{t}$;
		\item $\vec{f}(t, \vec{0})$ è limitata e tutte le derivate $\partial f_j / \partial y_j$ sono continue e limitate in $\overline{S}$.
	\end{enumerate}

	Un altro teorema di prolungamento è il seguente, che esprime la prolungabilità della soluzione fino alla frontiera di $D$.
	\begin{teor}
		\label{teor:prolfront}
		Valgano le ipotesi del Teorema~\ref{teor:esistunicloc}. Siano $D_0$ un compatto contenuto in $D$, $(t, \xi)$ un punto in $D_0$ e $J_{\vec{\phi}} = (T_\textup{min}, T_\textup{max})$ l'intervallo massimale di esistenza di $\vec{\phi}_t(\tau, \vec{\xi})$. Allora il grafico di $\vec{\phi}$ esce da $D_0$ definitivamente per $t \to T_\textup{min}^+$ e per $t \to T_\textup{max}^+$.
\end{teor}

\begin{cor}
Valgano le ipotesi del Teorema~\ref{teor:prolfront} con $D = \mathbb{R}^{n+1}$. Se esistono due costanti $c_1 \ge 0$ e $c_2 \ge 0$ tali che
	\begin{equation}
		\norma{\vec{\phi}(t)} \le c_1 + c_2(t - \tau), \quad \forall t \in J_{\vec{\phi}}^+ = [\tau, T_\textup{max}),
	\end{equation}
allora $J_{\vec{\phi}}^+ = [ \tau, \infty)$. In particolare, ciò accade se $\vec{\phi}$ è limitata in $J_\vec{\phi}^+$ (con $c_2 = 0$).
\end{cor}

\subsection{Integrazione di alcune equazioni del primo ordine}

\subsubsection*{Equazioni lineari}
Sono le equazioni del tipo
	\begin{equation}
		\label{eqn:edolin}
		\dot{y} = P(t)y + Q(t),
	\end{equation}
dove $P$ e $Q$ sono continue in un intervallo $I \subseteq \mathbb{R}$. Se $Q = 0$ la~\eqref{eqn:edolin} si dice lineare omogenea.

La formula valida in $I$ che dà l'integrale generale è
	\begin{equation}
		\label{eqn:soledolin}
y(t) = e^{\int P(t)\,dt}\Biggl( c + \int Q(t) e^{-\int P(t)\,dt}\,dt \Biggr),
	\end{equation}
dove $c$ è una costante arbitraria e
	\begin{equation*}
		\int P(t)\,dt
	\end{equation*}
è una primitiva di $P$ in $I$.

\subsubsection*{Equazioni differenziali esatte}
Sono equazioni del tipo
	\begin{equation}
		\label{eqn:edoesatta}
		\dot{y} =  - \frac{P(t,y)}{Q(t, y)},
	\end{equation}
dove $P$ e $Q$ sono continue in un dominio $A \subseteq \mathbb{R}^2$, $Q \ne 0$, tali che la forma differenziale
	\begin{equation*}
		\omega = P(t,y)\,dt + Q(t,y)\,dy
	\end{equation*}
sia esatta, cioè esista $F=F(t,y)$ funzione potenziale tale che
	\begin{equation*}
		\frac{\partial F}{\partial t} = P, \quad \frac{\partial F}{\partial y} = Q
	\end{equation*}
in $A$.\footnote{Ricordiamo che, se $A$ è semplicemente connesso in $\mathbb{R}^2$ e se
	\begin{equation*}
		\frac{\partial Q}{\partial t}, \quad \frac{\partial P}{\partial y}
	\end{equation*}
sono continue in $A$, $\omega$ è esatta se e solo se in $A$ risulta
	\begin{equation*}
		\frac{\partial Q}{\partial t} = \frac{\partial P}{\partial y}.
	\end{equation*}}

	In tal caso, una formula per la funzione $F$ è per esempio la seguente:
		\begin{equation}
			F(t,y) = \int_{\tau}^t P(s, y)\,ds + \int_{\xi}^y Q(\tau, s)\,ds,
		\end{equation}
	valida (almeno) in un intorno di $(\tau, \xi) \in A$. L'integrale generale della~\eqref{eqn:edoesatta} è dato in forma implicita dalla formula
		\begin{equation}
			F(t, y) = c.
		\end{equation}

		\subsubsection*{Equazioni a variabili separabili}
		Sono della forma
			\begin{equation}
				\label{eqn:edovarsep}
				\dot{y} = f(t)\,g(y),
			\end{equation}
		con $f \in C(I)$ e $g \in C(J)$, ove $I$ e $J$ sono intervalli contenuti in $\mathbb{R}$.

		Se $\bar{y}$ è soluzione dell'equazione $g(y) = 0$ la retta $y= \bar{y}$ è una curva integrale. Se $g(y) \ne 0$ in  $J' \subseteq J$, la~\eqref{eqn:edovarsep} è esatta con $P(t,y) = f(t)$ e $Q(t,y) = -1/g(y)$. L'integrale generale in tal caso è dato dalla formula
		 	\begin{equation}
				\int \frac{1}{g(y)}\,dy = \int f(t)\,dt + c.
			\end{equation}

			\subsubsection{Equazioni di Bernoulli}
			Sono equazioni del tipo
				\begin{equation}
					\dot{y} = P(t)y + Q(t)y^{\alpha},
				\end{equation}
			con $\alpha \in \mathbb{R}$, $\alpha \ne 0$, $\alpha \ne 1$ e con $P$ e $Q$ continue in un intervallo $I \subseteq \mathbb{R}$.

			Se $\alpha > 0$, $y = 0$ è soluzione dell'equazione. Se $y \ne 0$, dividendo per $y^{\alpha}$ si ottiene $y^{-\alpha}\dot{y} = P(t)y^{1-\alpha} + Q(t)$. Ponendo
				\begin{equation*}
					z = y^{1 - \alpha},
				\end{equation*}
			si ha
				\begin{equation*}
					\dot{z} = (1- \alpha)y^{-\alpha}\dot{y},
				\end{equation*}
			e perciò $z$ è soluzione dell'equazione lineare
				\begin{equation}
					\dot{z} = (1 - \alpha)P(t)z + (1 - \alpha)Q(t).
				\end{equation}
			Trovata $z$ con la formula~\eqref{eqn:soledolin} si trova\footnote{$y = \pm z^{\frac{1}{1-\alpha}}$ se $1 - \alpha$ è un intero pari o un razionale con numeratore pari.} $y = z^{\frac{1}{1-\alpha}}$.

			\subsubsection{Equazioni di Riccati}
			Sono equazioni del tipo
			 	\begin{equation}
					\label{eqn:edoriccati}
					\dot{y} = P(t)y + Q(t)y^2 + R(t),
				\end{equation}
			con $P, Q, R$ continue in un intervallo $I \subseteq \mathbb{R}$, $R(t) \ne 0$; non si conoscono metodi generali di soluzione.

			Conoscendo una soluzione particolare $\psi$, la~\eqref{eqn:edoriccati} si riduce a lineare con il cambio di variabili
				\begin{equation*}
					y = \psi + \frac{1}{z}.
				\end{equation*}
Si ha infatti $\dot{y} = \dot{psi} - \dot{z}z^{-2}$ che, sostituita nella~\eqref{eqn:edoriccati}, dà per $z$ l'equazione lineare
	\begin{equation*}
		\dot{z} = [P(t) + 2Q(t)\psi]z + Q(t).
	\end{equation*}

	\subsubsection*{Equazioni omogenee (o di Manfredi)}
	Sono equazioni della forma
		\begin{equation}
			\label{eqn:edomanfredi}
			\dot{y} = f\Biggr(\frac{y}{y}\Biggr),
		\end{equation}
	o alternativamente $\dot{y} = \phi(t, y)$ con $\phi$ positivamente omogenea di grado zero.\footnote{Cioè $\phi(\lambda t, \lambda y) = \phi(t, y)$, $\lambda > 0$.}
	Assumiamo che $f$ e $\phi$ siano continue in un intervallo e in un aperto di $\mathbb{R}^2$ rispettivamente.

	Ponendo
		\begin{equation}
			\label{eqn:edomanfrsost}
			z(t) = \frac{y(t)}{t},
		\end{equation}
	ossia $y(t) = tz(t)$, si trova $\dot{y} = z + t\dot{z}$, che sostituita nella~\eqref{eqn:edomanfredi} dà per $z$ l'equazione a variabili separabili
		\begin{equation*}
			\dot{z} = \frac{f(z) - z}{t};
		\end{equation*}
	risolta l'equazione in $z$, si determina $y$ dalla~\eqref{eqn:edomanfrsost}.







		\chapter{Misura e integrazione}
		\section{Integrale multiplo secondo Riemann}
		\subsection{Integrale doppio per funzioni definite su un rettangolo}
		Sia $Q = [a, b] \times [c, d]$ un rettangolo e siano
		\begin{align*}
			\mathcal{D}_1 &= \{ x_0 = a, x_1, \dots, x_{r-1}, x_r = b\} \\
			\mathcal{D}_2 &= \{ y_0 = c, y_1, \dots, y_{s-1}, y_s = d \}
		\end{align*}
		due suddivisioni di $[a, b]$ e $[c, d]$ rispettivamente. Il prodotto cartesiano $\mathcal{D} = \mathcal{D}_1 \times \mathcal{D}_2$ è chiamato \emph{suddivisione} o \emph{partizione} di $Q$; si pone
		\begin{align*}
			I_k = [x_{k-1}, x_k], \quad \Delta x_k = x_k - x_{k-1}, \quad k = 1, 2, \dots, r \\
			J_h = [y_{h-1}, y_h], \quad \Delta y_h = y_h - y_{h-1}, \quad h = 1, 2, \dots, s.
		\end{align*}

		Il rettangolo $Q$ è decomposto nell'unione degli $rs$ rettangoli $Q_{jh} = I_k \times J_h$; consideriamo adesso una funzione $f \colon Q \to \mathbb{R}$ limitata:
		\begin{equation}
			\label{eqn:flimitata}
			m \le f(x, y) \le M,
		\end{equation}
		per ogni $(x, y) \in Q$.
		Per $k = 1, 2, \dots, r$ e $h = 1, 2, \dots, s$ poniamo
		\begin{equation}
			m_{kh} = \inf_{Q_{kh}}f, \quad M_{kh} = \sup_{Q_{kh}} f.
		\end{equation}

		Definiamo la somma inferiore e la somma superiore di $f$ relativamente a $\mathcal{D}$ come, rispettivamente,
		\begin{align*}
			s &= s(\mathcal{D}, f) = \sum_{k=1}^r\sum_{h=1}^s m_{kh}\Delta x_k \Delta y_h, \\
			S &= S(\mathcal{D}, f) = \sum_{k=1}^r \sum_{h=1}^s M_{kh} \Delta x_k \Delta y_h.
		\end{align*}

		Per la~\eqref{eqn:flimitata} risulta, per ogni suddivisione $\mathcal{D}$ di $Q$,
		\begin{equation*}
			m(b -a )(c-d) \le s(\mathcal{D}, f) \le S(\mathcal{D}, f) \le M(b-a)(c-d),
		\end{equation*}
		e sono perciò ben definite le quantità
		\begin{equation*}
			\inf_{\mathcal{D}}S(\mathcal{D}, f), \quad \sup_{\mathcal{D}}s(\mathcal{D}, f),
		\end{equation*}
		ove i due estremi sono cercati al variare di tutte le possibili suddivisioni di $Q$.

		Come nel caso unidimensionale, si può mostrare che
		\begin{equation*}
			\sup_{\mathcal{D}} s(\mathcal{D}, f) \le \inf_{\mathcal{D}} S(\mathcal{D}, f).
		\end{equation*}

		\begin{defn}
			Una funzione $f \colon Q \to \mathbb{R}$ limitata si dice \emph{integrabile} secondo Riemann se
			\begin{equation*}
				\sup_{\mathcal{D}} s(\mathcal{D}, f) = \inf_{\mathcal{D}} S(\mathcal{D}, f).
			\end{equation*}
		\end{defn}

		Il valore comune dei due estremi si chiama \emph{integrale} di Riemann di $f$ in $Q$ e si denota con i simboli
		\begin{equation*}
			\mathcal{I}(Q, f), \quad \int_Q f, \quad \iint_Q f(x,y)\, dxdy, \quad \int_a^b \int_c^d f(x,y) \, dxdy.
		\end{equation*}
		$Q$ è detto dominio d'integrazione ed $f$ funzione integranda: si noti che le variabili $x$ e $y$, come nel caso unidimensionale, sono mute e si può utilizzare una qualsiasi altra coppia di variabili senza modificare il valore dell'integrale.

		La classe delle funzioni limitate integrabili secondo Riemann su $Q$ si indica con il simbolo $\mathcal{R}(Q)$.

		\begin{teor}
			Sia $f \colon Q \to \mathbb{R}$ limitata. Allora $f \in \mathcal{R}(Q)$ se e solo se, per ogni $\epsilon > 0$, esiste una suddivisione $\mathcal{D}_{\epsilon}$ di $Q$ tale che
			\begin{equation}
				S(\mathcal{D}_{\epsilon}, f) - s(\mathcal{D}_{\epsilon}, f) < \epsilon.
			\end{equation}
		\end{teor}

		\begin{teor}
			Se $f$ è continua in $Q$, allora è integrabile in $Q$.
		\end{teor}
		\subsection{Calcolo di un integrale doppio mediante due integrazioni semplici}
		\begin{teor}[di riduzione]
			Sia $f \in \mathcal{R}(Q)$, $Q = [a, b] \times [c, d]$.
			\begin{enumerate}
				\item Se, per ogni $y \in [c, d]$, esiste l'integrale
				\begin{equation*}
					\int_a^b f(x, y) \, dx,
				\end{equation*}
				allora la funzione $y \mapsto G(y)$ è integrabile in $[c, d]$ e vale la formula
				\begin{equation}
					\label{eqn:iteratox}
					\iint_Q f = \int_c^d G(y)\, dy = \int_c^d \biggl( \int_a^b f(x, y) \, dx \biggr) \, dy.
				\end{equation}

				\item Se, per ogni $x \in [a, b]$, esiste l'integrale
				\begin{equation*}
					\int_c^d f(x,y) \, dy,
				\end{equation*}
				allora la funzione $x \mapsto H(x)$ è integrabile in $[a, b]$ e vale la formula
				\begin{equation}
					\label{eqn:iteratoy}
					\iint_Q f = \int_a^b H(x) \, dx = \int_a^b \biggl( \int_c^d f(x,y) \, dy \biggr) \, dx.
				\end{equation}
			\end{enumerate}
		\end{teor}

		\begin{oss}
			Le formule~\eqref{eqn:iteratox} e~\eqref{eqn:iteratoy} prendono il nome di \emph{formule di riduzione}; gli integrali a destra si dicono \emph{integrali iterati}. Se possono essere applicate entrambe le formule, si ottiene
			\begin{equation}
				\int_c^d \biggl( \int_a^b f(x,y) \, dx \biggr) \, dy = \int_a^b \biggl( \int_c^d f(x,y) \,dy \biggr) \, dx,
			\end{equation}
			detta \emph{formula di scambio dell'ordine d'integrazione}.
		\end{oss}

		Si noti che l'esistenza dell'integrale doppio \emph{non} implica l'esistenza degli integrali iterati, che dev'essere provata prima di applicare le formule di riduzione.

		Se però $f$ è continua in $Q$ (e perciò ivi integrabile), per ogni $y \in [c,d]$ la funzione $x \mapsto f(x, y)$ è continua in $[a, b]$ e perciò integrabile; esiste cioè per ogni $y \in [c, d]$ l'integrale
		\begin{equation*}
			\int_a^b f(x,y) \, dx.
		\end{equation*}
		Analogamente, esiste l'integrale
		\begin{equation*}
			\int_c^d f(x,y) \, dy
		\end{equation*}
		per ogni $x \in [a, b]$. Le ipotesi del teorema sono così verificate ed è possibile utilizzare entrambe le formule~\eqref{eqn:iteratox} e~\eqref{eqn:iteratoy}.

		\begin{oss}
			Un caso particolare si presenta quando $f(x,y) = g(x)h(y)$ (si dice che le variabili sono separate). Se $g \in \mathcal{R}(a,b)$ e $h \in \mathcal{R}(c, d)$, allora $f \in \mathcal{R}(Q)$ e la formula di riduzione vale nella forma
			\begin{equation}
				\iint_Q f = \int_a^b g(x) \, dx \int_c^d h(y)\,dy.
			\end{equation}
		\end{oss}

		\subsection{Integrali su regioni più generali, misura di Peano-Jordan}
		Consideriamo regioni $\Omega$ del piano limitate e funzioni $f \colon \Omega \to \mathbb{R}$ limitate.

		Racchiudiamo $\Omega$ in un rettangolo $Q$ e definiamo una nuove funzione $\tilde{f}$ come segue:
		\begin{equation}
			\tilde{f}(x, y) = \begin{dcases*}
			f(x,y) &se $(x, y) \in \Omega$ \\
			0 &se $(x, y) \in Q\setminus \Omega$.
		\end{dcases*}
	\end{equation}

	\begin{defn}
		Si dice che $f$ è integrabile in $\Omega$ (secondo Riemann) se $\tilde{f}$ è integrabile in $Q$ e si pone
		\begin{equation*}
			\iint_{\Omega} f \coloneqq \iint_Q \tilde{f}.
		\end{equation*}
	\end{defn}

	Dato $\Omega \subset \mathbb{R}^2$ limitato, la sua \emph{funzione caratteristica} è definita come
	\begin{equation*}
		\mathbf{1}_{\Omega}(x, y) = \begin{dcases*}
		1 &se $(x, y) \in \Omega$ \\
		0 &se $(x, y) \notin \Omega$.
	\end{dcases*}
\end{equation*}

\begin{defn}
	Un sottoinsieme $\Omega \subset \mathbb{R}^2$ limitato si dice \emph{misurabile} (secondo Peano-Jordan) se $\mathbf{1}_{\Omega} \in \mathcal{R}(\Omega)$. Chiameremo \emph{area} o \emph{misura} di $\Omega$ il numero non negativo
	\begin{equation}
		\abs{\Omega} \coloneqq \mathcal{I}(\mathbf{1}_{\Omega}) =  \iint_{\Omega} dxdy.
	\end{equation}
\end{defn}

Un insieme $Z$ ha \emph{misura nulla} se $\mathbf{1}_Z$ è integrabile e
\begin{equation*}
	\iint_Z dxdy = 0.
\end{equation*}

\begin{prop}[Caratterizzazione degli insiemi di misura nulla]
	Un insieme $Z \subset \mathbb{R}^2$ è di misura nulla se e solo se per ogni $\epsilon > 0$ esiste un numero finito $N_{\epsilon}$ di rettangoli, $Q_1, Q_2, \dots, Q_{N_\epsilon}$ tali che:
	\begin{equation*}
		Z \subset \bigcup_{j=1}^{N_{\epsilon}} Q_j, \quad
		\sum_{j=1} ^{N_{\epsilon}} \abs{Q_j} < \epsilon.
	\end{equation*}
\end{prop}

\begin{prop}[Caratterizzazione degli insiemi secondo Peano-Jordan]
	Un insieme $\Omega \subset \mathbb{R}^2$ limitato è misurabile secondo Peano-Jordan se e solo se $\partial \Omega$ è misurabile e $\abs{\partial \Omega} = 0$.
\end{prop}

\begin{exmp}
	I seguenti insiemi di $\mathbb{R}^2$ sono di misura nulla.
	\begin{enumerate}
		\item Un insieme costituito da un numero finito di punti.
		\item Un segmento di retta.
		\item Se $Z_1$ è misurabile, $Z_1 \subset Z$ e $Z$ ha misura nulla, allora anche $Z_1$ ha misura nulla.
		\item L'unione di un numero finito di insiemi di misura nulla (in particolare, il bordo di un poligono di $m$ lati).
	\end{enumerate}
\end{exmp}

\begin{prop}
	Sia $g \colon [a, b] \to \mathbb{R}$ limitata e integrabile. Allora il suo grafico ha misura nulla.
\end{prop}

\begin{oss}
	La precedente Proposizione si estende alle curve regolari $\gamma \colon [a, b] \to \mathbb{R}^3$: i sostegni di tali curve, come insieme di punti in $\mathbb{R}^3$ (in $\mathbb{R}^2$ se la curva è piana) hanno misura nulla (misura che non va confusa con la lunghezza della curva, sempre positiva).
\end{oss}

Inoltre, il grafico di una funzione \emph{continua} su un intervallo $[a, b]$ ha misura nulla e risultano misurabili regioni del tipo
\begin{equation}
	\{(x,y) \in \mathbb{R}^2  \colon x \in [a, b], \ g_1 (x) \le y \le g_2(x)\},
\end{equation}
con $g_1$ e $g_2$ continue, e
\begin{equation}
	\{(x,y) \in \mathbb{R}^2 \colon y \in [c,d], \ h_1(y) \le x \le h_2(y) \},
\end{equation}
con $h_1$ e $h_2$ continue. Tali insiemi si dicono $semplici$ rispetto all'asse $y$ e all'asse $x$ rispettivamente.

\subsection{Funzioni generalmente continue}
\begin{defn}
	Siano $Q$ un rettangolo, $f \colon Q \to \mathbb{R}$ limitata. Si dice che $f$ è \emph{generalmente continua} se l'insieme dei suoi punti di discontinuità ha misura nulla.
\end{defn}

\begin{teor}
	Sia $f \colon Q \to \mathbb{R}$ limitata. Se $f$ è generalmente continua, allora è integrabile.
\end{teor}

Si ricava subito che:
\begin{enumerate}
	\item le funzioni continue su un compatto $\Omega$ misurabile sono integrabili in $\Omega$;
	\item le funzioni limitate e continue su un aperto $\Omega$ misurabile sono integrabili in $\Omega$.
\end{enumerate}

\begin{prop}
	Sia $\Omega$ una regione semplice rispetto a uno dei due assi (eventualmente a entrambi). Se $f \colon \Omega \to \mathbb{R}$ è limitata e continua in $\overset{\circ}{\Omega}$, allora è integrabile in $\Omega$ e valgono le seguenti formule:
	\begin{align}
		\iint_{\Omega} f = \int_a^b \biggl( \int_{g_1(x)}^{g_2(x)} f(x, y) \, dy \biggr) \, dx \quad \text{($\Omega$ semplice rispetto all'asse $y$)} \\
		\iint_{\Omega} f = \int_c^d \biggl(\int_{h_1(y)}^{h_2(y)} f(x,y) \, dx \biggr) \, dy \quad \text{($\Omega$ semplice rispetto all'asse $x$)}
	\end{align}
\end{prop}

\subsection{Proprietà dell'integrale}
\begin{teor}
	Siano $\Omega$ limitato in $\mathbb{R}^2$, $f, g \in \mathcal{R}(\Omega)$, $\alpha, \beta \in \mathbb{R}$.
	\begin{enumerate}
		\item Linearità: $\alpha f + \beta g \in \mathcal{R}(\Omega)$ e
		\begin{equation*}
			\iint_{\Omega} (\alpha f + \beta g) = \alpha \iint_{\Omega} f + \beta \iint_{\Omega} g.
		\end{equation*}
		\item Monotonia:
		\begin{enumerate}
			\item se $f \ge g$, allora
			\begin{equation*}
				\iint_{\Omega} f \ge \iint_{\Omega} g.
			\end{equation*}
			\item $\abs{f} \in \mathcal{R}(\Omega)$ e si ha
			\begin{equation*}
				\abs{\iint_{\Omega} f} \le \iint_{\Omega} \abs{f}.
			\end{equation*}
		\end{enumerate}
		In particolare, se $\Omega$ è misurabile e $M_1 = \sup_{\Omega} \abs{f}$,
		\begin{equation}
			\abs{\iint_{\Omega} f} \le M_1 \abs{\Omega}.
		\end{equation}
		\item Teorema della media:
		\begin{enumerate}
			\item Se $\Omega$ è misurabile e $m =\sup_{\Omega} f$, si ha:
			\begin{equation*}
				m \le \frac{1}{\abs{\Omega}} \iint_{\Omega} f \le M.
			\end{equation*}
			\item Se $\Omega$ è misurabile, compatto e connesso e se $f \in C(\Omega)$, allora esiste $(x_0, y_0) \in \Omega$ tale che
			\begin{equation*}
				\frac{1}{\abs{\Omega}} \iint_{\Omega} f = f(x_0, y_0).
			\end{equation*}
		\end{enumerate}
	\end{enumerate}
\end{teor}

\subsection{Cambiamento delle variabili d'integrazione}
Consideriamo nel piano un aperto limitato $\Omega$ e una trasformazione biunivoca $\mathbf{T} \colon \Omega \leftrightarrow \mathbf{T}(\Omega) \subset \mathbb{R}^2$ assegnata dalle formule
\begin{equation}
	\label{eqn:trasfvar}
	\begin{dcases}
		x = \phi(u, v) \\
		y = \psi(u, v)
	\end{dcases}
\end{equation}
o, in forma compatta,
\begin{equation*}
	\mathbf{T}(u, v) = (\phi(u, v), \psi(u, v)).
\end{equation*}

Siano $A$ un aperto misurabile, tale che $\overline{A} \subset \mathbf{T}(\Omega)$ e $f\colon \overline{A} \to \mathbb{R}$ una funzione continua. Come si trasforma l'integrale
\begin{equation}
	\iint_A f(x, y) \,dxdy
\end{equation}
operando la trasformazione di variabili~\eqref{eqn:trasfvar}?

Se valgono le seguenti ipotesi su $\mathbf{T}$\footnote{Si osserva facilmente che in tal caso $\mathbf{T}^{-1}$ ha le stesse proprietà.}:
\begin{enumerate}
	\item $\mathbf{T} \in C^1(\Omega)$ (cioè $\phi$ e $\psi$ sono differenziabili con continuità in $\Omega$);
	\item lo jacobiano della trasformazione non si annulla in $\Omega$, cioè
	\begin{equation*}
		\det \mathbf{J} = \frac{\partial(\phi, \psi)}{\partial (u, v)} = \det{\begin{pmatrix} \phi_u & \phi_v \\ \psi_u & \psi_v  \end{pmatrix}} \ne 0
	\end{equation*}
	in $\Omega$,
\end{enumerate}
allora si può enunciare il seguente\footnote{Nel nostro caso studieremo solo (imponendo una condizione più forte) i diffeomorfismi; dati $X \subset \mathbb{R}^n$, $Y \subset \mathbb{R}^m$, $\mathbf{T} \colon X \to \mathbb{R}^m$, si dice che $\mathbf{T}$ è un diffeomorfismo da $X$ su $Y$ se:
\begin{enumerate}
	\item $\mathbf{T}$ è una biiezione da $X$ in $Y$;
	\item $\mathbf{T}$ è estendibile a un'applicazione $C^1$ in un aperto $A \supset X$;
	\item $\mathbf{T}^{-1}$ è estendibile a un'applicazione $C^1$ in un aperto $B \supset Y$.
\end{enumerate}
Ovviamente. $\mathbf{T}$ è un diffeomorfismo se e solo se lo è $\mathbf{T}^{-1}$.
}
\begin{teor}
	\label{cambiovariabili}
	Sia $\mathbf{T} \colon \Omega \to \mathbf{T}(\Omega)$, con $\Omega$ aperto di $\mathbb{R}^2$, un'applicazione biunivoca soddisfacente le proprietà sopra enunciate. Sia $S$ un insieme tale che $S \subset \Omega$. Allora:
	\begin{enumerate}
		\item $S$ è misurabile se e solo se $\mathbf{T}(S)$ è misurabile;
		\item se $S$ è misurabile e $f \in C(\mathbf{T}(S))$, vale la formula
		\begin{equation}
			\label{eqn:cambiovariabili}
			\iint_{\mathbf{T}(S)} f(x,y) \, dxdy = \iint_S \overline{f}(u, v) \abs{\frac{\partial(\phi, \psi)}{\partial(u, v)}} \, dudv.
		\end{equation}
	\end{enumerate}
\end{teor}

\begin{exmp}[Coordinate polari] Sia $\vec{T} = \vec{T](\rho, \theta)}$ definita dalle equazioni
	\begin{gather*}
		x(\rho, \theta) = \rho\cos\theta
		y(\rho, \theta) = \rho\sin\theta.
	\end{gather*}
	Se $\Omega = (0, +\infty) \times (0, 2\pi)$ allora $\vec{T}(\Omega)$ coincide con il piano privato del semiasse $y = 0, x \ge 0$. Poiché
	\begin{equation*}
		\frac{\partial (x, y)}{\partial (\rho, \theta)} = \rho,
	\end{equation*}
	$\vec{T}$ soddisfa le ipotesi del Teorema~\ref{cambiovariabili}. Se $A \subset \vec{T}(\Omega)$ è misurabile e limitato ed $f$ è continua e limitata su $A$ si ha dunque
	\begin{equation}
		\iint_A f(x, y) \, dx \, dy = \iint_{\vec{T}^{-1}(A)}f(\rho\cos\theta, \rho\sin\theta) \rho \, d\rho\,d\theta
	\end{equation}
\end{exmp}

\subsection{Integrali multipli}
La teoria degli integrali doppi si può estendere senza problemi a dimensioni $n \ge 3$; ci limitiamo a enunciare i principali risultati, dando per scontate le ovvie differenze dovute alla dimensione maggiore.

Gli insiemi semplici rispetto a un asse (ad esempio l'asse $z$) in $\mathbb{R}^3$ sono della forma
\begin{equation*}
	E = \{ (x, y, z) \in \mathbb{R}^3 \colon g_1(x, y) < z < g_2(x, y), (x, y) \in \Omega \}
\end{equation*}
dove $g_1, g_2$ sono continue in $\overline{\Omega}$ e $\Omega$ è misurabile.

Per quanto riguarda le formule di riduzione, nel caso $n$-dimensionale esistono molti modi di ordinare e raggruppare le variabili; a ciascuno di essi corrisponde una formula di riduzione.

Nel caso $n = 3$, al raggruppamento $\xi = x$ e $\vec{\eta} = (y, z)$ corrisponde la formula
\begin{equation}
	\iiint_Q f = \int_{a_1}^{b_1} dx \iint_{[a_2, b_2]\times[a_3, b_3]} f(x, y, z)\,dy\,dz
\end{equation}
(integrazione \emph{per strati}); al raggruppamento $\vec{\xi} = (x, y)$, $\eta = z$ corrisponde la formula
\begin{equation}
	\label{eqn:fili}
	\iiint_Q f = \int_{[a_1, b_1]\times[a_2, b_2]} dx \, dy \int_{a_3}^{b_3}f(x, y, z)\,dz
\end{equation}
(integrazione \emph{per fili}).

Nel caso di funzioni definite su domini $E$ semplici rispetto all'asse $z$ la formula~\eqref{eqn:fili} prende la forma
\begin{equation}
	\iiint_E f = \iint_{\Omega} dx \, dy \int_{g_1(x, y)}^{g_2(x, y)}f(x, y, z)\, dz.
\end{equation}
Si noti che $\Omega$ coincide con la proiezione di $E$ sul piano $x, y$.

\begin{exmp}[Volume dei solidi di rotazione]
	Sia $V$ un solido ottenuto ruotando attorno all'asse $z$ il trapezioide corrispondente alla funzione $y = \phi(z)$, $a \le z \le b$, $\phi$ continua; calcoliamone il volume.

	Fissato $\bar{z} \in [a, b]$, sia $a(\bar{z})$ l'area della sezione di $V$ con il piano $z = \bar{z}$.

	Integrando per strati si trova
	\begin{equation*}
		\Vol(V) = \int_a^b a(z)\,dz.
	\end{equation*}
	Essendo poi $a(z) = \pi(\phi(z))^2$ si ottiene
	\begin{equation*}
		\Vol(V) = \pi \int_a^b (\phi(z))^2\,dz.
	\end{equation*}
\end{exmp}

Passiamo ai cambi di coordinate: siano $\Omega$ un aperto di $\mathbb{R}^n$ e $\vec{T}\colon\Omega \to \vec{T}(\Omega) \subseteq \mathbb{R}^n$ una trasformazione biunivoca assegnata dalle formule
\begin{gather*}
	x_1 = \phi_1(u_1, \dots, u_n) \\
	x_2 = \phi_2(u_1, \dots, u_n) \\
	\vdots \\
	x_ n = \phi_n(u_n, \dots, u_n).
\end{gather*}

Se $\vec{T} \in C^1(\Omega)$ e
\begin{equation*}
	\frac{\partial(x_1, \dots, x_n)}{\partial (u_1, \dots, u_n)} \ne 0
\end{equation*}
in $\Omega$, allora il Teorema~\ref{cambiovariabili} si estende direttamente al caso multi-dimensionale. La formula~\eqref{eqn:cambiovariabili} diviene
\begin{equation}
	\label{eqn:cambiovariabilimult}
	\begin{split}
		&\int_{\vec{T}(S)}f(x_1, \dots, x_n)\,dx_1,\dots,dx_n
		= \\ = &\int_S \bar{f}(u_1,\dots, u_n)\abs{\frac{\partial (x_1, \dots, x_n)}{\partial(u_1, \dots, u_n)}}\,du_1, \dots, du_n
	\end{split}
\end{equation}
ove $S$ è un sottoinsieme misurabile in $\Omega$ e $f \in C(\vec{T}(S))$.
\begin{exmp}[Coordinate cilindriche in $\mathbb{R}^3$ rispetto all'asse $z$] $\vec{T}$ è definita dalle equazioni
	\begin{equation}
		\begin{dcases}
			x = \rho \cos \theta \\
			y = \rho \sin \theta \\
			z = z;
		\end{dcases}
	\end{equation}
	si lascia cioè $z$ invariato e si introducono le coordinate polari nel piano $x, y$.

	Se $(\rho, \theta, z) \in (0, +\infty)\times(0, 2 \pi) \times \mathbb{R}= \Omega$, $\vec{T}$ realizza una corrispondenza biunivoca tra $\Omega$ e lo spazio $\mathbb{R}^3$ con esclusione del semipiano $\{(x, y, z) \in \mathbb{R}^3 \colon y = 0, z \ge 0 \}$. Si ha che
	\begin{equation*}
		\frac{\partial (x, y, z)}{\partial (\rho, \theta, z)} = \rho,
	\end{equation*}
	perciò la~\eqref{eqn:cambiovariabilimult} diventa
	\begin{equation*}
		\iiint_{\vec{T}(S)}f(x, y, z)\,dx\,dy\,dz = \iiint_S f(\rho\cos\theta, \rho\sin\theta, z)\rho \,d\rho\,d\theta\,dz.
	\end{equation*}
\end{exmp}

\begin{exmp}[Coordinate sferiche o polari in $\mathbb{R}^3$] $\vec{T}$ è definita dalle equazioni
	\begin{equation}
		\begin{dcases}
			x = \rho\sin\psi\cos\theta \\
			y = \rho\sin\psi\sin\theta \\
			z = \rho\cos\psi.
		\end{dcases}
	\end{equation}
	Se $(\rho, \psi, \theta) \in \Omega = (0, +\infty) \times (0, \pi) \times (0, 2\pi)$, $\vec{T}$ realizza una corrispondenza biunivoca tra $\Omega$ e lo spazio $\mathbb{R}^3$ privato del semipiano $\{(x, y, z) \in \mathbb{R}^3 \colon y = 0, x \ge 0\}$.

	Si trova che
	\begin{equation*}
		\frac{\partial (x, y, z)}{\partial (\rho, \psi, \theta)} = \rho^2\sin\psi \ne 0
	\end{equation*}
	in $\Omega$, perciò la~\eqref{eqn:cambiovariabilimult} diventa
	\begin{equation*}
		\iiint_{\vec{T}(S)}f(x, y, z)\,dx\,dy\,dz = \iiint_S \bar{f}(\rho, \psi, \theta)\rho^2\sin\psi\,d\rho\,d\psi\,d\theta.
	\end{equation*}

\end{exmp}

\subsection{Alcune applicazioni}
Integrali doppi e tripli si possono usare per il calcolo di baricentri e momenti di inerzia per distirbuzioni continue di massa bidimensionali o tridimensionali.

Se $m_1, m_2, \dots, m_k$ sono masse dislocate rispettivamente in punti $\vec{p}_1, \vec{p}_2, \dots, \vec{p}_k$ il \emph{baricentro} della loro distribuzione è definito dal punto
\begin{equation*}
	\vec{b} \coloneqq \frac{1}{m}\sum_{j = 1}^k m_j\vec{p}_j,
\end{equation*}
ove
\begin{equation*}
	m = \sum_{j=1}^km_j
\end{equation*}
è la massa totale. Il \emph{momento d'inerzia} rispetto a una retta $r$ o a un polo $\vec{p}$ è dato dalla formula
\begin{equation*}
	I \coloneqq \sum_{j=1}^k d_k^2m_k
\end{equation*}
dove $d_k$ indica la distanza di $\vec{p}_k$ dalla retta o dal polo.

Supponiamo ora che un corpo occupi una regione piana $\Omega$. Se $\mu = \mu(x, y)$ indica la densità superficiale di massa, la massa totale $m$ del corpo è data dall'integrale
\begin{equation*}
	m = \iint_{\Omega} \mu(x, y)\,dx\,dy.
\end{equation*}

Le coordinate del baricentro sono assegnate dalle formule
\begin{equation}
	x_{\vec{b}} \coloneqq \frac{1}{m}\iint_{\Omega}x\mu(x, y)\,dx\,dy, \quad y_{\vec{b}} \coloneqq \frac{1}{m}\iint_{\Omega}y\mu(x, y)\,dx\,dy.
\end{equation}

Se la densità è costante, $\mu = k$ (corpo omogeneo), il baricentro si chiama anche \emph{centroide} ed essendo $m = k\cdot(\Omega)$ le formule precedneti diventano
\begin{equation}
	x_{\vec{b}} = \frac{1}{a(\Omega)} \iint_{\Omega}x\,dx\,dy, \quad y_{\vec{b}} = \frac{1}{a(\Omega)}\iint_{\Omega}y \, dx\,dy.
\end{equation}

Il momento d'inerzia rispetto a una retta $r$ o un polo $\vec{p}$ giacenti nel piano di $\Omega$ è assegnato dalla formula
\begin{equation}
	I \coloneqq \iint_{\Omega}(\delta(x,y))^2\mu(x,y)\,dx\,dy,
\end{equation}
dove $\delta = \delta(x, y)$ indica la distanza del punto $(x, y)$ dalla retta o dal polo. Per esempio, per i momenti d'inerzia rispetto agli assi coordinati si ha
\begin{equation*}
	I_x = \iint_{\Omega}y^2\mu(x, y)\,dx\,dy, \quad I_y = \iint_{\Omega}x^2 \mu(x, y)\,dx\,dy,
\end{equation*}
mentre per quello rispetto all'origine
\begin{equation*}
	I_{\vec{0}} = \iint_{\Omega}(x^2+y^2)\mu(x, y)\,dx\,dy = I_x + I_y.
\end{equation*}

Le formule precedenti si estendono a una distribuzione tridimensionale di massa con densità $\mu = \mu(x, y, z)$, $(x, y, z) \in V$.

Per il baricentro si avrà
\begin{gather*}
	x_{\vec{b}} \coloneqq \frac{1}{m} \iint_V x\mu(x, y, z)\,dx\,dy\,dz, \\
	y_{\vec{b}} \coloneqq \frac{1]{m}}\iint_V y\mu(x, y, z)\,dx\,dy\,dz, \\
	z_{\vec{b}} \coloneqq \frac{1]{m}}\iint_V z\mu(x, y, z)\,dx\,dy\,dz,
\end{gather*}
con
\begin{equation*}
	m = \iiint_V \mu\,dx\,dy\,dz
\end{equation*}
massa totale, mentre per il momento d'inerzia
\begin{equation*}
	I \coloneqq \iiint_V (\delta(x, y, z))^2\mu(x, y, z)\,dx\,dy\,dz.
\end{equation*}

\subsection{Cenni agli integrali multipli generalizzati}
Quando la funzione integranda o il dominio d'integrazione (o entrambi) non sono limitati si parla di integrale generalizzato o improprio.

Consideriamo il caso in cui la funzione integranda non è limitata ma il dominio $\Omega$ d'integrazione resta limitato; nella maggioranza dei casi concreti, le funzioni che si integrano sono funzioni continue, illimitate in un intorno di insiemi di misura nulla (per esempio nell'intorno di un numero finito di punti isolati).

L'idea è quella di togliere dalla regione $\Omega$ un insieme $S$ di misura piccola con i punti problematici, integrare su $\Omega \setminus S$ e calcolare il limite facendo tendere a zero la misura di $S$. Se il limite esiste, si chiamerà integrale improprio o generalizzato di $f$ in $\Omega$.

Ci limitiamo alla situazione più semplice in cui esiste non solo l'integrale improprio di $f$, ma anche quello di $\abs{f}$ (cioè la funzione è \emph{assolutamente integrabile} in senso generalizzato).

Siano $\Omega$ una regione misurabile e limitata di $\mathbb{R}^n$ ed $f \colon \Omega \to \mathbb{R}$. Supponiamo di poter trovare una successione $\{\Omega_j \}_{j \ge 1}$ di sottoinsiemi di $\Omega$ con le seguenti proprietà:
\begin{enumerate}
	\item $\Omega_j \subset \Omega_{j+1}$ per $j \ge 1$;
	\item ogni $\Omega_j$ è misurabile e $\abs{\Omega}_j \to \abs{\Omega}$ se $j \to +\infty$;
	\item $f \in \mathcal{R}(\Omega_j)$ (e quindi anche $\abs{f} \in \mathcal{R}(\Omega_j)$) e
	\begin{equation*}
		\lim_{j\to+\infty}\int_{\Omega_j}\abs{f}
	\end{equation*}
	esiste finito.
\end{enumerate}

Allora si può dimostrare che anche
\begin{equation}
	\label{eqn:intimp1}
	I = \lim_{j\to+\infty}\int_{\Omega_j}f
\end{equation}
esiste finito e che non dipende dalla particolare scelta della successione $\{ \Omega_j\}$ che approssima $\Omega$; $I$ si chiama integrale generalizzato o improprio di $f$ su $\Omega$.

Consideriamo ora l'integrale di una funzione su un dominio illimitato; il procedimento è sostanzialmente lo stesso considerato in precedenta.

Siamo $\Omega$ misurabile e illimitato in $\mathbb{R}^n$ ed $f \colon \Omega \to \mathbb{R}$. Suponiamo di poter trovare una successione $\{\Omega_j \}$ di sottoinsiemi limitati e misurabili di $\Omega$ per cui valgano le proprietà 1. e 3. precedenti e la seguente:
\begin{enumerate}[label=\arabic*'.]
	\setcounter{enumi}{1}
	\item per ogni $K$ compatto e contenuto in $\Omega$ esiste $j$ tale che $K \subset \Omega_j$.
\end{enumerate}

Valgono allora le stesse considerazioni fatte nel caso precedente e analogamente si può definire
\begin{equation}
	\int_{\Omega} f \coloneqq \lim_{j\to +\infty} \int_{\Omega_j} f.
\end{equation}

\begin{exmp}
	Tramite gli integrali doppi generalizzati si può calcolare
	\begin{equation*}
		I = \int_{-\infty}^{+\infty}e^{-x^2}\,dx.
	\end{equation*}

	Consideriamo infatti
	\begin{equation*}
		\iint_{\mathbb{R}^2}e^{-x^2-y^2}\,dx\,dy.
	\end{equation*}
	Scegliendo $\Omega_j = B_j =$ cerchio con centro in $(0,0)$ e raggio $j$, le ipotesi 1. e 2'. sono derificate; notiamo inoltre che $e^{-x^2-y^2} > 0$ in $\mathbb{R}^2$.

	Si ha, usando le coordinate polari,
	\begin{equation*}
		\iint_{B_j} e^{-x^2-y^2}\,dx\,dy = \int_0^{2\pi}d\theta \int_0^j\rho e^{-\rho^2}\,d\rho = \pi(1-e^{-j^2})\to \pi
	\end{equation*}
	se $j \to +\infty$. Pertanto
	\begin{equation}
		\iint_{\mathbb{R}^2} e^{-x^2-y^2}\,dx\,dy = \pi.
	\end{equation}

	D'altra parte, si deve ottenere lo stesso risultato scegliendo
	\begin{equation*}
		\Omega_j = Q_j = \{(x,y) \in \mathbb{R}^2 \colon \abs{x} < j, \abs{y} < k \}.
	\end{equation*}
	Si ha
	\begin{equation*}
		\iint_{Q_j}e^{-x^2-y^2}\,dx\,dy = \int_{-j}^{j}e^{-x^2}\,dx\int_{-j}^{j}e^{-y^2}\,dy \to \biggl(\int_{-\infty}^{+\infty}e^{-t^2}\,dt \biggr)^2 = I^2,
	\end{equation*}
	per $j \to +\infty$.

	Quindi $I^2 = \pi$ e perciò
	\begin{equation}
		\int_{-\infty}^{+\infty}e^{-x^2}\,dx = \sqrt{\pi}.
	\end{equation}

\end{exmp}





\chapter{Superfici e integrali di superficie}

\section{Superfici in $\mathbb{R}^3$}

\subsection{Definizioni principali, superfici regolari}

Sia $A$ un aperto connesso di $\mathbb{R}^2$ e sia $T$ un insieme tale
\begin{equation*}
	A \subseteq T \subseteq \overline{A}.
\end{equation*}
Sia $\vec{r}\colon T \to \mathbb{R}^3$ una funzione continua; indichiamo con $\Sigma$ l'immagine di $T$: $\Sigma = \vec{r}(T)$. La funzione $\vec{r}$ si chiama parametrizzazione di $\Sigma$.
\begin{defn}
	Si dice superficie in $\mathbb{R}^3$ una coppia $(\Sigma, \vec{r})$ dove $\Sigma$ è un insieme di $\mathbb{R}^3$ ed $\vec{r}$ una sua parametrizzazione.
\end{defn}

Esplicitamente una parametrizzazione è assegnata mediante l'equazione
\begin{equation*}
	\vec{r}(u, v) = (x(u, v), y(u, v), z(u,v)) \quad (u, v) \in T,
\end{equation*}
oppure, in forma vettoriale,
\begin{equation*}
	\vec{r}(u, v) = x(u,v)\vec{i}+y(u,v)\vec{j} + z(u,v)\vec{k}.
\end{equation*}

Si noti che nella definizione di superficie rientrano i grafici di funzioni $f \colon T \to \mathbb{R}$, $f \in C^1(T)$, detti \emph{superfici cartesiane}; se, per esempio, $\Sigma$ è il grafico di una funzione del tipo $z = f(x, y)$, una parametrizzazione è assegnata dall'equazione
\begin{equation}
	\label{eqn:supcart}
	\vec{r}(x, y) = x\vec{i} + y\vec{j}+ f(x, y)\vec{k}, \quad (x, y) \in T.
\end{equation}

\begin{defn}
	Sia $\Sigma$ una superficie di classe $C^1$ di equazione $\vec{r} = \vec{r}(u,v)$, $(u, v) \in T$. Un punto $\vec{p} = \vec{r}(u_0, v_0)$, dove $(u_0, v_0)$ è interno a $T$, si dice regolare se la matrice
	\begin{equation}
		\label{eqn:matrsup}
		\begin{pmatrix}
			x_u(u_0, v_0) & y_u(u_0, v_0) & z_u(u_0, v_0) \\
			x_v(u_0, v_0) & y_v(u_0, v_0) & z_v(u_0, v_0)
		\end{pmatrix}
	\end{equation}
	ha rango $2$; in caso contrario il punto si dirà singolare. $\Sigma$ si dirà regolare se, per ogni $(u, v)$ interno a $T$, $\vec{r}(u, v)$ è un punto regolare.
\end{defn}

Una superficie cartesiana è regolare se la funzione $f$ che la definisce è di classe $C^1$; se l'equazione della superficie è la~\eqref{eqn:supcart} la matrice~\eqref{eqn:matrsup} prende la forma
\begin{equation*}
	\begin{pmatrix}
		1 & 0 & f_x \\
		0 & 1 & f_y
	\end{pmatrix}
\end{equation*}
che ha sempre rango 2.

Passiamo al significato geometrico della regolarità: se $\vec{p}$ è un punto regolare, allora in un intorno di $\vec{p}$ la superficie ammette una rappresentazione cartesiana. Sia infatti $\Sigma$ una superficie di equazione $\vec{r} = \vec{r}(u, v)$ e sia $\vec{p} = \vec{r}(u_0, v_0)$ un suo punto di regolarità; allora almeno uno dei tre determinanti seguenti è diverso da $0$ in $(u_0, v_0)$:
\begin{align*}
	\frac{\partial (y,z)}{\partial (u,v)} &= y_uz_v - y_vz_u \\
	\frac{\partial (z, x)}{\partial (u,v)} &= z_ux_v  z_vx_u \\
	\frac{\partial (x, y)}{\partial (u, v)} &= x_uy_v - x_vy_u.
\end{align*}
Per fissare le idee, supponiamo che il terzo sia non nullo. Per il teorema d'inversione locale, dalle equazioni
\begin{equation*}
	\begin{dcases}
		x = x(u, v) \\
		y = y(u,v)
	\end{dcases}
\end{equation*}
si possono ricavare, in un intorno di $\vec{p}$, $u = u(x, y)$ e $v = v(x, y)$, che sostituite in $z = z(u, v)$ forniscono l'equazione cartesiana di $\Sigma$:
\begin{equation*}
	z = z(u(x, y), v(x, y)) \coloneqq f(x, y).
\end{equation*}

Oltre a superfici definite mediante equazioni parametriche e cartesiane, è utile considerare le superfici definite come insiemi di livello di funzioni di tre variabili. Precisamente, sia $F \colon A \to \mathbb{R}$, $A$ aperto di $\mathbb{R}^3$, $F \in C^1(A)$, e consideriamo l'insieme di livello $E_0$ di equazione
\begin{equation}
	F(x, y, z) = 0.
\end{equation}

Diremo che $E_0$ definisce una superficie $\Sigma$ se $\nabla F(x, y, z) \ne \vec{0}$ in ogni punto di $E_0$, tranne al più in un numero finito di punti.

\subsection{Bordo di una superficie, superfici regolari a pezzi}
Se $\Sigma$ è una superficie di equazione vettoriale $\vec{r} = \vec{r}(u, v)$ e $\vec{p}_1, \vec{p}_2 \in T$, $\vec{p}_1 \ne \vec{p}_2$ con almeno uno dei due interno a $T$ implica $\vec{r}(\vec{p}_1) \ne \vec{r}(\vec{p}_2)$, la superficie si dice semplice.

Introduciamo le nozioni di bordo di una superficie e di superficie chiusa; limitiamoci a superfici $\Sigma$ di equazione $\vec{r} = \vec{r}(u, v) \in C^1(T)$, ove $T$ è aperto. Si dice bordo di $\Sigma$ l'insieme
\begin{equation*}
	\partial \Sigma \coloneqq \overline{\Sigma}\setminus \Sigma.
\end{equation*}

Le superfici senza bordo (cioè con $\partial \Sigma = \emptyset$) e limitate in $\mathbb{R}^3$ si dicono chiuse.

\begin{defn}
	Diremo che $\Sigma \in \mathbb{R}^3$ rappresenta una superficie $C^1$ a pezzi se esistono un numero finito di curve regolari a tratti (dette spigoli) $\gamma_1, \dots, \gamma_N$, contenute in $\Sigma$, che suddividono $\Sigma$ in un numero finito $N_0$ di superfici di classe $C^1$ (dette facce).
\end{defn}

Più precisamente dev'essere
\begin{equation*}
	\Sigma \setminus \bigcup_{j=1}^N \gamma_j = \bigcup_{i = 1}^N \Sigma_i
\end{equation*}
dove ogni $\Sigma_i$ ammette una parametrizzazione $\vec{r}\colon T_i \to \mathbb{R}^3$, $\vec{r} \in C^1(T_i)$, $T_i$ aperto in $\mathbb{R}^2$ e dove ogni $\gamma_j$ non può essere parte del bordo di più di due facce.

Il bordo di $\Sigma$ in questo caso è l'unione dei bordi delle $\Sigma_i$ con esclusione degli spigoli che appartengono al bordo di due facce adiacenti.

\subsection{Linee coordinate, coordinate locali, cambiamento di parametri}
Per una superficie regolare di equazione $\vec{r} = \vec{r}(u, v)$ introduciamo le curve di equazione
\begin{equation}
	\label{eqn:lineecoordinate}
	u \mapsto \vec{r}(u, \overline{v}), \quad v \mapsto \vec{r}(\overline{u}, v)
\end{equation}

che si ottengono considerando rispettivamente $v = \overline{v}$ costante e $u = \overline{u}$ costante.

Le~\eqref{eqn:lineecoordinate} si chiamano linee coordinate sulla superficie; $u$ e $v$ si chiamano coordinate locali dei punti sulla superficie.

I vettori tangenti alle linee coordinate sono assegnati dalle formule
\begin{gather*}
	\vec{r}_u(u, v) = x_u(u, v) \vec{i}+ y_u(u, v)\vec{j} + z_u(u, v)\vec{k} \\
	\vec{r}_v(u, v) = x_v(u,v)\vec{i}+y_v(u,v)\vec{j}+z_v(u,v)\vec{k}.
\end{gather*}

Eseguendo il prodotto vettoriale tra $\vec{r}_u$ e $\vec{r}_v$ si ottiene
\begin{equation*}
	\vec{r}_u \times \vec{r}_v = \det \begin{pmatrix}
	\vec{i} & \vec{j} & \vec{k} \\
	x_u & y_u & z_u \\
	x_v & y_v & z_v
	\end{pmatrix} = \frac{\partial (y, z)}{\partial (u, v)}\vec{i} + \frac{\partial(z, x)}{\partial (u,v)}\vec{j} + \frac{\partial (x, y)}{\partial (u, v)}\vec{k};
\end{equation*}
quindi $\vec{r}(u, v)$ è regolare se e solo se $\vec{r}_u \times \vec{r}_v \ne \vec{0}$, ovvero se $\vec{r}_u$ e $\vec{r}_v$ sono linearmente indipendenti.

Sia $\Sigma$ una superficie regolare di equazione $\vec{r}(u, v) = x(u, v)\vec{i} + y(u,v)\vec{j} + z(u,v)\vec{k}$, $(u ,v) \in T \subseteq \mathbb{R}^2$. Introduciamo il cambio di parametri definito dalle equazioni
\begin{equation}
	\label{eqn:parsup}
	u = \phi(s, t), \quad v = \psi(s, t),
\end{equation}
dove $(s, t) \in S \subseteq \mathbb{R}^2$ e $\phi, \psi \in C(S)$. Supponiamo che $\phi, \psi \in C^1(S)$, che
\begin{equation*}
	\frac{\partial (\phi, \psi)}{\partial (s, t)} \ne 0 \quad \forall (s, t) \in \overset{\circ}{S}
\end{equation*}
e che le~\eqref{eqn:parsup} realizzino una corrispondenza biunivoca tra $S$ e $T$; se valgono tali ipotesi, il cambiamento di variabile si dirà regolare. La superficie è descritta dall'equazione
\begin{equation*}
	\bar{\vec{r}}(s, t) = \bar{x}(s, t)\vec{i}+\bar{y}(s, t)\vec{j}+\bar{z}(s, t)\vec{k},
\end{equation*}
dove
\begin{gather*}
	\bar{x}(s, t) = x(\phi(s, t), \psi(s, t)) \\
	\bar{y}(s, t) = y(\phi(s, t), \psi(s, t)) \\
	\bar{z}(s, t) = z(\phi(s, t), \psi(s, t)).
\end{gather*}

Si prova facilmente che la regolarità dei punti non cambia operando una trasformazione regolare di parametri e che le coppie di vettori $\vec{r}_u, \vec{r}_v$ e $\bar{\vec{r}}_s$, $\bar{\vec{r}}_t$ individuano lo stesso piano, detto piano tangente alla superficie.

Le due parametrizzazioni si dicono equivalenti se sono legate da un cambiamento regolare di parametri con determinante jacobiano positivo; in tal caso, i vettori $\bar{\vec{r}}_s \times \bar{\vec{r}}_t$ e $\vec{r}_u \times \vec{r}_v$ hanno stessa direzione e stesso verso.

\begin{exmp}
	Facendo ruotare una curva piana $\gamma$ intorno a una retta giacente nel piano della curva si ottiene una \emph{superficie di rivoluzione}; $\gamma$ si chiama \emph{generatrice}. Se il piano ha equazione $x= 0$, la retta è l'asse $z$ e la curva ha equazioni parametriche
	\begin{equation*}
		y = f(t), \quad z = g(t), \quad t \in I \subseteq \mathbb{R},
	\end{equation*}
	la corrispondente superficie di rivoluzione ha equazione vettoriale
	\begin{equation}
		\vec{r}(\theta, t) = f(t)\cos\theta \vec{i}+f(t)\sin\theta\vec{j} + g(t) \vec{k},
	\end{equation}
	dove $(\theta, t) \in [0, 2\pi] \times I$.
	\end{exmp} Se la curva è di classe $C^k(I)$, la superficie sarà della stessa classe; se la curva è chiusa, anche la superficie risulterà chiusa.

	Le linee coordinate per $\theta$ costante corrispondono alle diverse posizioni assunte dalla generatrice (linee meridiane) mentre quelle corrispondenti a $t$ costante sono cerchi (paralleli).

	\subsection{Vettore normale, piano tangente, orientazione}

	Supporremo d'ora in poi $T$ aperto.

	Sia $\vec{r} = \vec{r}(u ,v)$, $(u, v) \in T \subseteq \mathbb{R}^2$ l'equazione di una superficie $\Sigma$ regolare. I due vettori $\vec{r}_u$ e $\vec{r}_v$ sono pertanto linearmente indipendenti per ogni $(u, v) \in T$.

	Sia ora $\gamma$ una curva regolare contenuta in $T$ di equazioni parametriche $u = u(t)$, $v = v(t)$, $t \in I \subseteq \mathbb{R}$. La curva di equazione
	\begin{equation}
		\label{eqn:curva}
		\bar{\vec{r}}(t) = \vec{r}(u(t), v(t))
	\end{equation}
	è una curva regolare giacente sulla superficie con vettore tangente
	\begin{equation}
		\label{eqn:vettang}
		\bar{\vec{r}}'(t) = \vec{r}(u(t), v(t))u'(t) + \vec{r}_v(u(t), v(t))v'(t).
	\end{equation}
	La~\eqref{eqn:vettang} indica che $\bar{\vec{r}}'$ è contenuto nel piano dei vettori $\vec{r}_u$ e $\vec{r}_v$.

	D'altra parte, fissato $\vec{p}_0 = \vec{r}(u_0, v_0)$. ogni curva regolare passante per $\vec{p}_0$ e giacente su $\Sigma$ si può rappresentare localmente nella forma~\eqref{eqn:curva} e quindi il suo vettore tangente è contenuto nel piano dei vettori $\vec{r}_u$ e $\vec{r}_v$. Il piano parallelo a questo e passante per $\vec{p}_0$ contiene dunque tutte le rette tangenti a ogni curva regolare passante per $\vec{p}_0$ e giacente sulla superficie; perciò è chiamato \emph{piano tangente} a $\Sigma$ nel punto $\vec{p_0} = \vec{r}(u_0, v_0)$; tale piano è invariante per cambi di parametrizzazione regolari. La sua equazione è la seguente:
	\begin{equation}
		(\vec{\xi} - \vec{r}(u_0, v_0), \vec{r}_u(u_0, v_0) \times \vec{r}_v(u_0, v_0)) = 0,
	\end{equation}
	dove $\vec{\xi} = x \vec{i} + y \vec{j} + z \vec{k}$ è il punto corrente sul piano.

	Il vettore $\vec{r}_u \times \vec{r}_v$ è un vettore normale alla superficie; il versore corrispondente è
	\begin{equation}
		\vec{n} = \frac{\vec{r}_u \times \vec{r}_v}{\norma{\vec{r}_u \times \vec{r}_v}}.
	\end{equation}

	\begin{exmp}
		Sia $z = f(x,y)$ l'equzione di una superficie, con $f \in C^1(T)$, $T$ aperto connesso di $\mathbb{R}^2$. Si trova facilmente che
		\begin{equation*}
			\vec{n} = \frac{1}{\sqrt{1 + \norma{\nabla f}^2}}(-f_x \vec{i} - f_y\vec{j} + \vec{k}),
		\end{equation*}
		mentre il piano tangente alla superficie nel punto $(x_0, y_0, f(x_0, y_0))$ ha equazione
		\begin{equation*}
			(x - x_0)f_x(x_0, y_0) + (y-y_0)f_y(x_0, y_0) - (z-z_0) = 0,
		\end{equation*}
		ove $z_0 = f(x_0, y_0)$.
	\end{exmp}

	Fra $\vec{n}$ e $-\vec{n}$ convenzionalmente chiameremo versore normale $\vec{n}$; tale scelta è legata al concetto di \emph{orientazione} di una superficie. Sia $\Sigma$ una superficie regolare; supponiamo che sia possibile scegliere il versore normale in modo che, partendo da un punto $\vec{p}_0 \in \Sigma$ e seguendo una qualunque curva regolare e chiusa (che dunque ritorni in $\vec{p}_0$) sulla superficie, il versore normale vari con continuità e ritorni nella posizione iniziale; in tal caso diremo che la superficie è \emph{orientabile} e che la scelta del versore normale determina l'orientazione.

	Una superficie regolare semplice con dominio base $T$ aperto è orientabile, come si verifica facilmente, essendo $\vec{n} = \vec{n}(u, v)$ un vettore continuo su $T$.

	Effettuando un cambio di parametri regolare, il piano tangente rimane invariato; il versore normale cambia verso se lo jacobiano della trasformazione è negativo, rimane invariato se è positivo. Ciò significa che se una superficie $\Sigma$ è orientabile, l'orientazione rimane inalterata rispetto a parametrizzazioni equivalenti.

	\subsection{Area di una superficie, integrali superficiali}

	Se $\vec{\xi}$ e $\vec{\eta}$ sono due vettori di $\mathbb{R}^3$ linearmente indipendenti, allora $\norma{\vec{\xi} \times \vec{\eta}}$ rappresenta l'area del parallelogramma $\bar{R}$ che essi individuano; se si vuole ottenere l'area della sua proiezione $R$ sul piano $xy$, basta moltiplicare $\norma{\vec{\xi} \times \vec{\eta}}$ per $\cos \alpha$, dove $\alpha$, $-\pi/2 < \alpha < \pi/2$, è l'angolo tra i vettori $\vec{\xi} \times \vec{\eta}$ e $\vec{k}$. Vale cioè la formula
	\begin{equation}
		a(R)=a(\bar{R})\cos\alpha.
	\end{equation}
	Applichiamo queste considerazioni a una superficie semplice e regolare $\Sigma$ di equazione $\vec{r} = \vec{r}(u, v)$, $(u, v) \in T \subset \mathbb{R}^2$, $T$ aperto limitato.

	Consideriamo una porzione (\emph{parallelogramma curvilineo}) di $\Sigma$ individuata da due coppie di linee coordinate, definite da $u = \bar{u}$, $u = \bar{u} + du$ e da $v = \bar{v}$, $v = \bar{v} + dv$, con $du >0$, $dv > 0$.  I due vettori $\vec{r}_udu = \vec{r}_u(u, \bar{v})$ e $\vec{r}_vdv = \vec{r}_v(\bar{u}, v) dv$ individuano un parallelogramma la cui area $\norma{\vec{r}_u \times \vec{r}_v}du dv$ intuitivamente coincide (a meno di infinitesimi di ordine superiore a $du^2 + dv^2$) con quella del parallelogramma curvilineo.
	Tale area è intuitivamente pari a $\vec{\xi} \times \vec{\eta}$, dove
	\begin{gather*}
		\vec{\xi} = \vec{r}( u + du, v) - \vec{r}(u, v) = \vec{r}_udu + o(du) \\
		\vec{\eta} = \vec{r}(u, v+dv) - \vec{r}(u, v) = \vec{r}_vdv + o(dv).
	\end{gather*}

	È quindi ragionevole chiamare \emph{elemento d'area} l'espressione simbolica
	\begin{equation}
		d\sigma \coloneqq \norma{\vec{r}_u \times \vec{r}_v} du\,dv
	\end{equation}
	e dare la seguente

	\begin{defn}
		L'area di $\Sigma$ è assegnata dalla formula
		\begin{equation}
			a(\Sigma) = \iint_{\Sigma} d\sigma \coloneqq \iint_{T} \norma{\vec{r}_u \times \vec{r}_v} \, du \,dv.
		\end{equation}
	\end{defn}

	Per una superficie data in forma implicita $F(x, y, z) = 0$, con $F_z \ne 0$, si ha localmente $z = f(x, y)$ e pertanto l'elemento d'area in termini di $F$ è dato dalla formula
	\begin{equation}
		\label{eqn:areasupimp}
		d\sigma = \sqrt{1 + \biggl(\frac{F_x}{F_z} \biggr)^2 + \biggl( \frac{F_y}{F_z} \biggr)^2} dx \, dy = \frac{\norma{\nabla F}}{\norma{F_z}} dx \,dy.
	\end{equation}

	Passiamo alla definizione di \emph{integrale di superficie}.

	Se $h = h(x, y, z)$ è una funzione reale definita su $\Sigma$, si definisce l'integrale di $h$ su $\Sigma$ mediante la formula
	\begin{equation}
		\label{eqn:intsup}
		\iint_{\Sigma} h \, d\sigma \coloneqq \iint_T h(\vec{r}(u, v)) \norma{\vec{r}_u \times \vec{r}_v} \, du\,dv,
	\end{equation}
	quando l'integrale a destra della~\eqref{eqn:intsup} è ben definito. È facile mostrare che l'integrale~\eqref{eqn:intsup} (e quindi l'area di $\Sigma$) non dipendono dall'orientazione; le nozioni di area e integrale di superfici si estendono senza difficoltà al caso di superfici regolari a tratti.

	\subsection{Alcune applicazioni fisiche e geometriche}

	\subsubsection*{Calcolo di baricentri e momenti d'inerzia}

	Sia $\Sigma$ una superficie regolare e semplice e $\delta = \delta(x, y, z)$ definita su $\Sigma$. Se $\delta$ s'interpreta come \emph{densità superficiale} di una massa distribuita su $\Sigma$, l'integrale
	\begin{equation*}
		m = \int_{\Sigma} \delta \, d\sigma
	\end{equation*}
	rappresenta la massa totale.

	In questo caso, gli integrali
	\begin{equation*}
		x_b = \frac{1}{m}\iint_{\Sigma}x\delta \,d\sigma, \quad y_b = \frac{1}{m}\iint_{\Sigma}y\delta\, d\sigma, \quad z_b = \frac{1}{m}\iint_{\Sigma}z\delta\, d\sigma
	\end{equation*}
	rappresentano le coordinate del baricentro della distribuzione di massa.

	\subsubsection*{Flussi}

	Consideriamo un campo vettoriale $\vec{F} = F_1\vec{i} + F_2\vec{j}+F_3\vec{k}$ definito in una regione $U$ dello spazio contenente una superficie regolare e orientabile $\Sigma$. Se $\vec{n}$ è il versore normale alla superficie, l'integrale del campo vettoriale $\vec{F}$ su $\Sigma$ si chiama \emph{flusso} di $\vec{F}$ attraverso $\Sigma$ nella direzione $\vec{n}$ ed è dato dalla formula
	\begin{equation}
		\iint_{\Sigma}(\vec{F}, \vec{n}) \, d\sigma = \iint_{\Sigma} (F_1(\vec{n}, \vec{i}), F_2(\vec{n}, \vec{j}), F_3(\vec{n},\vec{k}))\,d\sigma.
	\end{equation}

	Si noti che il flusso è invariante per cambi di parametrizzazione equivalenti, ma cambia segno se cambia il verso di $\vec{n}$, ovvero l'orientazione di $\Sigma$.

	\subsubsection{Formula di coarea}

	Consideriamo una famiglia di superfici regolari $\Sigma_t$ di equazione
	\begin{equation}
		\phi (x, y, z) = t
	\end{equation}
	dove $t \in [t_1, t_2]$ ha il ruolo di parametro. Essendo $\Sigma_t$ regolare per ogni $t$ avremo $\nabla \phi \ne \vec{0}$ in ogni punto; ricordiamo che il vettore
	\begin{equation*}
		\vec{\ni} = \frac{\nabla \phi}{\norma{\nabla \phi}}
	\end{equation*}
	è ortogonale alla superficie ed è diretto nel verso delle $t$ crescenti. Se interpretiamo $t$ come tempo, possiamo pensare a una superficie che si muove nello spazio.

	Al variare di $t$ da $t_1$ a $t_2$, $\Sigma_t$ descrive una regione $D \in \mathbb{R}^3$,
	\begin{equation*}
		D = \{(x, y, z) \in \mathbb{R}^3 \colon t_1 \le \phi(x, y, z) \le t_2 \}.
	\end{equation*}
	Supponiamo che ogni punto di $D$ appartenga a una e una sola delle superfici $\Sigma_t$: come si calcola il volume di $D$ in termini delle aree delle superfici $\Sigma_t$? Si ha evidentemente
	\begin{equation*}
		\Vol(D) = \iiint_D dx \,dy\, dz.
	\end{equation*}

	Eseguiamo il cambiamento di variabili
	\begin{equation*}
		\xi = x, \quad \eta = y, \quad t = \phi(x, y, z);
	\end{equation*}
	lo jacobiano della trasformazione (si verifica facilmente) è $\phi_z$, pertanto
	\begin{equation*}
		\iiint_D dx \, dy \, dz = \int_{t_1}^{t_2} dt \iint_{\Sigma_t} \frac{1}{\norma{\phi_z}} \, d\xi \, d\eta.
	\end{equation*}

	Dalla~\eqref{eqn:areasupimp} discende la formula seguente, detta \emph{formula di coarea}:
	\begin{equation}
		\label{eqn:coarea}
		\Vol(D) = \int_{t_1}^{t_2} dt \iint_{\Sigma_t} \frac{1}{\norma{\nabla \phi}} \, d\sigma.
	\end{equation}

	La~\eqref{eqn:coarea} diventa più interessante dal punto di vista cinematico facendo intervenire la velocità con cui si spostano le $\Sigma_t$; pensiamo $\Sigma_t$ composta da particelle puntiformi la cui traiettoria nello spazio è descritta dal vettore posizione $(x(t), y(t), z(t))$. Poiché, per ogni $t$, $(x(t), y(t), z(t)) \in \Sigma_t$, avremo
	\begin{equation*}
		\label{eqn:coareapos}
		\phi(x(t), y(t), z(t)) = t, \quad t \in [t_1, t_2].
	\end{equation*}
	Derivando, otteniamo
	\begin{equation*}
		x'\phi_x + y'\phi_y + z'\phi_z = 1.
	\end{equation*}
	Ponendo $\vec{v}(t) = (x'(t), y'(t), z'(t))$ e dividendo per $\norma{\nabla \phi}$, si deduce l'equazione
	\begin{equation}
		(\vec{v}, \vec{\ni}) = \frac{1}{\norma{\nabla \phi}},
	\end{equation}
	che indica che l'integranda in~\eqref{eqn:coareapos}, calcolata in $\vec{p} \in \Sigma_t$, ha il significato di \emph{componente normale} della velocità con la quale $\vec{p}$ si muove nello spazio. Indicando con $c_{\vec{\ni}}$ tale componente si ha dunque
	\begin{equation}
		\Vol(D) = \int_{t_1}^{t_2} dt \iint_{\Sigma_t} c_{\vec{\ni}} \, d\sigma.
	\end{equation}

	\section{I teoremi di Green, Gauss e Stokes}
	\subsection{La formula di Gauss-Green nel piano}

	In questo paragrafo studieremo la relazione tra integrali doppi e integrali curvilinei di forme differenziali.

	Ricordiamo che, se $D$ è un dominio limitato in $\mathbb{R}^2$, la cui frontiera $\partial D$ sia una curva di Jordan regolare a tratti, si dice che $\partial D$ è orientata positivamente se è orientata in senso antiorario; lo indicheremo con il simbolo $\partial^+ D$. In tal caso, si lasciano i punti di $D$ a sinistra.

	Cominciamo considerando domini $D$ semplici rispetto agli assi e un campo vettoriale $\vec{F}(x, y) = P(x, y) \vec{i} + Q(x, y)\vec{j}$ definito sulla chiusura di $D$, $\overline{D} = D \cup \partial D$. Vale il seguente
	\begin{lem}
		Sia $\vec{F} \in C^1(D)$.
		\begin{enumerate}
			\item Se $D = \{ (x, y) \in \mathbb{R}^2 \colon a < x < b, \phi_1(x) < y < \phi_2(x) \}$, con $\phi_1, \phi_2$ regolari a tratti, allora
			\begin{equation}
				\iint_D P_y \, dx \, dy = - \int_{\partial^+ D} P \, dx.
			\end{equation}

			\item Se $D = \{ (x, y) \in \mathbb{R}^2 \colon c < y < d, \psi_1(y) < x < \psi_2(y) \}$, con $\psi_1, \psi_2$ regolari a tratti, allora
			\begin{equation}
				\iint_D Q_x \, dx \, dy = \int_{\partial^+ D} Q \, dy.
			\end{equation}
		\end{enumerate}
	\end{lem}

	\begin{teor}[di Gauss-Green I]
		\label{gaussgreen1}
		Sia $D$ un dominio limitato in $\mathbb{R}^2$ la cui frontiera sia una curva di Jordan regolare a tratti e che sia semplice rispetto a entrambi gli assi. Se $\vec{F} = P\vec{i} + Q\vec{j} \in C^1(\overline{D})$, allora vale la formula
		\begin{equation}
			\label{eqn:gaussgreen1}
			\iint_D (Q_x - P_y) \, dx \, dy  = \int_{\partial^+ D} P \, dx + \int_{\partial^+ D} Q \, dy.
		\end{equation}
	\end{teor}

	La~\eqref{eqn:gaussgreen1} vale per domini molto più generali di quelli considerati finora; chiameremo \emph{ammissibili} i domini per cui è valido il teorema di Gauss-Green.

	In questa classe rientrano domini $D$ la cui frontiera è unione disgiunta di un numero finito di curve di Jordan regolari a tratti e che siano decomponibili in un numero finito di sottodomini $D_1, D_2, \dots, D_k$ semplici rispetto a entrambi gli assi, cioè
	\begin{equation*}
		D = \bigcup_{j=1}^k D_j, \quad \overset{\circ}{D_i} \cap \overset{\circ}{D_j} = \emptyset, \quad i \ne j.
	\end{equation*}

	Chiameremo questi domini s-decomponibili; un dominio s-decomponibile può non essere semplicemente connesso.

	L'orientazione positiva di $\partial D$ si ottiene orientando le singole curve che la compongono in modo tale che percorrendole si lasci il dominio alla propria sinistra.

	\begin{prop}
		Se $D$ è s-decomponibile e $\vec{F} \in C^1(\overline{D})$, allora vale la~\eqref{eqn:gaussgreen1}.
	\end{prop}

	\subsection{Applicazioni}

	\subsubsection*{Calcolo di aree mediante integrali curvilinei}
	Le formule precedenti si possono usare per calcolare l'area di un dominio $D$, ammissibile per il Teorema~\ref{gaussgreen1}, mediante un integrale curvilineo esteso a $\partial D$.

	Si ha infatti
	\begin{equation}
		a(D) = \frac{1}{2} \biggl( \int_{\partial^+D}x dy - \int_{\partial^+D}y dx \biggr).
	\end{equation}

	\subsubsection*{Significati fisici}
	Introducendo il vettore $\vec{k} = (0, 0, 1)$ e ricordando che per il vettore piano $\vec{F}$ si ha $\rot \vec{F} = (Q_x - P_y) \vec{k}$, si ha al primo membro della~\eqref{eqn:gaussgreen1}, ponendo $\rot_z \vec{F} = (\rot \vec{F}, \vec{k})$,
	\begin{equation}
		\iint_D (Q_x - P_y)\,dx \,dy = \iint_D \rot_z \vec{F} \,dx \, dy.
	\end{equation}
	L'integrale curvilineo a destra nella~\eqref{eqn:gaussgreen1} si può scrivere come
	\begin{equation*}
		\int_{\partial^+D} (\vec{F}, \vec{T}) \, ds
	\end{equation*}
	dove $\vec{T}$ è il versore tangente di $\partial^+ D$ e $ds$ è il differenziale della lunghezza d'arco. Si arriva così alla seguente versione del teorema di Gauss-Green:
	\begin{teor}[di Gauss-Green II]
		Se $D$ è un dominio ammissibile per il Teorema~\ref{gaussgreen1} e $\vec{F} \in C^1(\overline{D})$, allora
		\begin{equation}
			\label{eqn:stokespiano}
			\iint_D \rot_z \vec{F} \, dx \, dy = \int_{\partial^+D} (\vec{F}, \vec{T}) \, ds.
		\end{equation}
	\end{teor}

	La~\eqref{eqn:stokespiano} si chiama \emph{formula di Stokes} nel piano; il suo significato fisico è che il flusso del vettore $\rot\vec{F}$ attraverso $D$ nella direzione e verso di $\vec{k}$ uguaglia la circuitazione di $\vec{F}$ lungo $\partial^+D$.

	Si può ottenere una terza versione del Teorema~\ref{gaussgreen1} ponendo nella~\eqref{eqn:gaussgreen1} $P$ al posto di $Q$ e $-Q$ al posto di $P$; la formula diventa
	\begin{equation}
		\iint_D (P_x - Q_y) \, dx \, dy = \int_{\partial^+ D} P \, dy - \int_{\partial^+ D} Q \, dx.
	\end{equation}

	L'integranda al primo membro è $\divg \vec{F}$, mentre introducendo il versore normale esterno a $\partial^+ D$,
	\begin{equation*}
		\vec{n}_e = \frac{dy}{ds}\vec{i} - \frac{dx}{ds}\vec{j},
	\end{equation*}
	si può scrivere
	\begin{equation*}
		\int_{\partial ^+ D}P \, dy - \int_{\partial^+D}Q \, dx = \int_{\partial^+D}\biggl( P\frac{dy}{ds} - Q\frac{dx}{ds} \biggr) \, ds = \int_{\partial D}(\vec{F}, \vec{n}_e)\, ds.
	\end{equation*}

	Si arriva quindi al seguente
	\begin{teor}[di Gauss-Green III]
		Se $D$ è un dominio ammissibile per il Teorema~\ref{gaussgreen1}, allora
		\begin{equation}
			\label{eqn:div1}
			\iint_D \divg \vec{F} \, dx \, dy = \int_{\partial D} (\vec{F}, \vec{n}_e)\, ds.
		\end{equation}
	\end{teor}

	La~\eqref{eqn:div1} si chiama \emph{formula della divergenza} nel piano. Il suo significato fisico è che il flusso di $\vec{F}$ uscente da $\partial D$ uguaglia l'integrale della divergenza in $D$.

	\subsection{Il teorema di Stokes nello spazio}
	Esaminiamo ora la relazione tra integrali su una superficie orientabile e integrali curvilinei estesi al bordo della superficie.

	Sia $\Sigma$ una superficie semplice, regolare, orientabile e con bordo $\partial \Sigma$ costituito da una curva chiusa regolare a tratti; scegliendo un verso per la normale $\vec{n}$ si determinano su $\Sigma$ due lati che chiameremo convenzionalmente \emph{positivo} (quello verso il quale punta $\vec{n}$) e \emph{negativo}.

	Diremo che $\partial \Sigma$ è orientata positivamente rispetto a $\Sigma$ se percorrendo $\partial \Sigma$ mantenendosi sul lato positivo di $\Sigma$ si lasciano i punti di $\Sigma$ sulla sinistra; scriveremo in tal caso $\partial^+ \Sigma$.

	Frequentemente $\Sigma$ ha una parametrizzazione
	\begin{equation*}
		\vec{r}\colon\!\overline{T} \to \mathbb{R}^3,
	\end{equation*}
	dove $T$ è l'interno di una curva di Jordan regolare a tratti in $\mathbb{R}^2$, biunivoca fra $\overline{T}$ e $\Sigma$.

	In tal caso $\vec{r}(\partial T) = \partial \Sigma$; se inoltre $(u(t), v(t))$, $t \in [a, b]$ è una parametrizzazione di $\partial^+ T$, allora $\vec{r}(u(t), v(t))$, con la scelta del versore normale
	\begin{equation*}
		\vec{n} = \frac{\vec{r}_u \times \vec{r}_v}{\norma{\vec{r}_u \times \vec{r}_v}},
	\end{equation*}
	è automaticamente un'orientazione positiva di $\partial \Sigma$.

	Generalizzando, introduciamo la seguente classe di superfici:
	\begin{defn}
		Indichiamo con $\mathcal{S}$ la classe delle superfici $\Sigma$ che ammettono una parametrizzazione $\vec{r} \colon \! \overline{T} \to \mathbb{R}^3$ con le seguenti proprietà:
		\begin{enumerate}
			\item T è un dominio s-decomponibile la cui frontiera è unione di un numero finito di curve di Jordan regolari a tratti $\gamma_1, \dots, \gamma_k$;
			\item $\vec{r} \in C^2(\overline{T})$ e $\vec{r}$ è biunivoca fra $\overline{T}$ e $\Sigma$.
		\end{enumerate}
	\end{defn}

	Il bordo di una superficie $\Sigma \in \mathcal{S}$ consiste di un numero finito di curve chiuse $\Gamma_j$ immagini delle $\gamma_j$. Scelto un verso per la normale a $\Sigma$, si orienta positivamente $\partial \Sigma$ scegliendo il verso di percorrenza su ciascuna delle $\Gamma_j$ in modo da lasciare i punti di $\Sigma$ a sinistra.

	\begin{teor}[di Stokes]
		Sia $\Sigma$ una superficie appartenente a $\mathcal{S}$, contenuta in un aperto $A \subseteq \mathbb{R}^3$. Sia $\vec{F} = P\vec{i} + Q\vec{j} + R\vec{k}$ un campo vettoriale di classe $C^1(A)$. Vale la formula
		\begin{equation}
			\label{eqn:stokespazio}
			\iint_{\Sigma} (\rot \vec{F}, \vec{n}) \, d\sigma = \int_{\partial^+\Sigma} (\vec{F}, \vec{T})\, ds,
		\end{equation}
		dove $\vec{T}$ indica il versore tangente a $\partial^+ \Sigma$.
	\end{teor}

	Il significato fisico è che il flusso del rotore di $\vec{F}$ attraverso $\Sigma$ nella direzione $\vec{n}$ eguaglia la circuitazione di $\vec{F}$ lungo $\partial^+ \Sigma$.

	Il teorema di Stokes vale per superfici più generali di quelle appartenenti a $\mathcal{S}$; chiameremo ammissibili le superfici a cui si può applicare la~\eqref{eqn:stokespazio}.

	Sono ammissibili, ad esempio, superfici $\Sigma$ orientabili, anche regolari a tratti, che si possano decomporre nell'unione di un numero finito di superfici $\Sigma_j \in \mathcal{S}$, tali che
	\begin{equation*}
		\overset{\circ}{\Sigma_i} \cap \overset{\circ}{\Sigma_j} \ne \emptyset, \quad i \ne j.
	\end{equation*}

	\subsection{Il teorema della divergenza}
	Sia $D \subseteq \mathbb{R}^3$ un dominio limitato la cui frontiera $\partial D$ sia una superficie chiusa, regolare e orientabile. Indichiamo con $\vec{n}_e$ il versore normale esterno a $\partial D$.
	Cominciamo enunciando il seguente lemma, valido in un dominio $D$ semplice rispetto all'asse $z$, ovvero
	\begin{equation*}
		D = \{(x, y, z) \in \mathbb{R}^3 \colon \phi_1(x, y) < z < \phi_2(x, y), (x, y) \in B \subset \mathbb{R}^2\},
	\end{equation*}
	dove $B$ è un aperto semplicemente connesso del piano e $\phi_1, \phi_2$ sono funzioni di classe $C^1(\overline{B})$.

	\begin{lem}
		Sia $\vec{F} = P\vec{i} + Q\vec{j} + R\vec{k} \in C^1(\overline{D})$. Vale la formula
		\begin{equation}
			\iiint_D R_z \,dx\,dy\,dz = \iint_{\partial D}R(\vec{n}_e, \vec{k})\,d\sigma.
		\end{equation}
	\end{lem}

	Analogamente, si ha che se $D$ è semplice rispetto all'asse $y$, allora
	\begin{equation}
		\iiint_D Q_y \,dx \,dy\,dz = \iint_{\partial D}Q(\vec{n}_e, \vec{j}) \, d\sigma;
	\end{equation}
	se $D$ è semplice rispetto all'asse $x$, allora
	\begin{equation}
		\iiint_D P_x\,dx\,dy\,dz = \iint_{\partial D} P (\vec{n}_e, \vec{i})\, d\sigma.
	\end{equation}

	\begin{teor}[della divergenza]
		\label{divergenza}
		Sia $D$ un dominio semplice rispetto a tutti e tre gli assi cartesiani. Sia $\vec{F} = P\vec{i} + Q\vec{j} + R\vec{k}$ un campo vettoriale di classe $C^1(\overline{D})$. Allora vale la formula
		\begin{equation}
			\label{eqn:divergenza}
			\iiint_D \divg \vec{F} \, dx \, dy \, dz = \iint_{\partial D} (\vec{F}, \vec{n}_e)\, d \sigma.
		\end{equation}
	\end{teor}

	La~\eqref{eqn:divergenza} significa che il flusso di $\vec{F}$ uscente da $\partial D$ uguaglia l'integrale della divergenza di $\vec{F}$ in $D$. Più esplicitamente, la~\eqref{eqn:divergenza} si può scrivere nella forma
	\begin{equation*}
		\begin{split}
			&\iiint_D(P_x + Q_y + R_z)\,dx\,dy\,dz = \\
			&= \iint_{\partial D}(P(\vec{n}_e, \vec{i}) + Q(\vec{n}_e, \vec{j}) + R(\vec{n}_e, \vec{k})\,d\sigma.)
		\end{split}
	\end{equation*}

	Il Teorema~\ref{divergenza} è valido per una classe di domini (detti ammissibili) molto più vasta; si può estendere ad esempio a domini $D$ limitati, la cui frontiera è costituita dall'unione di un numero finito di superfici $\Sigma_j$ chiuse, regolari, orientabili e disgiunte e che siano decomponibili in un numero finito di sottodomini semplici rispetto ai tre assi; cioè esiste un numero finito di domini $D_1, \dots, D_k$ tali che
	\begin{equation*}
		D=\bigcup_{j=1}^k D_j, \quad \overset{\circ}{D_i}\cap\overset{\circ}{D_j} = \emptyset, \quad i \ne j.
	\end{equation*}
	Si può anche richiedere che le $\Sigma_j$ siano solo regolari a pezzi.

	\appendix \chapter{Dimostrazioni}
	\section{Brevi cenni agli spazi metrici}
	\begin{itemize}
		\item Disuguaglianza di Cauchy-Schwarz
		\item Proprietà della norma.
	\end{itemize}
	\section{Funzioni di più variabili}
	\begin{itemize}
		\item Teorema della permanenza del sostegno
		\item Continuità della funzione che associa a $x=(x_1,x_2,...,x_n)$ una sua componente $x_i$
		\item La composizione di funzioni continue è continua.
		\item Teoremi dei valori intermedi
		\item Teorema degli zeri
		\item Teorema di Weiestrass
		\item Proprietà di sottolivelli e sopralivelli di essere chiusi o aperti
		\item Una funzione (a valori reali) differenziabile in $x_0$ interno a $D(i)$ è continua in $x_0$
		\item Teorema del differenziale totale
		\item Teorema di Fermat
		\item Ortogonalità del gradiente
		\item Formula di Taylor con resto di Lagrange
		\item Formula di Taylor con resto di Peano
		\item Condizione necessaria e sufficiente punto di massimo/minimo.
			\item Sia $I$ intervallo di $\mathbb{R}$; un'applicazione $r \colon I \to \mathbb{R}^n$ è continua in $t_0 \in I$ se e solo se lo sono tutte le sue componenti
	\end{itemize}
	\section{Curve}
	\begin{itemize}
		\item Una curva piana regolare è localmente una curva cartesiana
		\item Curve rettificabili
		\item Il grafico della funzione definita da $f(x) = x\sin(\frac{1}{x})$ in $(0,2/\pi]$, $f(0) = 0$ non è rettificabile
	\end{itemize}


	\section{Ottimizzazione di funzioni in più variabili}
	\begin{itemize}
		\item Teorema dei moltiplicatori di Lagrange
		\item Parallelepipedo dalla superficie massima è il cubo.
		\item Scheda moodle
	\end{itemize}

	\section{Calcolo Integrale}
	\begin{itemize}
		\item Caratterizzazione dell'integrabilità
		\item Una funzione continua in un rettangolo è integrabile
		\item Una funzione limitata in un rettangangolo $Q$, continua in $Q$ tranne al più in un insieme di misura nulla, è integrabile.
		\item L'integrale della funzione di Gauss.
	\end{itemize}

	\section{Campi vettoriali}
	\begin{itemize}
		\item Il lavoro dipende dall'orientazione della curva
		\item Teorema fondamentale del calcolo Integrale
		\item Lemma di Poincarè
		\item $A$ aperto connesso le primitive di $f$ sono $h=f+c$
		\item $A$ aperto connesso e $\nabla F = 0$ allora $f$ costante
	\end{itemize}

	\section{Successioni di funzioni}
	\begin{itemize}
		\item La proprietà di continuità di $f_n$ in $A$ contenuto in $\mathbb{R}$ sono preservate se vale la convergenza uniforme in $A$.
		\item Convergenza uniforme e passaggio al limite sotto il segno di derivata
		\item Convergenza uniforme e passaggio al limite sotto il segno di integrale
		\item Criterio di Weiestrass
	\end{itemize}

	\section{Equazioni Differenziali Ordinarie}
	\begin{itemize}
		\item Teorema di esistenza e unicità
	\end{itemize}

	\section{Superfici}
	\begin{itemize}
		\item Formule di Green nel piano
		\item Teorema della divergenza
		\item Teorema di Stokes
	\end{itemize}
	\end{document}
